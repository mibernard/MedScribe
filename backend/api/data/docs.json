[
    {
      "codeTitle": "Converting Text to Speech using ElevenLabs API in Python",
      "codeDescription": "This Python script demonstrates how to use the ElevenLabs API to convert text to speech. It initializes the client, makes a text-to-speech conversion request, and plays the resulting audio.",
      "codeLanguage": "python",
      "codeTokens": 170,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/quickstart.mdx#2025-04-17_snippet_0",
      "pageTitle": "ElevenLabs API Developer Quickstart",
      "codeList": [
        {
          "language": "python",
          "code": "from dotenv import load_dotenv\nfrom elevenlabs.client import ElevenLabs\nfrom elevenlabs import play\n\nload_dotenv()\n\nclient = ElevenLabs(\n  api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n)\n\naudio = client.text_to_speech.convert(\n    text=\"The first move is what sets everything in motion.\",\n    voice_id=\"JBFqnCBsd6RMkjVDRZzb\",\n    model_id=\"eleven_multilingual_v2\",\n    output_format=\"mp3_44100_128\",\n)\n\nplay(audio)"
        }
      ],
      "relevance": 0.995
    },
    {
      "codeTitle": "Generating Sound Effects with ElevenLabs Python SDK",
      "codeDescription": "Python implementation for generating sound effects using ElevenLabs API. Uses environment variables for API key configuration and includes audio playback functionality.",
      "codeLanguage": "python",
      "codeTokens": 120,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/sound-effects/quickstart.mdx#2025-04-17_snippet_0",
      "pageTitle": "Sound Effects Generation with ElevenLabs API",
      "codeList": [
        {
          "language": "python",
          "code": "# example.py\nimport os\nfrom dotenv import load_dotenv\nfrom elevenlabs.client import ElevenLabs\nfrom elevenlabs import play\n\nload_dotenv()\n\nclient = ElevenLabs(\n  api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n)\naudio = client.text_to_sound_effects.convert(text=\"Cinematic Braam, Horror\")\n\nplay(audio)"
        }
      ],
      "relevance": 0.99
    },
    {
      "codeTitle": "Implementing Voice Isolation in Python",
      "codeDescription": "Python implementation for removing background noise from an audio file using ElevenLabs Voice Isolator API. Uses requests to download sample audio and processes it through the isolation API. Requires dotenv for environment variables and elevenlabs SDK.",
      "codeLanguage": "python",
      "codeTokens": 183,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voice-isolator/quickstart.mdx#2025-04-17_snippet_0",
      "pageTitle": "Voice Isolator API Integration Guide",
      "codeList": [
        {
          "language": "python",
          "code": "# example.py\nimport os\nfrom dotenv import load_dotenv\nfrom elevenlabs.client import ElevenLabs\nfrom elevenlabs import play\nimport requests\nfrom io import BytesIO\n\nload_dotenv()\n\nclient = ElevenLabs(\n  api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n)\n\naudio_url = \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/fin.mp3\"\nresponse = requests.get(audio_url)\naudio_data = BytesIO(response.content)\n\naudio_stream = client.audio_isolation.audio_isolation(audio=audio_data)\n\nplay(audio_stream)"
        }
      ],
      "relevance": 0.987
    },
    {
      "codeTitle": "Enable RAG Integration in JavaScript",
      "codeDescription": "Implements RAG functionality for an ElevenLabs agent using the JavaScript client. Handles document indexing, status monitoring, and agent configuration updates. Includes error handling and automatic retries for indexing status checks.",
      "codeLanguage": "javascript",
      "codeTokens": 505,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/rag.mdx#2025-04-17_snippet_2",
      "pageTitle": "Retrieval-Augmented Generation (RAG) Documentation for ElevenLabs API",
      "codeList": [
        {
          "language": "javascript",
          "code": "async function enableRAG(documentId, agentId, apiKey) {\n  try {\n    // Initialize the ElevenLabs client\n    const { ElevenLabs } = require('elevenlabs');\n    const client = new ElevenLabs({\n      apiKey: apiKey,\n    });\n\n    // Start document indexing for RAG\n    let response = await client.conversationalAi.ragIndexStatus(documentId, {\n      model: 'e5_mistral_7b_instruct',\n    });\n\n    // Check indexing status until completion\n    while (response.status !== 'SUCCEEDED' && response.status !== 'FAILED') {\n      await new Promise((resolve) => setTimeout(resolve, 5000)); // Wait 5 seconds\n      response = await client.conversationalAi.ragIndexStatus(documentId, {\n        model: 'e5_mistral_7b_instruct',\n      });\n    }\n\n    if (response.status === 'FAILED') {\n      throw new Error('RAG indexing failed');\n    }\n\n    // Get current agent configuration\n    const agentConfig = await client.conversationalAi.getAgent(agentId);\n\n    // Enable RAG in the agent configuration\n    const updatedConfig = {\n      conversation_config: {\n        ...agentConfig.agent,\n        prompt: {\n          ...agentConfig.agent.prompt,\n          rag: {\n            enabled: true,\n            embedding_model: 'e5_mistral_7b_instruct',\n            max_documents_length: 10000,\n          },\n        },\n      },\n    };\n\n    // Update document usage mode if needed\n    if (agentConfig.agent.prompt.knowledge_base) {\n      agentConfig.agent.prompt.knowledge_base.forEach((doc, index) => {\n        if (doc.id === documentId) {\n          updatedConfig.conversation_config.prompt.knowledge_base[index].usage_mode = 'auto';\n        }\n      });\n    }\n\n    // Update the agent configuration\n    await client.conversationalAi.updateAgent(agentId, updatedConfig);\n\n    console.log('RAG configuration updated successfully');\n    return true;\n  } catch (error) {\n    console.error('Error configuring RAG:', error);\n    throw error;\n  }\n}"
        }
      ],
      "relevance": 0.985
    },
    {
      "codeTitle": "Node.js Server Implementation for Twilio and ElevenLabs Integration",
      "codeDescription": "Complete implementation of a Node.js server that connects Twilio voice calls to ElevenLabs Conversational AI. The server handles WebSocket connections, audio streaming between Twilio and ElevenLabs, and manages the conversation flow.",
      "codeLanguage": "javascript",
      "codeTokens": 1268,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-custom-server.mdx#2025-04-17_snippet_3",
      "pageTitle": "Twilio Integration Guide for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "import Fastify from \"fastify\";\nimport WebSocket from \"ws\";\nimport dotenv from \"dotenv\";\nimport fastifyFormBody from \"@fastify/formbody\";\nimport fastifyWs from \"@fastify/websocket\";\n\n// Load environment variables from .env file\ndotenv.config();\n\nconst { ELEVENLABS_AGENT_ID } = process.env;\n\n// Check for the required ElevenLabs Agent ID\nif (!ELEVENLABS_AGENT_ID) {\nconsole.error(\"Missing ELEVENLABS_AGENT_ID in environment variables\");\nprocess.exit(1);\n}\n\n// Initialize Fastify server\nconst fastify = Fastify();\nfastify.register(fastifyFormBody);\nfastify.register(fastifyWs);\n\nconst PORT = process.env.PORT || 8000;\n\n// Root route for health check\nfastify.get(\"/\", async (_, reply) => {\nreply.send({ message: \"Server is running\" });\n});\n\n// Route to handle incoming calls from Twilio\nfastify.all(\"/twilio/inbound_call\", async (request, reply) => {\n// Generate TwiML response to connect the call to a WebSocket stream\nconst twimlResponse = `<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n    <Response>\n    <Connect>\n        <Stream url=\"wss://${request.headers.host}/media-stream\" />\n    </Connect>\n    </Response>`;\n\nreply.type(\"text/xml\").send(twimlResponse);\n});\n\n// WebSocket route for handling media streams from Twilio\nfastify.register(async (fastifyInstance) => {\nfastifyInstance.get(\"/media-stream\", { websocket: true }, (connection, req) => {\n    console.info(\"[Server] Twilio connected to media stream.\");\n\n    let streamSid = null;\n\n    // Connect to ElevenLabs Conversational AI WebSocket\n    const elevenLabsWs = new WebSocket(\n    `wss://api.elevenlabs.io/v1/convai/conversation?agent_id=${ELEVENLABS_AGENT_ID}`\n    );\n\n    // Handle open event for ElevenLabs WebSocket\n    elevenLabsWs.on(\"open\", () => {\n    console.log(\"[II] Connected to Conversational AI.\");\n    });\n\n    // Handle messages from ElevenLabs\n    elevenLabsWs.on(\"message\", (data) => {\n    try {\n        const message = JSON.parse(data);\n        handleElevenLabsMessage(message, connection);\n    } catch (error) {\n        console.error(\"[II] Error parsing message:\", error);\n    }\n    });\n\n    // Handle errors from ElevenLabs WebSocket\n    elevenLabsWs.on(\"error\", (error) => {\n    console.error(\"[II] WebSocket error:\", error);\n    });\n\n    // Handle close event for ElevenLabs WebSocket\n    elevenLabsWs.on(\"close\", () => {\n    console.log(\"[II] Disconnected.\");\n    });\n\n    // Function to handle messages from ElevenLabs\n    const handleElevenLabsMessage = (message, connection) => {\n    switch (message.type) {\n        case \"conversation_initiation_metadata\":\n        console.info(\"[II] Received conversation initiation metadata.\");\n        break;\n        case \"audio\":\n        if (message.audio_event?.audio_base_64) {\n            // Send audio data to Twilio\n            const audioData = {\n            event: \"media\",\n            streamSid,\n            media: {\n                payload: message.audio_event.audio_base_64,\n            },\n            };\n            connection.send(JSON.stringify(audioData));\n        }\n        break;\n        case \"interruption\":\n        // Clear Twilio's audio queue\n        connection.send(JSON.stringify({ event: \"clear\", streamSid }));\n        break;\n        case \"ping\":\n        // Respond to ping events from ElevenLabs\n        if (message.ping_event?.event_id) {\n            const pongResponse = {\n            type: \"pong\",\n            event_id: message.ping_event.event_id,\n            };\n            elevenLabsWs.send(JSON.stringify(pongResponse));\n        }\n        break;\n    }\n    };\n\n    // Handle messages from Twilio\n    connection.on(\"message\", async (message) => {\n    try {\n        const data = JSON.parse(message);\n        switch (data.event) {\n        case \"start\":\n            // Store Stream SID when stream starts\n            streamSid = data.start.streamSid;\n            console.log(`[Twilio] Stream started with ID: ${streamSid}`);\n            break;\n        case \"media\":\n            // Route audio from Twilio to ElevenLabs\n            if (elevenLabsWs.readyState === WebSocket.OPEN) {\n            // data.media.payload is base64 encoded\n            const audioMessage = {\n                user_audio_chunk: Buffer.from(\n                    data.media.payload,\n                    \"base64\"\n                ).toString(\"base64\"),\n            };\n            elevenLabsWs.send(JSON.stringify(audioMessage));\n            }\n            break;\n        case \"stop\":\n            // Close ElevenLabs WebSocket when Twilio stream stops\n            elevenLabsWs.close();\n            break;\n        default:\n            console.log(`[Twilio] Received unhandled event: ${data.event}`);\n        }\n    } catch (error) {\n        console.error(\"[Twilio] Error processing message:\", error);\n    }\n    });\n\n    // Handle close event from Twilio\n    connection.on(\"close\", () => {\n    elevenLabsWs.close();\n    console.log(\"[Twilio] Client disconnected\");\n    });\n\n    // Handle errors from Twilio WebSocket\n    connection.on(\"error\", (error) => {\n    console.error(\"[Twilio] WebSocket error:\", error);\n    elevenLabsWs.close();\n    });\n});\n});\n\n// Start the Fastify server\nfastify.listen({ port: PORT }, (err) => {\nif (err) {\n    console.error(\"Error starting server:\", err);\n    process.exit(1);\n}\nconsole.log(`[Server] Listening on port ${PORT}`);\n});"
        }
      ],
      "relevance": 0.985
    },
    {
      "codeTitle": "Implementing Agent Transfer in Python with ElevenLabs API",
      "codeDescription": "Python code for configuring the 'transfer_to_agent' system tool when creating an agent via the ElevenLabs API. The code demonstrates how to define transfer rules, create a transfer tool configuration, and incorporate it into an agent configuration.",
      "codeLanguage": "python",
      "codeTokens": 338,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/agent-transfer.mdx#2025-04-17_snippet_1",
      "pageTitle": "Configuring Agent-to-Agent Transfer in ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "python",
          "code": "from elevenlabs import (\n    ConversationalConfig,\n    ElevenLabs,\n    AgentConfig,\n    PromptAgent,\n    PromptAgentToolsItem_System,\n    SystemToolConfig,\n    TransferToAgentToolConfig,\n    Transfer\n)\n\n# Initialize the client\nclient = ElevenLabs(api_key=\"YOUR_API_KEY\")\n\n# Define transfer rules\ntransfer_rules = [\n    Transfer(agent_id=\"AGENT_ID_1\", condition=\"When the user asks for billing support.\"),\n    Transfer(agent_id=\"AGENT_ID_2\", condition=\"When the user requests advanced technical help.\")\n]\n\n# Create the transfer tool configuration\ntransfer_tool = PromptAgentToolsItem_System(\n    type=\"system\",\n    name=\"transfer_to_agent\",\n    description=\"Transfer the user to a specialized agent based on their request.\", # Optional custom description\n    params=SystemToolConfigInputParams_TransferToAgent(\n        transfers=transfer_rules\n    )\n)\n\n# Create the agent configuration\nconversation_config = ConversationalConfig(\n    agent=AgentConfig(\n        prompt=PromptAgent(\n            prompt=\"You are a helpful assistant.\",\n            first_message=\"Hi, how can I help you today?\",\n            tools=[transfer_tool],\n        )\n    )\n)\n\n# Create the agent\nresponse = client.conversational_ai.create_agent(\n    conversation_config=conversation_config\n)\n\nprint(response)"
        }
      ],
      "relevance": 0.985
    },
    {
      "codeTitle": "Implementing Conversation Overrides in HTML Widget for ElevenLabs Conversational AI",
      "codeDescription": "This HTML code snippet demonstrates how to use the ElevenLabs Conversational AI widget with overrides for language, system prompt, first message, and voice ID. It allows for easy integration of a customized conversational AI agent into a web page.",
      "codeLanguage": "html",
      "codeTokens": 148,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/overrides.mdx#2025-04-17_snippet_3",
      "pageTitle": "Implementing Overrides for ElevenLabs Conversational AI Agents",
      "codeList": [
        {
          "language": "html",
          "code": "<elevenlabs-convai\n  agent-id=\"your-agent-id\"\n  override-language=\"es\"\n  override-prompt=\"Custom system prompt for this user\"\n  override-first-message=\"Hi! How can I help you today?\"\n  override-voice-id=\"axXgspJ2msm3clMCkdW3\"\n></elevenlabs-convai>"
        }
      ],
      "relevance": 0.985
    },
    {
      "codeTitle": "Voice Transformation Implementation in TypeScript",
      "codeDescription": "TypeScript implementation for transforming voice using ElevenLabs API. Uses the ElevenLabsClient to handle API interactions, fetches audio data, and processes it through the speech-to-speech conversion API. Requires ElevenLabs SDK and additional dependencies for audio playback.",
      "codeLanguage": "typescript",
      "codeTokens": 227,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voice-changer/quickstart.mdx#2025-04-17_snippet_1",
      "pageTitle": "Voice Changer API Implementation Guide",
      "codeList": [
        {
          "language": "typescript",
          "code": "// example.mts\nimport { ElevenLabsClient, play } from \"elevenlabs\";\nimport \"dotenv/config\";\n\nconst client = new ElevenLabsClient();\nconst voiceId = \"JBFqnCBsd6RMkjVDRZzb\";\n\nconst response = await fetch(\n  \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/nicole.mp3\"\n);\nconst audioBlob = new Blob([await response.arrayBuffer()], { type: \"audio/mp3\" });\n\nconst audioStream = await client.speechToSpeech.convert(voiceId, {\n  audio: audioBlob,\n  model_id: \"eleven_multilingual_sts_v2\",\n  output_format: \"mp3_44100_128\",\n});\n\nawait play(audioStream);"
        }
      ],
      "relevance": 0.985
    },
    {
      "codeTitle": "Creating Conversational AI Agent with Custom Voice and Knowledge Base in TypeScript",
      "codeDescription": "This snippet demonstrates how to create a conversational AI agent using the ElevenLabs API. It configures the agent with a custom voice, user-provided agent description, and knowledge base documents retrieved from Redis.",
      "codeLanguage": "typescript",
      "codeTokens": 190,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#2025-04-17_snippet_11",
      "pageTitle": "Data Collection and Analysis with Conversational AI in Next.js",
      "codeList": [
        {
          "language": "typescript",
          "code": "// ...\n\n// Handle agent creation\nconst agent = await elevenLabsClient.conversationalAi.createAgent({\n  name: `Agent for ${conversation_id}`,\n  conversation_config: {\n    tts: { voice_id: voice.voice_id },\n    agent: {\n      prompt: {\n        prompt:\n          analysis.data_collection_results.agent_description?.value ??\n          'You are a helpful assistant.',\n        knowledge_base: redisRes.knowledgeBase,\n      },\n      first_message: 'Hello, how can I help you today?',\n    },\n  },\n});\nconsole.log('Agent created', { agent: agent.agent_id });\n\n// ..."
        }
      ],
      "relevance": 0.985
    },
    {
      "codeTitle": "Initializing Conversation Session in JavaScript",
      "codeDescription": "Code snippet demonstrating how to initialize a Conversation instance using the ElevenLabs SDK. This starts a websocket connection and begins using the microphone for communication with the AI agent.",
      "codeLanguage": "javascript",
      "codeTokens": 63,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/javascript.mdx#2025-04-17_snippet_1",
      "pageTitle": "JavaScript SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "const conversation = await Conversation.startSession(options);"
        }
      ],
      "relevance": 0.985
    },
    {
      "codeTitle": "Text-to-Speech Streaming Implementation",
      "codeDescription": "Functions to convert text to speech with streaming output. Implements real-time audio streaming without saving to disk in both Python and TypeScript.",
      "codeLanguage": "python",
      "codeTokens": 267,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/streaming.mdx#2025-04-17_snippet_3",
      "pageTitle": "ElevenLabs Text-to-Speech Integration Guide",
      "codeList": [
        {
          "language": "python",
          "code": "import os\nfrom typing import IO\nfrom io import BytesIO\nfrom elevenlabs import VoiceSettings\nfrom elevenlabs.client import ElevenLabs\n\nELEVENLABS_API_KEY = os.getenv(\"ELEVENLABS_API_KEY\")\nclient = ElevenLabs(\n    api_key=ELEVENLABS_API_KEY,\n)\n\n\ndef text_to_speech_stream(text: str) -> IO[bytes]:\n    response = client.text_to_speech.convert(\n        voice_id=\"pNInz6obpgDQGcFmaJgB\",\n        output_format=\"mp3_22050_32\",\n        text=text,\n        model_id=\"eleven_multilingual_v2\",\n        voice_settings=VoiceSettings(\n            stability=0.0,\n            similarity_boost=1.0,\n            style=0.0,\n            use_speaker_boost=True,\n            speed=1.0,\n        ),\n    )\n\n    audio_stream = BytesIO()\n\n    for chunk in response:\n        if chunk:\n            audio_stream.write(chunk)\n\n    audio_stream.seek(0)\n\n    return audio_stream"
        },
        {
          "language": "typescript",
          "code": "import * as dotenv from 'dotenv';\nimport { ElevenLabsClient } from 'elevenlabs';\n\ndotenv.config();\n\nconst ELEVENLABS_API_KEY = process.env.ELEVENLABS_API_KEY;\n\nif (!ELEVENLABS_API_KEY) {\n  throw new Error('Missing ELEVENLABS_API_KEY in environment variables');\n}\n\nconst client = new ElevenLabsClient({\n  apiKey: ELEVENLABS_API_KEY,\n});\n\nexport const createAudioStreamFromText = async (text: string): Promise<Buffer> => {\n  const audioStream = await client.textToSpeech.convertAsStream('JBFqnCBsd6RMkjVDRZzb', {\n    model_id: 'eleven_multilingual_v2',\n    text,\n    output_format: 'mp3_44100_128',\n    voice_settings: {\n      stability: 0,\n      similarity_boost: 1.0,\n      use_speaker_boost: true,\n      speed: 1.0,\n    },\n  });\n\n  const chunks: Buffer[] = [];\n  for await (const chunk of audioStream) {\n    chunks.push(chunk);\n  }\n\n  const content = Buffer.concat(chunks);\n  return content;\n};"
        }
      ],
      "relevance": 0.98
    },
    {
      "codeTitle": "Update Agent Configuration in Python",
      "codeDescription": "Updates an existing agent's configuration using the ElevenLabs Python client. Takes an agent_id and agent_config as parameters to modify the conversation configuration.",
      "codeLanguage": "python",
      "codeTokens": 80,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/rag.mdx#2025-04-17_snippet_1",
      "pageTitle": "Retrieval-Augmented Generation (RAG) Documentation for ElevenLabs API",
      "codeList": [
        {
          "language": "python",
          "code": "client.conversational_ai.update_agent(\n    agent_id=agent_id,\n    conversation_config=agent_config.agent\n)"
        }
      ],
      "relevance": 0.98
    },
    {
      "codeTitle": "Performing Forced Alignment with ElevenLabs API in TypeScript",
      "codeDescription": "This TypeScript script demonstrates how to use the ElevenLabs API to perform forced alignment of text to audio. It fetches an audio file from a URL, initializes the ElevenLabs client, and calls the forcedAlignment.create method with the audio blob and text.",
      "codeLanguage": "typescript",
      "codeTokens": 212,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/forced-alignment/quickstart.mdx#2025-04-17_snippet_1",
      "pageTitle": "Forced Alignment API Quickstart Guide",
      "codeList": [
        {
          "language": "typescript",
          "code": "// example.ts\nimport { ElevenLabsClient } from \"elevenlabs\";\nimport \"dotenv/config\";\nconst client = new ElevenLabsClient();\n\nconst response = await fetch(\n    \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/nicole.mp3\"\n);\nconst audioBlob = new Blob([await response.arrayBuffer()], { type: \"audio/mp3\" });\n\nconst transcript = await client.forcedAlignment.create({\n    file: audioBlob,\n    text: \"With a soft and whispery American accent, I'm the ideal choice for creating ASMR content, meditative guides, or adding an intimate feel to your narrative projects.\"\n})\n\nconsole.log(transcript);"
        }
      ],
      "relevance": 0.98
    },
    {
      "codeTitle": "Converting Audio to Text using Python ElevenLabs SDK",
      "codeDescription": "Python implementation for converting audio to text using ElevenLabs API. Uses requests library to fetch audio from URL and processes it using the speech_to_text.convert method with options for model selection, audio event tagging, language detection, and speaker diarization.",
      "codeLanguage": "python",
      "codeTokens": 268,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/quickstart.mdx#2025-04-17_snippet_0",
      "pageTitle": "Speech to Text API Documentation",
      "codeList": [
        {
          "language": "python",
          "code": "# example.py\nimport os\nfrom dotenv import load_dotenv\nfrom io import BytesIO\nimport requests\nfrom elevenlabs.client import ElevenLabs\n\nload_dotenv()\n\nclient = ElevenLabs(\n  api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n)\n\naudio_url = (\n    \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/nicole.mp3\"\n)\nresponse = requests.get(audio_url)\naudio_data = BytesIO(response.content)\n\ntranscription = client.speech_to_text.convert(\n    file=audio_data,\n    model_id=\"scribe_v1\", # Model to use, for now only \"scribe_v1\" is supported\n    tag_audio_events=True, # Tag audio events like laughter, applause, etc.\n    language_code=\"eng\", # Language of the audio file. If set to None, the model will detect the language automatically.\n    diarize=True, # Whether to annotate who is speaking\n)\n\nprint(transcription)"
        }
      ],
      "relevance": 0.98
    },
    {
      "codeTitle": "Implementing Conversational AI DOM Component in React Native",
      "codeDescription": "TypeScript React component for implementing the conversational AI agent using Expo DOM components. It handles microphone permissions, conversation start/stop, and renders a call button.",
      "codeLanguage": "tsx",
      "codeTokens": 811,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/expo-react-native.mdx#2025-04-17_snippet_3",
      "pageTitle": "Building Cross-platform Voice Agents with Expo React Native and ElevenLabs",
      "codeList": [
        {
          "language": "tsx",
          "code": "'use dom';\n\nimport { useConversation } from '@11labs/react';\nimport { Mic } from 'lucide-react-native';\nimport { useCallback } from 'react';\nimport { View, Pressable, StyleSheet } from 'react-native';\n\nimport tools from '../utils/tools';\n\nasync function requestMicrophonePermission() {\n  try {\n    await navigator.mediaDevices.getUserMedia({ audio: true });\n    return true;\n  } catch (error) {\n    console.log(error);\n    console.error('Microphone permission denied');\n    return false;\n  }\n}\n\nexport default function ConvAiDOMComponent({\n  platform,\n  get_battery_level,\n  change_brightness,\n  flash_screen,\n}: {\n  dom?: import('expo/dom').DOMProps;\n  platform: string;\n  get_battery_level: typeof tools.get_battery_level;\n  change_brightness: typeof tools.change_brightness;\n  flash_screen: typeof tools.flash_screen;\n}) {\n  const conversation = useConversation({\n    onConnect: () => console.log('Connected'),\n    onDisconnect: () => console.log('Disconnected'),\n    onMessage: (message) => {\n      console.log(message);\n    },\n    onError: (error) => console.error('Error:', error),\n  });\n  const startConversation = useCallback(async () => {\n    try {\n      // Request microphone permission\n      const hasPermission = await requestMicrophonePermission();\n      if (!hasPermission) {\n        alert('No permission');\n        return;\n      }\n\n      // Start the conversation with your agent\n      await conversation.startSession({\n        agentId: 'YOUR_AGENT_ID', // Replace with your agent ID\n        dynamicVariables: {\n          platform,\n        },\n        clientTools: {\n          get_battery_level,\n          change_brightness,\n          flash_screen,\n        },\n      });\n    } catch (error) {\n      console.error('Failed to start conversation:', error);\n    }\n  }, [conversation]);\n\n  const stopConversation = useCallback(async () => {\n    await conversation.endSession();\n  }, [conversation]);\n\n  return (\n    <Pressable\n      style={[styles.callButton, conversation.status === 'connected' && styles.callButtonActive]}\n      onPress={conversation.status === 'disconnected' ? startConversation : stopConversation}\n    >\n      <View\n        style={[\n          styles.buttonInner,\n          conversation.status === 'connected' && styles.buttonInnerActive,\n        ]}\n      >\n        <Mic size={32} color=\"#E2E8F0\" strokeWidth={1.5} style={styles.buttonIcon} />\n      </View>\n    </Pressable>\n  );\n}\n\nconst styles = StyleSheet.create({\n  callButton: {\n    width: 120,\n    height: 120,\n    borderRadius: 60,\n    backgroundColor: 'rgba(255, 255, 255, 0.1)',\n    alignItems: 'center',\n    justifyContent: 'center',\n    marginBottom: 24,\n  },\n  callButtonActive: {\n    backgroundColor: 'rgba(239, 68, 68, 0.2)',\n  },\n  buttonInner: {\n    width: 80,\n    height: 80,\n    borderRadius: 40,\n    backgroundColor: '#3B82F6',\n    alignItems: 'center',\n    justifyContent: 'center',\n    shadowColor: '#3B82F6',\n    shadowOffset: {\n      width: 0,\n      height: 0,\n    },\n    shadowOpacity: 0.5,\n    shadowRadius: 20,\n    elevation: 5,\n  },\n  buttonInnerActive: {\n    backgroundColor: '#EF4444',\n    shadowColor: '#EF4444',\n  },\n  buttonIcon: {\n    transform: [{ translateY: 2 }],\n  },\n});"
        }
      ],
      "relevance": 0.978
    },
    {
      "codeTitle": "Implementing Complete Post-Call Webhook Handler in TypeScript",
      "codeDescription": "Main implementation of the webhook handler including Redis initialization, webhook verification, voice creation, and agent setup. Uses ElevenLabs client, Redis, and Resend for email notifications.",
      "codeLanguage": "typescript",
      "codeTokens": 741,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#2025-04-17_snippet_7",
      "pageTitle": "Data Collection and Analysis with Conversational AI in Next.js",
      "codeList": [
        {
          "language": "typescript",
          "code": "import { Redis } from '@upstash/redis';\nimport crypto from 'crypto';\nimport { ElevenLabsClient } from 'elevenlabs';\nimport { NextResponse } from 'next/server';\nimport type { NextRequest } from 'next/server';\nimport { Resend } from 'resend';\n\nimport { EmailTemplate } from '@/components/email/post-call-webhook-email';\n\n// Initialize Redis\nconst redis = Redis.fromEnv();\n// Initialize Resend\nconst resend = new Resend(process.env.RESEND_API_KEY);\n\nconst elevenLabsClient = new ElevenLabsClient({\n  apiKey: process.env.ELEVENLABS_API_KEY,\n});\n\nexport async function GET() {\n  return NextResponse.json({ status: 'webhook listening' }, { status: 200 });\n}\n\nexport async function POST(req: NextRequest) {\n  const secret = process.env.ELEVENLABS_CONVAI_WEBHOOK_SECRET;\n  const { event, error } = await constructWebhookEvent(req, secret);\n  if (error) {\n    return NextResponse.json({ error: error }, { status: 401 });\n  }\n\n  if (event.type === 'post_call_transcription') {\n    const { conversation_id, analysis, agent_id } = event.data;\n\n    if (\n      agent_id === process.env.ELEVENLABS_AGENT_ID &&\n      analysis.evaluation_criteria_results.all_data_provided?.result === 'success' &&\n      analysis.data_collection_results.voice_description?.value\n    ) {\n      try {\n        const voicePreview = await elevenLabsClient.textToVoice.createPreviews({\n          voice_description: analysis.data_collection_results.voice_description.value,\n          text: 'The night air carried whispers of betrayal, thick as London fog. I adjusted my cufflinks - after all, even spies must maintain appearances, especially when the game is afoot.',\n        });\n        const voice = await elevenLabsClient.textToVoice.createVoiceFromPreview({\n          voice_name: `voice-${conversation_id}`,\n          voice_description: `Voice for ${conversation_id}`,\n          generated_voice_id: voicePreview.previews[0].generated_voice_id,\n        });\n\n        const redisRes = await getRedisDataWithRetry(conversation_id);\n        if (!redisRes) throw new Error('Conversation data not found!');\n        const agent = await elevenLabsClient.conversationalAi.createAgent({\n          name: `Agent for ${conversation_id}`,\n          conversation_config: {\n            tts: { voice_id: voice.voice_id },\n            agent: {\n              prompt: {\n                prompt:\n                  analysis.data_collection_results.agent_description?.value ??\n                  'You are a helpful assistant.',\n                knowledge_base: redisRes.knowledgeBase,\n              },\n              first_message: 'Hello, how can I help you today?',\n            },\n          },\n        });\n        console.log('Agent created', { agent: agent.agent_id });\n        await resend.emails.send({\n          from: process.env.RESEND_FROM_EMAIL!,\n          to: redisRes.email,\n          subject: 'Your Conversational AI agent is ready to chat!',\n          react: EmailTemplate({ agentId: agent.agent_id }),\n        });\n      } catch (error) {\n        console.error(error);\n        return NextResponse.json({ error }, { status: 500 });\n      }\n    }\n  }\n\n  return NextResponse.json({ received: true }, { status: 200 });\n}"
        }
      ],
      "relevance": 0.975
    },
    {
      "codeTitle": "Initializing FastAPI Server with Twilio and ElevenLabs Integration",
      "codeDescription": "Sets up a FastAPI server that handles incoming Twilio calls and integrates with ElevenLabs for AI-powered conversations. It includes endpoints for call handling and WebSocket connections for audio streaming.",
      "codeLanguage": "python",
      "codeTokens": 572,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-custom-server.mdx#2025-04-17_snippet_10",
      "pageTitle": "Twilio Integration Guide for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "python",
          "code": "from fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect\nfrom fastapi.responses import HTMLResponse\nfrom twilio.twiml.voice_response import VoiceResponse, Connect\nfrom elevenlabs import ElevenLabs\nfrom elevenlabs.conversational_ai.conversation import Conversation\nfrom twilio_audio_interface import TwilioAudioInterface\n\n# Load environment variables\nload_dotenv()\n\n# Initialize FastAPI app\napp = FastAPI()\n\n# Initialize ElevenLabs client\neleven_labs_client = ElevenLabs(api_key=os.getenv(\"ELEVENLABS_API_KEY\"))\nELEVEN_LABS_AGENT_ID = os.getenv(\"AGENT_ID\")\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Twilio-ElevenLabs Integration Server\"}\n\n@app.api_route(\"/twilio/inbound_call\", methods=[\"GET\", \"POST\"])\nasync def handle_incoming_call(request: Request):\n    \"\"\"Handle incoming call and return TwiML response.\"\"\"\n    response = VoiceResponse()\n    host = request.url.hostname\n    connect = Connect()\n    connect.stream(url=f\"wss://{host}/media-stream-eleven\")\n    response.append(connect)\n    return HTMLResponse(content=str(response), media_type=\"application/xml\")\n\n@app.websocket(\"/media-stream-eleven\")\nasync def handle_media_stream(websocket: WebSocket):\n    await websocket.accept()\n    print(\"WebSocket connection established\")\n\n    audio_interface = TwilioAudioInterface(websocket)\n    conversation = None\n\n    try:\n        conversation = Conversation(\n            client=eleven_labs_client,\n            agent_id=ELEVEN_LABS_AGENT_ID,\n            requires_auth=False,\n            audio_interface=audio_interface,\n            callback_agent_response=lambda text: print(f\"Agent said: {text}\"),\n            callback_user_transcript=lambda text: print(f\"User said: {text}\"),\n        )\n\n        conversation.start_session()\n        print(\"Conversation session started\")\n\n        async for message in websocket.iter_text():\n            if not message:\n                continue\n\n            try:\n                data = json.loads(message)\n                await audio_interface.handle_twilio_message(data)\n            except Exception as e:\n                print(f\"Error processing message: {str(e)}\")\n                traceback.print_exc()\n\n    except WebSocketDisconnect:\n        print(\"WebSocket disconnected\")\n    finally:\n        if conversation:\n            print(\"Ending conversation session...\")\n            conversation.end_session()\n            conversation.wait_for_session_end()\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
        }
      ],
      "relevance": 0.975
    },
    {
      "codeTitle": "Implementing Conversation Component in TypeScript",
      "codeDescription": "TypeScript code for the Conversation component, which handles the core functionality of starting and stopping conversations with the AI agent. It uses the useConversation hook from the ElevenLabs React package.",
      "codeLanguage": "typescript",
      "codeTokens": 447,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/nextjs.mdx#2025-04-17_snippet_3",
      "pageTitle": "Implementing Conversational AI with Next.js and ElevenLabs",
      "codeList": [
        {
          "language": "typescript",
          "code": "'use client';\n\nimport { useConversation } from '@11labs/react';\nimport { useCallback } from 'react';\n\nexport function Conversation() {\n  const conversation = useConversation({\n    onConnect: () => console.log('Connected'),\n    onDisconnect: () => console.log('Disconnected'),\n    onMessage: (message) => console.log('Message:', message),\n    onError: (error) => console.error('Error:', error),\n  });\n\n\n  const startConversation = useCallback(async () => {\n    try {\n      // Request microphone permission\n      await navigator.mediaDevices.getUserMedia({ audio: true });\n\n      // Start the conversation with your agent\n      await conversation.startSession({\n        agentId: 'YOUR_AGENT_ID', // Replace with your agent ID\n      });\n\n    } catch (error) {\n      console.error('Failed to start conversation:', error);\n    }\n  }, [conversation]);\n\n  const stopConversation = useCallback(async () => {\n    await conversation.endSession();\n  }, [conversation]);\n\n  return (\n    <div className=\"flex flex-col items-center gap-4\">\n      <div className=\"flex gap-2\">\n        <button\n          onClick={startConversation}\n          disabled={conversation.status === 'connected'}\n          className=\"px-4 py-2 bg-blue-500 text-white rounded disabled:bg-gray-300\"\n        >\n          Start Conversation\n        </button>\n        <button\n          onClick={stopConversation}\n          disabled={conversation.status !== 'connected'}\n          className=\"px-4 py-2 bg-red-500 text-white rounded disabled:bg-gray-300\"\n        >\n          Stop Conversation\n        </button>\n      </div>\n\n      <div className=\"flex flex-col items-center\">\n        <p>Status: {conversation.status}</p>\n        <p>Agent is {conversation.isSpeaking ? 'speaking' : 'listening'}</p>\n      </div>\n    </div>\n  );\n}"
        }
      ],
      "relevance": 0.975
    },
    {
      "codeTitle": "Implementing Dynamic Variables in Python for ElevenLabs Conversational Agent",
      "codeDescription": "Python code demonstrating how to configure and start a conversation with ElevenLabs agent using dynamic variables. The example shows how to initialize a conversation with a user_name variable and set up appropriate configuration and callbacks.",
      "codeLanguage": "python",
      "codeTokens": 340,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/dynamic-variables.mdx#2025-04-17_snippet_0",
      "pageTitle": "Dynamic Variables for ElevenLabs Conversational Agents",
      "codeList": [
        {
          "language": "python",
          "code": "import os\nimport signal\nfrom elevenlabs.client import ElevenLabs\nfrom elevenlabs.conversational_ai.conversation import Conversation, ConversationConfig\nfrom elevenlabs.conversational_ai.default_audio_interface import DefaultAudioInterface\n\nagent_id = os.getenv(\"AGENT_ID\")\napi_key = os.getenv(\"ELEVENLABS_API_KEY\")\nclient = ElevenLabs(api_key=api_key)\n\ndynamic_vars = {\n  \"user_name\": \"Angelo\",\n}\n\nconfig = ConversationConfig(\n  dynamic_variables=dynamic_vars\n)\n\nconversation = Conversation(\n  client,\n  agent_id,\n  config=config,\n  # Assume auth is required when API_KEY is set.\n  requires_auth=bool(api_key),\n  # Use the default audio interface.\n  audio_interface=DefaultAudioInterface(),\n  # Simple callbacks that print the conversation to the console.\n  callback_agent_response=lambda response: print(f\"Agent: {response}\"),\n  callback_agent_response_correction=lambda original, corrected: print(f\"Agent: {original} -> {corrected}\"),\n  callback_user_transcript=lambda transcript: print(f\"User: {transcript}\"),\n  # Uncomment the below if you want to see latency measurements.\n  # callback_latency_measurement=lambda latency: print(f\"Latency: {latency}ms\"),\n)\n\nconversation.start_session()\n\nsignal.signal(signal.SIGINT, lambda sig, frame: conversation.end_session())"
        }
      ],
      "relevance": 0.975
    },
    {
      "codeTitle": "Converting Audio to Text using TypeScript ElevenLabs SDK",
      "codeDescription": "TypeScript implementation for converting audio to text using ElevenLabs API. Fetches audio using fetch API and processes it using the speechToText.convert method with similar configuration options as the Python version.",
      "codeLanguage": "typescript",
      "codeTokens": 246,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/quickstart.mdx#2025-04-17_snippet_1",
      "pageTitle": "Speech to Text API Documentation",
      "codeList": [
        {
          "language": "typescript",
          "code": "// example.mts\nimport { ElevenLabsClient } from \"elevenlabs\";\nimport \"dotenv/config\";\n\nconst client = new ElevenLabsClient();\n\nconst response = await fetch(\n  \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/nicole.mp3\"\n);\nconst audioBlob = new Blob([await response.arrayBuffer()], { type: \"audio/mp3\" });\n\nconst transcription = await client.speechToText.convert({\n  file: audioBlob,\n  model_id: \"scribe_v1\", // Model to use, for now only \"scribe_v1\" is support.\n  tag_audio_events: true, // Tag audio events like laughter, applause, etc.\n  language_code: \"eng\", // Language of the audio file. If set to null, the model will detect the language automatically.\n  diarize: true, // Whether to annotate who is speaking\n});\n\nconsole.log(transcription);"
        }
      ],
      "relevance": 0.975
    },
    {
      "codeTitle": "Sound Effects Generation - Python Implementation",
      "codeDescription": "Complete Python script demonstrating how to generate sound effects from text using the ElevenLabs SDK. Includes environment setup, API client initialization, and file handling for saving the generated audio.",
      "codeLanguage": "python",
      "codeTokens": 243,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/sound-effects/basics.mdx#2025-04-17_snippet_3",
      "pageTitle": "Text to Sound Effects Generation with ElevenLabs API",
      "codeList": [
        {
          "language": "python",
          "code": "import os\nfrom elevenlabs.client import ElevenLabs\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nelevenlabs = ElevenLabs(api_key=os.getenv(\"ELEVENLABS_API_KEY\"))\n\n\ndef generate_sound_effect(text: str, output_path: str):\n    print(\"Generating sound effects...\")\n\n    result = elevenlabs.text_to_sound_effects.convert(\n        text=text,\n        duration_seconds=10,  # Optional, if not provided will automatically determine the correct length\n        prompt_influence=0.3,  # Optional, if not provided will use the default value of 0.3\n    )\n\n    with open(output_path, \"wb\") as f:\n        for chunk in result:\n            f.write(chunk)\n\n    print(f\"Audio saved to {output_path}\")\n\n\nif __name__ == \"__main__\":\n    generate_sound_effect(\"Dog barking\", \"output.mp3\")"
        }
      ],
      "relevance": 0.975
    },
    {
      "codeTitle": "Performing Forced Alignment with ElevenLabs API in Python",
      "codeDescription": "This Python script demonstrates how to use the ElevenLabs API to perform forced alignment of text to audio. It fetches an audio file from a URL, initializes the ElevenLabs client, and calls the forced_alignment.create method with the audio data and text.",
      "codeLanguage": "python",
      "codeTokens": 235,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/forced-alignment/quickstart.mdx#2025-04-17_snippet_0",
      "pageTitle": "Forced Alignment API Quickstart Guide",
      "codeList": [
        {
          "language": "python",
          "code": "# example.py\nimport os\nfrom io import BytesIO\nfrom elevenlabs.client import ElevenLabs\nimport requests\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclient = ElevenLabs(\n    api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n)\n\naudio_url = (\n    \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/nicole.mp3\"\n)\nresponse = requests.get(audio_url)\naudio_data = BytesIO(response.content)\n\n# Perform the text-to-speech conversion\ntranscription = client.forced_alignment.create(\n    file=audio_data,\n    text=\"With a soft and whispery American accent, I'm the ideal choice for creating ASMR content, meditative guides, or adding an intimate feel to your narrative projects.\"\n)\n\nprint(transcription)"
        }
      ],
      "relevance": 0.975
    },
    {
      "codeTitle": "Converting Speech to Text in Python",
      "codeDescription": "Python script to convert speech to text using the ElevenLabs SDK. It includes setting up the client, fetching audio data, and using the convert method with various parameters.",
      "codeLanguage": "python",
      "codeTokens": 254,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/synchronous.mdx#2025-04-17_snippet_5",
      "pageTitle": "Synchronous Speech to Text with ElevenLabs",
      "codeList": [
        {
          "language": "python",
          "code": "from dotenv import load_dotenv\nfrom io import BytesIO\nimport requests\nfrom elevenlabs.client import ElevenLabs\nimport os\n\nload_dotenv()\n\nclient = ElevenLabs(\n    api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n)\n\naudio_url = (\n    \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/nicole.mp3\"\n)\nresponse = requests.get(audio_url)\naudio_data = BytesIO(response.content)\n\ntranscription = client.speech_to_text.convert(\n    file=audio_data,\n    model_id=\"scribe_v1\", # 'scribe_v1_experimental' is also available for new, experimental features\n    tag_audio_events=True, # Tag audio events like laughter, applause, etc.\n    language_code=\"eng\", # Language of the audio file. If set to None, the model will detect the language automatically.\n    diarize=True, # Whether to annotate who is speaking\n)\n\nprint(transcription.text)"
        }
      ],
      "relevance": 0.972
    },
    {
      "codeTitle": "Registering Client Tools in Swift",
      "codeDescription": "Code snippet showing how to register a client-side tool called 'logMessage' in Swift. The function processes parameters, extracts a message, and prints it.",
      "codeLanguage": "swift",
      "codeTokens": 118,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-tools.mdx#2025-04-17_snippet_2",
      "pageTitle": "Client Tools for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "swift",
          "code": "// ...\nvar clientTools = ElevenLabsSDK.ClientTools()\n\nclientTools.register(\"logMessage\") { parameters async throws -> String? in\n    guard let message = parameters[\"message\"] as? String else {\n        throw ElevenLabsSDK.ClientToolError.invalidParameters\n    }\n    print(message)\n    return message\n}"
        }
      ],
      "relevance": 0.972
    },
    {
      "codeTitle": "Overriding Conversation Settings in Python for ElevenLabs Conversational AI",
      "codeDescription": "This Python code snippet demonstrates how to override the default agent system prompt, first message, language, and voice ID when starting a conversation. It uses the ElevenLabs Conversational AI SDK to configure and initiate a personalized conversation.",
      "codeLanguage": "python",
      "codeTokens": 213,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/overrides.mdx#2025-04-17_snippet_0",
      "pageTitle": "Implementing Overrides for ElevenLabs Conversational AI Agents",
      "codeList": [
        {
          "language": "python",
          "code": "from elevenlabs.conversational_ai.conversation import Conversation, ConversationConfig\n...\nconversation_override = {\n    \"agent\": {\n        \"prompt\": {\n            \"prompt\": f\"The customer's bank account balance is {customer_balance}. They are based in {customer_location}.\"\n        },\n        \"first_message\": f\"Hi {customer_name}, how can I help you today?\",\n        \"language\": \"en\" # Optional: override the language.\n    },\n    \"tts\": {\n        \"voice_id\": \"\" # Optional: override the voice.\n    }\n}\n\nconfig = ConversationConfig(\n    conversation_config_override=conversation_override\n)\nconversation = Conversation(\n    ...\n    config=config,\n    ...\n)\nconversation.start_session()"
        }
      ],
      "relevance": 0.97
    },
    {
      "codeTitle": "Implementing Language Detection in Python with ElevenLabs API",
      "codeDescription": "Example of creating a Conversational AI agent with language detection capabilities using the ElevenLabs Python SDK. Demonstrates configuration of language presets and system tools for multiple languages including Dutch, Finnish, Turkish, Russian, and Portuguese.",
      "codeLanguage": "python",
      "codeTokens": 527,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/language-detection.mdx#2025-04-17_snippet_0",
      "pageTitle": "Language Detection System Tool Documentation",
      "codeList": [
        {
          "language": "python",
          "code": "from elevenlabs import (\n    ConversationalConfig,\n    ElevenLabs,\n    AgentConfig,\n    PromptAgent,\n    PromptAgentToolsItem_System,\n    LanguagePreset,\n    ConversationConfigClientOverride,\n    AgentConfigOverride,\n    LanguagePresetTranslation,\n    PromptAgentOverride\n)\n\n# Initialize the client\nclient = ElevenLabs(api_key=\"YOUR_API_KEY\")\n\n# Create the language detection tool\nlanguage_detection_tool = PromptAgentToolsItem_System(\n    name=\"language_detection\",\n    description=\"\"  # Optional: Customize when the tool should be triggered\n)\n\n# Create language presets\nlanguage_presets = {\n    \"nl\": LanguagePreset(\n        overrides=ConversationConfigClientOverride(\n            agent=AgentConfigOverride(\n                prompt=None,\n                first_message=\"Hoi, hoe gaat het met je?\",\n                language=None\n            ),\n            tts=None\n        ),\n        first_message_translation=None\n    ),\n    \"fi\": LanguagePreset(\n        overrides=ConversationConfigClientOverride(\n            agent=AgentConfigOverride(\n                first_message=\"Hei, kuinka voit?\",\n            ),\n            tts=None\n        ),\n    ),\n    \"tr\": LanguagePreset(\n        overrides=ConversationConfigClientOverride(\n            agent=AgentConfigOverride(\n                prompt=None,\n                first_message=\"Merhaba, nasılsın?\",\n                language=None\n            ),\n            tts=None\n        ),\n    ),\n    \"ru\": LanguagePreset(\n        overrides=ConversationConfigClientOverride(\n            agent=AgentConfigOverride(\n                prompt=None,\n                first_message=\"Привет, как ты?\",\n                language=None\n            ),\n            tts=None\n        ),\n    ),\n    \"pt\": LanguagePreset(\n        overrides=ConversationConfigClientOverride(\n            agent=AgentConfigOverride(\n                prompt=None,\n                first_message=\"Oi, como você está?\",\n                language=None\n            ),\n            tts=None\n        ),\n    )\n}\n\n# Create the agent configuration\nconversation_config = ConversationalConfig(\n    agent=AgentConfig(\n        prompt=PromptAgent(\n            tools=[language_detection_tool],\n            first_message=\"Hi how are you?\"\n        )\n    ),\n    language_presets=language_presets\n)\n\n# Create the agent\nresponse = client.conversational_ai.create_agent(\n    conversation_config=conversation_config\n)"
        }
      ],
      "relevance": 0.97
    },
    {
      "codeTitle": "Implementing Twilio-ElevenLabs Integration Server",
      "codeDescription": "Complete server implementation using Fastify that handles outbound Twilio calls and integrates with ElevenLabs conversational AI. Includes WebSocket handling, audio streaming, and conversation management.",
      "codeLanguage": "javascript",
      "codeTokens": 2123,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-outbound-calling.mdx#2025-04-17_snippet_4",
      "pageTitle": "Building Outbound Calling AI Agents with Twilio and ElevenLabs",
      "codeList": [
        {
          "language": "javascript",
          "code": "import fastifyFormBody from '@fastify/formbody';\nimport fastifyWs from '@fastify/websocket';\nimport dotenv from 'dotenv';\nimport Fastify from 'fastify';\nimport Twilio from 'twilio';\nimport WebSocket from 'ws';\n\n// Load environment variables from .env file\ndotenv.config();\n\n// Check for required environment variables\nconst {\n  ELEVENLABS_API_KEY,\n  ELEVENLABS_AGENT_ID,\n  TWILIO_ACCOUNT_SID,\n  TWILIO_AUTH_TOKEN,\n  TWILIO_PHONE_NUMBER,\n} = process.env;\n\nif (\n  !ELEVENLABS_API_KEY ||\n  !ELEVENLABS_AGENT_ID ||\n  !TWILIO_ACCOUNT_SID ||\n  !TWILIO_AUTH_TOKEN ||\n  !TWILIO_PHONE_NUMBER\n) {\n  console.error('Missing required environment variables');\n  throw new Error('Missing required environment variables');\n}\n\n// Initialize Fastify server\nconst fastify = Fastify();\nfastify.register(fastifyFormBody);\nfastify.register(fastifyWs);\n\nconst PORT = process.env.PORT || 8000;\n\n// Root route for health check\nfastify.get('/', async (_, reply) => {\n  reply.send({ message: 'Server is running' });\n});\n\n// Initialize Twilio client\nconst twilioClient = new Twilio(TWILIO_ACCOUNT_SID, TWILIO_AUTH_TOKEN);\n\n// Helper function to get signed URL for authenticated conversations\nasync function getSignedUrl() {\n  try {\n    const response = await fetch(\n      `https://api.elevenlabs.io/v1/convai/conversation/get_signed_url?agent_id=${ELEVENLABS_AGENT_ID}`,\n      {\n        method: 'GET',\n        headers: {\n          'xi-api-key': ELEVENLABS_API_KEY,\n        },\n      }\n    );\n\n    if (!response.ok) {\n      throw new Error(`Failed to get signed URL: ${response.statusText}`);\n    }\n\n    const data = await response.json();\n    return data.signed_url;\n  } catch (error) {\n    console.error('Error getting signed URL:', error);\n    throw error;\n  }\n}\n\n// Route to initiate outbound calls\nfastify.post('/outbound-call', async (request, reply) => {\n  const { number, prompt, first_message } = request.body;\n\n  if (!number) {\n    return reply.code(400).send({ error: 'Phone number is required' });\n  }\n\n  try {\n    const call = await twilioClient.calls.create({\n      from: TWILIO_PHONE_NUMBER,\n      to: number,\n      url: `https://${request.headers.host}/outbound-call-twiml?prompt=${encodeURIComponent(\n        prompt\n      )}&first_message=${encodeURIComponent(first_message)}`,\n    });\n\n    reply.send({\n      success: true,\n      message: 'Call initiated',\n      callSid: call.sid,\n    });\n  } catch (error) {\n    console.error('Error initiating outbound call:', error);\n    reply.code(500).send({\n      success: false,\n      error: 'Failed to initiate call',\n    });\n  }\n});\n\n// TwiML route for outbound calls\nfastify.all('/outbound-call-twiml', async (request, reply) => {\n  const prompt = request.query.prompt || '';\n  const first_message = request.query.first_message || '';\n\n  const twimlResponse = `<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\n    <Response>\n        <Connect>\n        <Stream url=\\\"wss://${request.headers.host}/outbound-media-stream\\\">\n            <Parameter name=\\\"prompt\\\" value=\\\"${prompt}\\\" />\n            <Parameter name=\\\"first_message\\\" value=\\\"${first_message}\\\" />\n        </Stream>\n        </Connect>\n    </Response>`;\n\n  reply.type('text/xml').send(twimlResponse);\n});\n\n// WebSocket route for handling media streams\nfastify.register(async (fastifyInstance) => {\n  fastifyInstance.get('/outbound-media-stream', { websocket: true }, (ws, req) => {\n    console.info('[Server] Twilio connected to outbound media stream');\n\n    // Variables to track the call\n    let streamSid = null;\n    let callSid = null;\n    let elevenLabsWs = null;\n    let customParameters = null; // Add this to store parameters\n\n    // Handle WebSocket errors\n    ws.on('error', console.error);\n\n    // Set up ElevenLabs connection\n    const setupElevenLabs = async () => {\n      try {\n        const signedUrl = await getSignedUrl();\n        elevenLabsWs = new WebSocket(signedUrl);\n\n        elevenLabsWs.on('open', () => {\n          console.log('[ElevenLabs] Connected to Conversational AI');\n\n          // Send initial configuration with prompt and first message\n          const initialConfig = {\n            type: 'conversation_initiation_client_data',\n            dynamic_variables: {\n              user_name: 'Angelo',\n              user_id: 1234,\n            },\n            conversation_config_override: {\n              agent: {\n                prompt: {\n                  prompt: customParameters?.prompt || 'you are a gary from the phone store',\n                },\n                first_message:\n                  customParameters?.first_message || 'hey there! how can I help you today?',\n              },\n            },\n          };\n\n          console.log(\n            '[ElevenLabs] Sending initial config with prompt:',\n            initialConfig.conversation_config_override.agent.prompt.prompt\n          );\n\n          // Send the configuration to ElevenLabs\n          elevenLabsWs.send(JSON.stringify(initialConfig));\n        });\n\n        elevenLabsWs.on('message', (data) => {\n          try {\n            const message = JSON.parse(data);\n\n            switch (message.type) {\n              case 'conversation_initiation_metadata':\n                console.log('[ElevenLabs] Received initiation metadata');\n                break;\n\n              case 'audio':\n                if (streamSid) {\n                  if (message.audio?.chunk) {\n                    const audioData = {\n                      event: 'media',\n                      streamSid,\n                      media: {\n                        payload: message.audio.chunk,\n                      },\n                    };\n                    ws.send(JSON.stringify(audioData));\n                  } else if (message.audio_event?.audio_base_64) {\n                    const audioData = {\n                      event: 'media',\n                      streamSid,\n                      media: {\n                        payload: message.audio_event.audio_base_64,\n                      },\n                    };\n                    ws.send(JSON.stringify(audioData));\n                  }\n                } else {\n                  console.log('[ElevenLabs] Received audio but no StreamSid yet');\n                }\n                break;\n\n              case 'interruption':\n                if (streamSid) {\n                  ws.send(\n                    JSON.stringify({\n                      event: 'clear',\n                      streamSid,\n                    })\n                  );\n                }\n                break;\n\n              case 'ping':\n                if (message.ping_event?.event_id) {\n                  elevenLabsWs.send(\n                    JSON.stringify({\n                      type: 'pong',\n                      event_id: message.ping_event.event_id,\n                    })\n                  );\n                }\n                break;\n\n              case 'agent_response':\n                console.log(\n                  `[Twilio] Agent response: ${message.agent_response_event?.agent_response}`\n                );\n                break;\n\n              case 'user_transcript':\n                console.log(\n                  `[Twilio] User transcript: ${message.user_transcription_event?.user_transcript}`\n                );\n                break;\n\n              default:\n                console.log(`[ElevenLabs] Unhandled message type: ${message.type}`);\n            }\n          } catch (error) {\n            console.error('[ElevenLabs] Error processing message:', error);\n          }\n        });\n\n        elevenLabsWs.on('error', (error) => {\n          console.error('[ElevenLabs] WebSocket error:', error);\n        });\n\n        elevenLabsWs.on('close', () => {\n          console.log('[ElevenLabs] Disconnected');\n        });\n      } catch (error) {\n        console.error('[ElevenLabs] Setup error:', error);\n      }\n    };\n\n    // Set up ElevenLabs connection\n    setupElevenLabs();\n\n    // Handle messages from Twilio\n    ws.on('message', (message) => {\n      try {\n        const msg = JSON.parse(message);\n        if (msg.event !== 'media') {\n          console.log(`[Twilio] Received event: ${msg.event}`);\n        }\n\n        switch (msg.event) {\n          case 'start':\n            streamSid = msg.start.streamSid;\n            callSid = msg.start.callSid;\n            customParameters = msg.start.customParameters; // Store parameters\n            console.log(`[Twilio] Stream started - StreamSid: ${streamSid}, CallSid: ${callSid}`);\n            console.log('[Twilio] Start parameters:', customParameters);\n            break;\n\n          case 'media':\n            if (elevenLabsWs?.readyState === WebSocket.OPEN) {\n              const audioMessage = {\n                user_audio_chunk: Buffer.from(msg.media.payload, 'base64').toString('base64'),\n              };\n              elevenLabsWs.send(JSON.stringify(audioMessage));\n            }\n            break;\n\n          case 'stop':\n            console.log(`[Twilio] Stream ${streamSid} ended`);\n            if (elevenLabsWs?.readyState === WebSocket.OPEN) {\n              elevenLabsWs.close();\n            }\n            break;\n\n          default:\n            console.log(`[Twilio] Unhandled event: ${msg.event}`);\n        }\n      } catch (error) {\n        console.error('[Twilio] Error processing message:', error);\n      }\n    });\n\n    // Handle WebSocket closure\n    ws.on('close', () => {\n      console.log('[Twilio] Client disconnected');\n      if (elevenLabsWs?.readyState === WebSocket.OPEN) {\n        elevenLabsWs.close();\n      }\n    });\n  });\n});\n\n// Start the Fastify server\nfastify.listen({ port: PORT }, (err) => {\n  if (err) {\n    console.error('Error starting server:', err);\n    process.exit(1);\n  }\n  console.log(`[Server] Listening on port ${PORT}`);\n});"
        }
      ],
      "relevance": 0.97
    },
    {
      "codeTitle": "Streaming Audio with ElevenLabs Python API",
      "codeDescription": "This Python code demonstrates how to use the ElevenLabs API for streaming audio. It shows two options: playing the streamed audio locally and processing the audio bytes manually.",
      "codeLanguage": "python",
      "codeTokens": 164,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/api-reference/pages/streaming.mdx#2025-04-17_snippet_0",
      "pageTitle": "ElevenLabs API Streaming Documentation",
      "codeList": [
        {
          "language": "python",
          "code": "from elevenlabs import stream\nfrom elevenlabs.client import ElevenLabs\n\nclient = ElevenLabs()\n\naudio_stream = client.text_to_speech.convert_as_stream(\n    text=\"This is a test\",\n    voice_id=\"JBFqnCBsd6RMkjVDRZzb\",\n    model_id=\"eleven_multilingual_v2\"\n)\n\n# option 1: play the streamed audio locally\nstream(audio_stream)\n\n# option 2: process the audio bytes manually\nfor chunk in audio_stream:\n    if isinstance(chunk, bytes):\n        print(chunk)"
        }
      ],
      "relevance": 0.97
    },
    {
      "codeTitle": "Main Application Implementation",
      "codeDescription": "Complete implementation showing the integration of text-to-speech and S3 upload functionality",
      "codeLanguage": "multiple",
      "codeTokens": 160,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/streaming.mdx#2025-04-17_snippet_8",
      "pageTitle": "ElevenLabs Text-to-Speech Integration Guide",
      "codeList": [
        {
          "language": "python",
          "code": "import os\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nfrom text_to_speech_stream import text_to_speech_stream\nfrom s3_uploader import upload_audiostream_to_s3, generate_presigned_url\n\n\ndef main():\n    text = \"This is James\"\n\n    audio_stream = text_to_speech_stream(text)\n    s3_file_name = upload_audiostream_to_s3(audio_stream)\n    signed_url = generate_presigned_url(s3_file_name)\n\n    print(f\"Signed URL to access the file: {signed_url}\")\n\n\nif __name__ == \"__main__\":\n    main()"
        },
        {
          "language": "typescript",
          "code": "import 'dotenv/config';\n\nimport { generatePresignedUrl, uploadAudioStreamToS3 } from './s3_uploader';\nimport { createAudioFileFromText } from './text_to_speech_file';\nimport { createAudioStreamFromText } from './text_to_speech_stream';\n\n(async () => {\n  // save the audio file to disk\n  const fileName = await createAudioFileFromText(\n    'Today, the sky is exceptionally clear, and the sun shines brightly.'\n  );\n\n  console.log('File name:', fileName);\n\n  // OR stream the audio, upload to S3, and get a presigned URL\n  const stream = await createAudioStreamFromText(\n    'Today, the sky is exceptionally clear, and the sun shines brightly.'\n  );\n\n  const s3path = await uploadAudioStreamToS3(stream);\n\n  const presignedUrl = await generatePresignedUrl(s3path);\n\n  console.log('Presigned URL:', presignedUrl);\n})();"
        }
      ],
      "relevance": 0.97
    },
    {
      "codeTitle": "Implementing API Route for Signed URL in TypeScript",
      "codeDescription": "TypeScript code for an API route that generates a signed URL for authenticating private agents. This route securely fetches the signed URL from the ElevenLabs API using server-side credentials.",
      "codeLanguage": "typescript",
      "codeTokens": 226,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/nextjs.mdx#2025-04-17_snippet_6",
      "pageTitle": "Implementing Conversational AI with Next.js and ElevenLabs",
      "codeList": [
        {
          "language": "typescript",
          "code": "import { NextResponse } from 'next/server';\n\nexport async function GET() {\n  try {\n    const response = await fetch(\n      `https://api.elevenlabs.io/v1/convai/conversation/get_signed_url?agent_id=${process.env.NEXT_PUBLIC_AGENT_ID}`,\n      {\n        headers: {\n          'xi-api-key': process.env.ELEVENLABS_API_KEY!,\n        },\n      }\n    );\n\n    if (!response.ok) {\n      throw new Error('Failed to get signed URL');\n    }\n\n    const data = await response.json();\n    return NextResponse.json({ signedUrl: data.signed_url });\n  } catch (error) {\n    return NextResponse.json(\n      { error: 'Failed to generate signed URL' },\n      { status: 500 }\n    );\n  }\n}"
        }
      ],
      "relevance": 0.97
    },
    {
      "codeTitle": "Dubbing Audio Files with ElevenLabs API in Python",
      "codeDescription": "Python implementation for audio file dubbing using ElevenLabs API. The script downloads an audio file, initiates the dubbing process to Spanish, polls for completion, and plays the dubbed audio. Requires dotenv for environment variables and the ElevenLabs SDK.",
      "codeLanguage": "python",
      "codeTokens": 320,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/dubbing/quickstart.mdx#2025-04-17_snippet_0",
      "pageTitle": "Audio Dubbing Guide with ElevenLabs API",
      "codeList": [
        {
          "language": "python",
          "code": "# example.py\nimport os\nfrom dotenv import load_dotenv\nfrom elevenlabs.client import ElevenLabs\nfrom elevenlabs import play\nimport requests\nfrom io import BytesIO\nimport time\n\nload_dotenv()\n\nclient = ElevenLabs(\n  api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n)\n\ntarget_lang = \"es\"  # Spanish\n\naudio_url = (\n    \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/nicole.mp3\"\n)\nresponse = requests.get(audio_url)\n\naudio_data = BytesIO(response.content)\naudio_data.name = \"audio.mp3\"\n\n# Start dubbing\ndubbed = client.dubbing.dub_a_video_or_an_audio_file(\n    file=audio_data, target_lang=target_lang\n)\n\nwhile True:\n    status = client.dubbing.get_dubbing_project_metadata(dubbed.dubbing_id).status\n    if status == \"dubbed\":\n        dubbed_file = client.dubbing.get_dubbed_file(dubbed.dubbing_id, target_lang)\n        play(dubbed_file)\n        break\n    else:\n        print(\"Audio is still being dubbed...\")\n        time.sleep(5)"
        }
      ],
      "relevance": 0.968
    },
    {
      "codeTitle": "Implementing Custom LLM Server with FastAPI",
      "codeDescription": "A FastAPI server implementation that replicates OpenAI's chat completion endpoint. It handles streaming responses and includes error handling for LLM interactions.",
      "codeLanguage": "python",
      "codeTokens": 488,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/custom-llm/overview.mdx#2025-04-17_snippet_0",
      "pageTitle": "Custom LLM Integration Guide for ElevenLabs",
      "codeList": [
        {
          "language": "python",
          "code": "import json\nimport os\nimport fastapi\nfrom fastapi.responses import StreamingResponse\nfrom openai import AsyncOpenAI\nimport uvicorn\nimport logging\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel\nfrom typing import List, Optional\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Retrieve API key from environment\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\nif not OPENAI_API_KEY:\n    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n\napp = fastapi.FastAPI()\noai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n\nclass Message(BaseModel):\n    role: str\n    content: str\n\nclass ChatCompletionRequest(BaseModel):\n    messages: List[Message]\n    model: str\n    temperature: Optional[float] = 0.7\n    max_tokens: Optional[int] = None\n    stream: Optional[bool] = False\n    user_id: Optional[str] = None\n\n@app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(request: ChatCompletionRequest) -> StreamingResponse:\n    oai_request = request.dict(exclude_none=True)\n    if \"user_id\" in oai_request:\n        oai_request[\"user\"] = oai_request.pop(\"user_id\")\n\n    chat_completion_coroutine = await oai_client.chat.completions.create(**oai_request)\n\n    async def event_stream():\n        try:\n            async for chunk in chat_completion_coroutine:\n                # Convert the ChatCompletionChunk to a dictionary before JSON serialization\n                chunk_dict = chunk.model_dump()\n                yield f\"data: {json.dumps(chunk_dict)}\\n\\n\"\n            yield \"data: [DONE]\\n\\n\"\n        except Exception as e:\n            logging.error(\"An error occurred: %s\", str(e))\n            yield f\"data: {json.dumps({'error': 'Internal error occurred!'})}\n\n    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8013)"
        }
      ],
      "relevance": 0.968
    },
    {
      "codeTitle": "Text-to-Speech File Conversion Implementation",
      "codeDescription": "Functions to convert text to speech and save as MP3 file. Includes voice customization settings and file handling in both Python and TypeScript.",
      "codeLanguage": "python",
      "codeTokens": 291,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/streaming.mdx#2025-04-17_snippet_2",
      "pageTitle": "ElevenLabs Text-to-Speech Integration Guide",
      "codeList": [
        {
          "language": "python",
          "code": "import os\nimport uuid\nfrom elevenlabs import VoiceSettings\nfrom elevenlabs.client import ElevenLabs\n\nELEVENLABS_API_KEY = os.getenv(\"ELEVENLABS_API_KEY\")\nclient = ElevenLabs(\n    api_key=ELEVENLABS_API_KEY,\n)\n\n\ndef text_to_speech_file(text: str) -> str:\n    response = client.text_to_speech.convert(\n        voice_id=\"pNInz6obpgDQGcFmaJgB\",\n        output_format=\"mp3_22050_32\",\n        text=text,\n        model_id=\"eleven_turbo_v2_5\",\n        voice_settings=VoiceSettings(\n            stability=0.0,\n            similarity_boost=1.0,\n            style=0.0,\n            use_speaker_boost=True,\n            speed=1.0,\n        ),\n    )\n\n    save_file_path = f\"{uuid.uuid4()}.mp3\"\n\n    with open(save_file_path, \"wb\") as f:\n        for chunk in response:\n            if chunk:\n                f.write(chunk)\n\n    print(f\"{save_file_path}: A new audio file was saved successfully!\")\n\n    return save_file_path"
        },
        {
          "language": "typescript",
          "code": "import * as dotenv from 'dotenv';\nimport { ElevenLabsClient } from 'elevenlabs';\nimport { createWriteStream } from 'fs';\nimport { v4 as uuid } from 'uuid';\n\ndotenv.config();\n\nconst ELEVENLABS_API_KEY = process.env.ELEVENLABS_API_KEY;\n\nconst client = new ElevenLabsClient({\n  apiKey: ELEVENLABS_API_KEY,\n});\n\nexport const createAudioFileFromText = async (text: string): Promise<string> => {\n  return new Promise<string>(async (resolve, reject) => {\n    try {\n      const audio = await client.textToSpeech.convert('JBFqnCBsd6RMkjVDRZzb', {\n        model_id: 'eleven_multilingual_v2',\n        text,\n        output_format: 'mp3_44100_128',\n        voice_settings: {\n          stability: 0,\n          similarity_boost: 0,\n          use_speaker_boost: true,\n          speed: 1.0,\n        },\n      });\n\n      const fileName = `${uuid()}.mp3`;\n      const fileStream = createWriteStream(fileName);\n\n      audio.pipe(fileStream);\n      fileStream.on('finish', () => resolve(fileName));\n      fileStream.on('error', reject);\n    } catch (error) {\n      reject(error);\n    }\n  });\n};"
        }
      ],
      "relevance": 0.968
    },
    {
      "codeTitle": "Creating Voice Clone with TypeScript SDK",
      "codeDescription": "TypeScript implementation for creating a voice clone using the ElevenLabs API. Uses environment configuration and file system operations to handle audio file streaming. Creates a voice clone and outputs the voice ID.",
      "codeLanguage": "typescript",
      "codeTokens": 170,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/clone-voice.mdx#2025-04-17_snippet_1",
      "pageTitle": "Voice Cloning API Documentation",
      "codeList": [
        {
          "language": "typescript",
          "code": "// example.mts\nimport { ElevenLabsClient } from \"elevenlabs\";\nimport \"dotenv/config\";\nimport fs from \"node:fs\";\n\nconst client = new ElevenLabsClient();\n\nconst voice = await client.voices.add({\n    name: \"My Voice Clone\",\n    // Replace with the paths to your audio files.\n    // The more files you add, the better the clone will be.\n    files: [\n        fs.createReadStream(\n            \"/path/to/your/audio/file.mp3\",\n        ),\n    ],\n});\n\nconsole.log(voice.voice_id);"
        }
      ],
      "relevance": 0.965
    },
    {
      "codeTitle": "Implementing Agent Transfer in JavaScript with ElevenLabs API",
      "codeDescription": "JavaScript code for setting up agent transfer functionality using the ElevenLabs API. The code shows how to define transfer rules and create an agent with the 'transfer_to_agent' system tool configured.",
      "codeLanguage": "javascript",
      "codeTokens": 283,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/agent-transfer.mdx#2025-04-17_snippet_2",
      "pageTitle": "Configuring Agent-to-Agent Transfer in ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "import { ElevenLabs } from 'elevenlabs';\n\n// Initialize the client\nconst client = new ElevenLabs({\n  apiKey: 'YOUR_API_KEY',\n});\n\n// Define transfer rules\nconst transferRules = [\n  { agent_id: 'AGENT_ID_1', condition: 'When the user asks for billing support.' },\n  { agent_id: 'AGENT_ID_2', condition: 'When the user requests advanced technical help.' },\n];\n\n// Create the agent with the transfer tool\nawait client.conversationalAi.createAgent({\n  conversation_config: {\n    agent: {\n      prompt: {\n        prompt: 'You are a helpful assistant.',\n        first_message: 'Hi, how can I help you today?',\n        tools: [\n          {\n            type: 'system',\n            name: 'transfer_to_agent',\n            description: 'Transfer the user to a specialized agent based on their request.', // Optional custom description\n            params: {\n              system_tool_type: 'transfer_to_agent',\n              transfers: transferRules,\n            },\n          },\n        ],\n      },\n    },\n  },\n});"
        }
      ],
      "relevance": 0.965
    },
    {
      "codeTitle": "Telegram Bot Implementation in TypeScript",
      "codeDescription": "Complete TypeScript implementation of the Telegram bot including webhook handling, audio transcription, and database logging using ElevenLabs API and Supabase.",
      "codeLanguage": "typescript",
      "codeTokens": 1025,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#2025-04-17_snippet_4",
      "pageTitle": "Building a Telegram Transcription Bot with ElevenLabs and TypeScript",
      "codeList": [
        {
          "language": "typescript",
          "code": "import { Bot, webhookCallback } from 'https://deno.land/x/grammy@v1.34.0/mod.ts';\nimport 'jsr:@supabase/functions-js/edge-runtime.d.ts';\nimport { createClient } from 'jsr:@supabase/supabase-js@2';\nimport { ElevenLabsClient } from 'npm:elevenlabs@1.50.5';\n\nconsole.log(`Function \"elevenlabs-scribe-bot\" up and running!`);\n\nconst elevenLabsClient = new ElevenLabsClient({\n  apiKey: Deno.env.get('ELEVENLABS_API_KEY') || '',\n});\n\nconst supabase = createClient(\n  Deno.env.get('SUPABASE_URL') || '',\n  Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') || ''\n);\n\nasync function scribe({\n  fileURL,\n  fileType,\n  duration,\n  chatId,\n  messageId,\n  username,\n}: {\n  fileURL: string;\n  fileType: string;\n  duration: number;\n  chatId: number;\n  messageId: number;\n  username: string;\n}) {\n  let transcript: string | null = null;\n  let languageCode: string | null = null;\n  let errorMsg: string | null = null;\n  try {\n    const sourceFileArrayBuffer = await fetch(fileURL).then((res) => res.arrayBuffer());\n    const sourceBlob = new Blob([sourceFileArrayBuffer], {\n      type: fileType,\n    });\n\n    const scribeResult = await elevenLabsClient.speechToText.convert({\n      file: sourceBlob,\n      model_id: 'scribe_v1',\n      tag_audio_events: false,\n    });\n\n    transcript = scribeResult.text;\n    languageCode = scribeResult.language_code;\n\n    await bot.api.sendMessage(chatId, transcript, {\n      reply_parameters: { message_id: messageId },\n    });\n  } catch (error) {\n    errorMsg = error.message;\n    console.log(errorMsg);\n    await bot.api.sendMessage(chatId, 'Sorry, there was an error. Please try again.', {\n      reply_parameters: { message_id: messageId },\n    });\n  }\n  const logLine = {\n    file_type: fileType,\n    duration,\n    chat_id: chatId,\n    message_id: messageId,\n    username,\n    language_code: languageCode,\n    error: errorMsg,\n  };\n  console.log({ logLine });\n  await supabase.from('transcription_logs').insert({ ...logLine, transcript });\n}\n\nconst telegramBotToken = Deno.env.get('TELEGRAM_BOT_TOKEN');\nconst bot = new Bot(telegramBotToken || '');\nconst startMessage = `Welcome to the ElevenLabs Scribe Bot\\! I can transcribe speech in 99 languages with super high accuracy\\!\n    \\nTry it out by sending or forwarding me a voice message, video, or audio file\\!\n    \\n[Learn more about Scribe](https://elevenlabs.io/speech-to-text) or [build your own bot](https://elevenlabs.io/docs/cookbooks/speech-to-text/telegram-bot)\\!\n  `;\nbot.command('start', (ctx) => ctx.reply(startMessage.trim(), { parse_mode: 'MarkdownV2' }));\n\nbot.on([':voice', ':audio', ':video'], async (ctx) => {\n  try {\n    const file = await ctx.getFile();\n    const fileURL = `https://api.telegram.org/file/bot${telegramBotToken}/${file.file_path}`;\n    const fileMeta = ctx.message?.video ?? ctx.message?.voice ?? ctx.message?.audio;\n\n    if (!fileMeta) {\n      return ctx.reply('No video|audio|voice metadata found. Please try again.');\n    }\n\n    EdgeRuntime.waitUntil(\n      scribe({\n        fileURL,\n        fileType: fileMeta.mime_type!,\n        duration: fileMeta.duration,\n        chatId: ctx.chat.id,\n        messageId: ctx.message?.message_id!,\n        username: ctx.from?.username || '',\n      })\n    );\n\n    return ctx.reply('Received. Scribing...');\n  } catch (error) {\n    console.error(error);\n    return ctx.reply(\n      'Sorry, there was an error getting the file. Please try again with a smaller file!'\n    );\n  }\n});\n\nconst handleUpdate = webhookCallback(bot, 'std/http');\n\nDeno.serve(async (req) => {\n  try {\n    const url = new URL(req.url);\n    if (url.searchParams.get('secret') !== Deno.env.get('FUNCTION_SECRET')) {\n      return new Response('not allowed', { status: 405 });\n    }\n\n    return await handleUpdate(req);\n  } catch (err) {\n    console.error(err);\n  }\n});"
        }
      ],
      "relevance": 0.965
    },
    {
      "codeTitle": "Implementing Dynamic Variables in JavaScript for ElevenLabs Conversational Agent",
      "codeDescription": "JavaScript implementation showing how to start a conversation with ElevenLabs agent and pass dynamic variables. The code is part of a VoiceAgent class that handles the conversation session and includes microphone access.",
      "codeLanguage": "javascript",
      "codeTokens": 192,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/dynamic-variables.mdx#2025-04-17_snippet_1",
      "pageTitle": "Dynamic Variables for ElevenLabs Conversational Agents",
      "codeList": [
        {
          "language": "javascript",
          "code": "import { Conversation } from '@11labs/client';\n\nclass VoiceAgent {\n  ...\n\n  async startConversation() {\n    try {\n        // Request microphone access\n        await navigator.mediaDevices.getUserMedia({ audio: true });\n\n        this.conversation = await Conversation.startSession({\n            agentId: 'agent_id_goes_here', // Replace with your actual agent ID\n\n            dynamicVariables: {\n                user_name: 'Angelo'\n            },\n\n            ... add some callbacks here\n        });\n    } catch (error) {\n        console.error('Failed to start conversation:', error);\n        alert('Failed to start conversation. Please ensure microphone access is granted.');\n    }\n  }\n}"
        }
      ],
      "relevance": 0.965
    },
    {
      "codeTitle": "Create Dub from File Function",
      "codeDescription": "Function to initiate dubbing process for local audio or video files with specified source and target languages.",
      "codeLanguage": "python",
      "codeTokens": 174,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/dubbing/basics.mdx#2025-04-17_snippet_2",
      "pageTitle": "ElevenLabs Dubbing API Documentation",
      "codeList": [
        {
          "language": "python",
          "code": "def create_dub_from_file(\n    input_file_path: str,\n    file_format: str,\n    source_language: str,\n    target_language: str,\n) -> Optional[str]:\n    if not os.path.isfile(input_file_path):\n        raise FileNotFoundError(f\"The input file does not exist: {input_file_path}\")\n\n    with open(input_file_path, \"rb\") as audio_file:\n        response = client.dubbing.dub_a_video_or_an_audio_file(\n            file=(os.path.basename(input_file_path), audio_file, file_format),\n            target_lang=target_language,\n            source_lang=source_language,\n            num_speakers=1,\n            watermark=False,\n        )"
        }
      ],
      "relevance": 0.965
    },
    {
      "codeTitle": "Past Generation Conditioning Implementation in Python",
      "codeDescription": "Implementation of request stitching using previous_request_ids parameter to condition on past generations. Maintains request history and concatenates audio segments.",
      "codeLanguage": "python",
      "codeTokens": 474,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/request-stitching.mdx#2025-04-17_snippet_2",
      "pageTitle": "Request Stitching Documentation for ElevenLabs API",
      "codeList": [
        {
          "language": "python",
          "code": "import os\nimport requests\nfrom pydub import AudioSegment\nimport io\n\nYOUR_XI_API_KEY = \"<insert your xi-api-key here>\"\nVOICE_ID = \"21m00Tcm4TlvDq8ikWAM\"  # Rachel\nPARAGRAPHS = [\n    \"The advent of technology has transformed countless sectors, with education \"\n    \"standing out as one of the most significantly impacted fields.\",\n    \"In recent years, educational technology, or EdTech, has revolutionized the way \"\n    \"teachers deliver instruction and students absorb information.\",\n    \"From interactive whiteboards to individual tablets loaded with educational software, \"\n    \"technology has opened up new avenues for learning that were previously unimaginable.\",\n    \"One of the primary benefits of technology in education is the accessibility it provides.\",\n]\nsegments = []\nprevious_request_ids = []\n\nfor i, paragraph in enumerate(PARAGRAPHS):\n    response = requests.post(\n        f\"https://api.elevenlabs.io/v1/text-to-speech/{VOICE_ID}/stream\",\n        json={\n            \"text\": paragraph,\n            \"model_id\": \"eleven_multilingual_v2\",\n            # A maximum of three next or previous history item ids can be send\n            \"previous_request_ids\": previous_request_ids[-3:],\n        },\n        headers={\"xi-api-key\": YOUR_XI_API_KEY},\n    )\n\n    if response.status_code != 200:\n        print(f\"Error encountered, status: {response.status_code}, \"\n               f\"content: {response.text}\")\n        quit()\n\n    print(f\"Successfully converted paragraph {i + 1}/{len(PARAGRAPHS)}\")\n    previous_request_ids.append(response.headers[\"request-id\"])\n    segments.append(AudioSegment.from_mp3(io.BytesIO(response.content)))\n\nsegment = segments[0]\nfor new_segment in segments[1:]:\n    segment = segment + new_segment\n\naudio_out_path = os.path.join(os.getcwd(), \"with_previous_request_ids_conditioning.wav\")\nsegment.export(audio_out_path, format=\"wav\")\nprint(f\"Success! Wrote audio to {audio_out_path}\")"
        }
      ],
      "relevance": 0.965
    },
    {
      "codeTitle": "Uploading Documents to ElevenLabs Knowledge Base",
      "codeDescription": "Server action implementation for handling document uploads to the ElevenLabs knowledge base. Uses Next.js 15's after function for background processing and includes Redis storage for conversation and knowledge base data.",
      "codeLanguage": "tsx",
      "codeTokens": 473,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#2025-04-17_snippet_6",
      "pageTitle": "Data Collection and Analysis with Conversational AI in Next.js",
      "codeList": [
        {
          "language": "tsx",
          "code": "'use server';\n\nimport { Redis } from '@upstash/redis';\nimport { ElevenLabsClient } from 'elevenlabs';\nimport { redirect } from 'next/navigation';\nimport { after } from 'next/server';\n\n// Initialize Redis\nconst redis = Redis.fromEnv();\n\nconst elevenLabsClient = new ElevenLabsClient({\n  apiKey: process.env.ELEVENLABS_API_KEY,\n});\n\nexport async function uploadFormData(formData: FormData) {\n  const knowledgeBase: Array<{\n    id: string;\n    type: 'file' | 'url';\n    name: string;\n  }> = [];\n  const files = formData.getAll('file-upload') as File[];\n  const email = formData.get('email-input');\n  const urls = formData.getAll('url-input');\n  const conversationId = formData.get('conversation-id');\n\n  after(async () => {\n    // Upload files as background job\n    // Create knowledge base entries\n    // Loop trhough files and create knowledge base entries\n    for (const file of files) {\n      if (file.size > 0) {\n        const response = await elevenLabsClient.conversationalAi.addToKnowledgeBase({ file });\n        if (response.id) {\n          knowledgeBase.push({\n            id: response.id,\n            type: 'file',\n            name: file.name,\n          });\n        }\n      }\n    }\n    // Append all urls\n    for (const url of urls) {\n      const response = await elevenLabsClient.conversationalAi.addToKnowledgeBase({\n        url: url as string,\n      });\n      if (response.id) {\n        knowledgeBase.push({\n          id: response.id,\n          type: 'url',\n          name: `url for ${conversationId}`,\n        });\n      }\n    }\n\n    // Store knowledge base IDs and conversation ID in database.\n    const redisRes = await redis.set(\n      conversationId as string,\n      JSON.stringify({ email, knowledgeBase })\n    );\n    console.log({ redisRes });\n  });\n\n  redirect('/success');\n}"
        }
      ],
      "relevance": 0.965
    },
    {
      "codeTitle": "Implementing Express Server for ElevenLabs-Twilio Integration",
      "codeDescription": "TypeScript code for creating an Express server that handles incoming Twilio calls, generates speech using ElevenLabs, and sends the audio back through a WebSocket connection.",
      "codeLanguage": "TypeScript",
      "codeTokens": 627,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/twilio.mdx#2025-04-17_snippet_1",
      "pageTitle": "Integrating ElevenLabs Text-to-Speech with Twilio Phone Calls",
      "codeList": [
        {
          "language": "typescript",
          "code": "// src/app.ts\nimport 'dotenv/config';\nimport { ElevenLabsClient } from 'elevenlabs';\nimport express, { Response } from 'express';\nimport ExpressWs from 'express-ws';\nimport { Readable } from 'stream';\nimport VoiceResponse from 'twilio/lib/twiml/VoiceResponse';\nimport { type WebSocket } from 'ws';\n\nconst app = ExpressWs(express()).app;\nconst PORT: number = parseInt(process.env.PORT || '5000');\n\nconst elevenlabs = new ElevenLabsClient();\nconst voiceId = '21m00Tcm4TlvDq8ikWAM';\nconst outputFormat = 'ulaw_8000';\nconst text = 'This is a test. You can now hang up. Thank you.';\n\nfunction startApp() {\n  app.post('/call/incoming', (_, res: Response) => {\n    const twiml = new VoiceResponse();\n\n    twiml.connect().stream({\n      url: `wss://${process.env.SERVER_DOMAIN}/call/connection`,\n    });\n\n    res.writeHead(200, { 'Content-Type': 'text/xml' });\n    res.end(twiml.toString());\n  });\n\n  app.ws('/call/connection', (ws: WebSocket) => {\n    ws.on('message', async (data: string) => {\n      const message: {\n        event: string;\n        start?: { streamSid: string; callSid: string };\n      } = JSON.parse(data);\n\n      if (message.event === 'start' && message.start) {\n        const streamSid = message.start.streamSid;\n        const response = await elevenlabs.textToSpeech.convert(voiceId, {\n          model_id: 'eleven_flash_v2_5',\n          output_format: outputFormat,\n          text,\n        });\n\n        const readableStream = Readable.from(response);\n        const audioArrayBuffer = await streamToArrayBuffer(readableStream);\n\n        ws.send(\n          JSON.stringify({\n            streamSid,\n            event: 'media',\n            media: {\n              payload: Buffer.from(audioArrayBuffer as any).toString('base64'),\n            },\n          })\n        );\n      }\n    });\n\n    ws.on('error', console.error);\n  });\n\n  app.listen(PORT, () => {\n    console.log(`Local: http://localhost:${PORT}`);\n    console.log(`Remote: https://${process.env.SERVER_DOMAIN}`);\n  });\n}\n\nfunction streamToArrayBuffer(readableStream: Readable) {\n  return new Promise((resolve, reject) => {\n    const chunks: Buffer[] = [];\n\n    readableStream.on('data', (chunk) => {\n      chunks.push(chunk);\n    });\n\n    readableStream.on('end', () => {\n      resolve(Buffer.concat(chunks).buffer);\n    });\n\n    readableStream.on('error', reject);\n  });\n}\n\nstartApp();"
        }
      ],
      "relevance": 0.965
    },
    {
      "codeTitle": "Python Text Normalization for TTS",
      "codeDescription": "A Python function that uses regular expressions and the inflect library to normalize text for TTS. It handles monetary values with different currency symbols and phone numbers, converting them to word representations that are easier for TTS systems to pronounce correctly.",
      "codeLanguage": "python",
      "codeTokens": 552,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/normalization.mdx#2025-04-17_snippet_1",
      "pageTitle": "Text to Speech Normalization Guide",
      "codeList": [
        {
          "language": "python",
          "code": "# Be sure to install the inflect library before running this code\nimport inflect\nimport re\n\n# Initialize inflect engine for number-to-word conversion\np = inflect.engine()\n\ndef normalize_text(text: str) -> str:\n    # Convert monetary values\n    def money_replacer(match):\n        currency_map = {\"$\": \"dollars\", \"£\": \"pounds\", \"€\": \"euros\", \"¥\": \"yen\"}\n        currency_symbol, num = match.groups()\n\n        # Remove commas before parsing\n        num_without_commas = num.replace(',', '')\n\n        # Check for decimal points to handle cents\n        if '.' in num_without_commas:\n            dollars, cents = num_without_commas.split('.')\n            dollars_in_words = p.number_to_words(int(dollars))\n            cents_in_words = p.number_to_words(int(cents))\n            return f\"{dollars_in_words} {currency_map.get(currency_symbol, 'currency')} and {cents_in_words} cents\"\n        else:\n            # Handle whole numbers\n            num_in_words = p.number_to_words(int(num_without_commas))\n            return f\"{num_in_words} {currency_map.get(currency_symbol, 'currency')}\"\n\n    # Regex to handle commas and decimals\n    text = re.sub(r\"([$£€¥])(\\d+(?:,\\d{3})*(?:\\.\\d{2})?)\", money_replacer, text)\n\n    # Convert phone numbers\n    def phone_replacer(match):\n        return \", \".join(\" \".join(p.number_to_words(int(digit)) for digit in group) for group in match.groups())\n\n    text = re.sub(r\"(\\d{3})-(\\d{3})-(\\d{4})\", phone_replacer, text)\n\n    return text\n\n# Example usage\nprint(normalize_text(\"$1,000\"))   # \"one thousand dollars\"\nprint(normalize_text(\"£1000\"))   # \"one thousand pounds\"\nprint(normalize_text(\"€1000\"))   # \"one thousand euros\"\nprint(normalize_text(\"¥1000\"))   # \"one thousand yen\"\nprint(normalize_text(\"$1,234.56\"))   # \"one thousand two hundred thirty-four dollars and fifty-six cents\"\nprint(normalize_text(\"555-555-5555\"))  # \"five five five, five five five, five five five five\""
        }
      ],
      "relevance": 0.963
    },
    {
      "codeTitle": "Generating Sound Effects with ElevenLabs TypeScript SDK",
      "codeDescription": "TypeScript implementation for generating sound effects using ElevenLabs API. Uses environment variables for configuration and includes audio playback functionality.",
      "codeLanguage": "typescript",
      "codeTokens": 106,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/sound-effects/quickstart.mdx#2025-04-17_snippet_1",
      "pageTitle": "Sound Effects Generation with ElevenLabs API",
      "codeList": [
        {
          "language": "typescript",
          "code": "// example.mts\nimport { ElevenLabsClient, play } from \"elevenlabs\";\nimport \"dotenv/config\";\n\nconst client = new ElevenLabsClient();\n\nconst audio = await client.textToSoundEffects.convert({\n  text: \"Cinematic Braam, Horror\",\n});\n\nawait play(audio);"
        }
      ],
      "relevance": 0.96
    },
    {
      "codeTitle": "Generating Signed URLs with JavaScript SDK",
      "codeDescription": "Server-side code to obtain a signed URL using the ElevenLabs JavaScript SDK. The function authenticates with an API key and retrieves a temporary signed WebSocket URL for client use.",
      "codeLanguage": "javascript",
      "codeTokens": 162,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/authentication.mdx#2025-04-17_snippet_1",
      "pageTitle": "Authentication for ElevenLabs Conversational AI Agents",
      "codeList": [
        {
          "language": "javascript",
          "code": "import { ElevenLabsClient } from 'elevenlabs';\n\n// Server-side code using the JavaScript SDK\nconst client = new ElevenLabsClient({ apiKey: 'your-api-key' });\nasync function getSignedUrl() {\n  try {\n    const response = await client.conversationalAi.getSignedUrl({\n      agent_id: 'your-agent-id',\n    });\n\n    return response.signed_url;\n  } catch (error) {\n    console.error('Error getting signed URL:', error);\n    throw error;\n  }\n}"
        }
      ],
      "relevance": 0.96
    },
    {
      "codeTitle": "Implementing TwilioAudioInterface for ElevenLabs Conversation",
      "codeDescription": "Defines a custom TwilioAudioInterface class that implements the AudioInterface for ElevenLabs Conversation. It handles audio input/output through WebSocket connections with Twilio, including base64 encoding/decoding of audio data.",
      "codeLanguage": "python",
      "codeTokens": 603,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-custom-server.mdx#2025-04-17_snippet_11",
      "pageTitle": "Twilio Integration Guide for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "python",
          "code": "import asyncio\nfrom typing import Callable\nimport queue\nimport threading\nimport base64\nfrom elevenlabs.conversational_ai.conversation import AudioInterface\nimport websockets\n\nclass TwilioAudioInterface(AudioInterface):\n    def __init__(self, websocket):\n        self.websocket = websocket\n        self.output_queue = queue.Queue()\n        self.should_stop = threading.Event()\n        self.stream_sid = None\n        self.input_callback = None\n        self.output_thread = None\n\n    def start(self, input_callback: Callable[[bytes], None]):\n        self.input_callback = input_callback\n        self.output_thread = threading.Thread(target=self._output_thread)\n        self.output_thread.start()\n\n    def stop(self):\n        self.should_stop.set()\n        if self.output_thread:\n            self.output_thread.join(timeout=5.0)\n        self.stream_sid = None\n\n    def output(self, audio: bytes):\n        self.output_queue.put(audio)\n\n    def interrupt(self):\n        try:\n            while True:\n                _ = self.output_queue.get(block=False)\n        except queue.Empty:\n            pass\n        asyncio.run(self._send_clear_message_to_twilio())\n\n    async def handle_twilio_message(self, data):\n        try:\n            if data[\"event\"] == \"start\":\n                self.stream_sid = data[\"start\"][\"streamSid\"]\n                print(f\"Started stream with stream_sid: {self.stream_sid}\")\n            if data[\"event\"] == \"media\":\n                audio_data = base64.b64decode(data[\"media\"][\"payload\"])\n                if self.input_callback:\n                    self.input_callback(audio_data)\n        except Exception as e:\n            print(f\"Error in input_callback: {e}\")\n\n    def _output_thread(self):\n        while not self.should_stop.is_set():\n            asyncio.run(self._send_audio_to_twilio())\n\n    async def _send_audio_to_twilio(self):\n        try:\n            audio = self.output_queue.get(timeout=0.2)\n            audio_payload = base64.b64encode(audio).decode(\"utf-8\")\n            audio_delta = {\n                \"event\": \"media\",\n                \"streamSid\": self.stream_sid,\n                \"media\": {\"payload\": audio_payload},\n            }\n            await self.websocket.send_json(audio_delta)\n        except queue.Empty:\n            pass\n        except Exception as e:\n            print(f\"Error sending audio: {e}\")\n\n    async def _send_clear_message_to_twilio(self):\n        try:\n            clear_message = {\"event\": \"clear\", \"streamSid\": self.stream_sid}\n            await self.websocket.send_json(clear_message)\n        except Exception as e:\n            print(f\"Error sending clear message to Twilio: {e}\")"
        }
      ],
      "relevance": 0.96
    },
    {
      "codeTitle": "Implementing Combined Authentication with JavaScript",
      "codeDescription": "JavaScript function to create an agent with both authentication methods enabled. The code combines signed URL requirement with domain restrictions to create a two-layer authentication system.",
      "codeLanguage": "javascript",
      "codeTokens": 186,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/authentication.mdx#2025-04-17_snippet_9",
      "pageTitle": "Authentication for ElevenLabs Conversational AI Agents",
      "codeList": [
        {
          "language": "javascript",
          "code": "async function createAuthenticatedAgent(client) {\n  try {\n    const agent = await client.conversationalAi.createAgent({\n      conversation_config: {\n        agent: {\n          first_message: \"Hi. I'm an authenticated agent.\",\n        },\n      },\n      platform_settings: {\n        auth: {\n          enable_auth: true,\n          allowlist: [\n            { hostname: 'example.com' },\n            { hostname: 'app.example.com' },\n            { hostname: 'localhost:3000' },\n          ],\n        },\n      },\n    });\n\n    return agent;\n  } catch (error) {\n    console.error('Error creating agent:', error);\n    throw error;\n  }\n}"
        }
      ],
      "relevance": 0.96
    },
    {
      "codeTitle": "Streaming Speech-to-Text Implementation in JavaScript",
      "codeDescription": "JavaScript implementation for streaming speech-to-text conversion using ElevenLabs SDK, with additional configuration options for transcription.",
      "codeLanguage": "javascript",
      "codeTokens": 205,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/streaming.mdx#2025-04-17_snippet_4",
      "pageTitle": "Speech-to-Text Streaming Guide with ElevenLabs SDK",
      "codeList": [
        {
          "language": "javascript",
          "code": "import * as dotenv from 'dotenv';\nimport { ElevenLabsClient } from 'elevenlabs';\n\ndotenv.config();\n\nconst client = new ElevenLabsClient();\n\nconst response = await fetch(\n  'https://storage.googleapis.com/eleven-public-cdn/audio/marketing/nicole.mp3'\n);\nconst audioBlob = new Blob([await response.arrayBuffer()], { type: 'audio/mp3' });\n\nconst transcription = await client.speechToText.convertAsStream({\n  file: audioBlob,\n  model_id: 'scribe_v1',\n  maximum_speakers: 1,\n  tag_audio_events: true,\n  speech_to_text_language_code: 'en',\n  transcribe_verbatim: false,\n});\n\nfor await (const chunk of transcription) {\n  console.log(chunk.text);\n}"
        }
      ],
      "relevance": 0.96
    },
    {
      "codeTitle": "Connecting to an Agent with Signed URL in Python",
      "codeDescription": "Client-side code to connect to an agent using a signed URL with the Python SDK. The code retrieves a server-generated signed URL and uses it to initiate a secure conversation session.",
      "codeLanguage": "python",
      "codeTokens": 229,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/authentication.mdx#2025-04-17_snippet_4",
      "pageTitle": "Authentication for ElevenLabs Conversational AI Agents",
      "codeList": [
        {
          "language": "python",
          "code": "# Client-side code using the Python SDK\nfrom elevenlabs.conversational_ai.conversation import (\n    Conversation,\n    AudioInterface,\n    ClientTools,\n    ConversationInitiationData\n)\nimport os\nfrom elevenlabs.client import ElevenLabs\napi_key = os.getenv(\"ELEVENLABS_API_KEY\")\n\nclient = ElevenLabs(api_key=api_key)\n\nconversation = Conversation(\n  client=client,\n  agent_id=os.getenv(\"AGENT_ID\"),\n  requires_auth=True,\n  audio_interface=AudioInterface(),\n  config=ConversationInitiationData()\n)\n\nasync def start_conversation():\n  try:\n    signed_url = await get_signed_url()\n    conversation = Conversation(\n      client=client,\n      url=signed_url,\n    )\n\n    conversation.start_session()\n  except Exception as error:\n    print(f\"Failed to start conversation: {error}\")"
        }
      ],
      "relevance": 0.96
    },
    {
      "codeTitle": "Implementing Text-to-Speech Edge Function",
      "codeDescription": "Implements a Supabase Edge Function that generates speech using ElevenLabs API, streams it to the client, and caches it in Supabase Storage. It handles request parsing, audio file caching, and background upload tasks.",
      "codeLanguage": "typescript",
      "codeTokens": 797,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/supabase-streaming.mdx#2025-04-17_snippet_5",
      "pageTitle": "Streaming and Caching Speech with Supabase and ElevenLabs",
      "codeList": [
        {
          "language": "typescript",
          "code": "// Setup type definitions for built-in Supabase Runtime APIs\nimport 'jsr:@supabase/functions-js/edge-runtime.d.ts';\nimport { createClient } from 'jsr:@supabase/supabase-js@2';\nimport { ElevenLabsClient } from 'npm:elevenlabs';\nimport * as hash from 'npm:object-hash';\n\nconst supabase = createClient(\n  Deno.env.get('SUPABASE_URL')!,\n  Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!\n);\n\nconst client = new ElevenLabsClient({\n  apiKey: Deno.env.get('ELEVENLABS_API_KEY'),\n});\n\n// Upload audio to Supabase Storage in a background task\nasync function uploadAudioToStorage(stream: ReadableStream, requestHash: string) {\n  const { data, error } = await supabase.storage\n    .from('audio')\n    .upload(`${requestHash}.mp3`, stream, {\n      contentType: 'audio/mp3',\n    });\n\n  console.log('Storage upload result', { data, error });\n}\n\nDeno.serve(async (req) => {\n  // To secure your function for production, you can for example validate the request origin,\n  // or append a user access token and validate it with Supabase Auth.\n  console.log('Request origin', req.headers.get('host'));\n  const url = new URL(req.url);\n  const params = new URLSearchParams(url.search);\n  const text = params.get('text');\n  const voiceId = params.get('voiceId') ?? 'JBFqnCBsd6RMkjVDRZzb';\n\n  const requestHash = hash.MD5({ text, voiceId });\n  console.log('Request hash', requestHash);\n\n  // Check storage for existing audio file\n  const { data } = await supabase.storage.from('audio').createSignedUrl(`${requestHash}.mp3`, 60);\n\n  if (data) {\n    console.log('Audio file found in storage', data);\n    const storageRes = await fetch(data.signedUrl);\n    if (storageRes.ok) return storageRes;\n  }\n\n  if (!text) {\n    return new Response(JSON.stringify({ error: 'Text parameter is required' }), {\n      status: 400,\n      headers: { 'Content-Type': 'application/json' },\n    });\n  }\n\n  try {\n    console.log('ElevenLabs API call');\n    const response = await client.textToSpeech.convertAsStream(voiceId, {\n      output_format: 'mp3_44100_128',\n      model_id: 'eleven_multilingual_v2',\n      text,\n    });\n\n    const stream = new ReadableStream({\n      async start(controller) {\n        for await (const chunk of response) {\n          controller.enqueue(chunk);\n        }\n        controller.close();\n      },\n    });\n\n    // Branch stream to Supabase Storage\n    const [browserStream, storageStream] = stream.tee();\n\n    // Upload to Supabase Storage in the background\n    EdgeRuntime.waitUntil(uploadAudioToStorage(storageStream, requestHash));\n\n    // Return the streaming response immediately\n    return new Response(browserStream, {\n      headers: {\n        'Content-Type': 'audio/mpeg',\n      },\n    });\n  } catch (error) {\n    console.log('error', { error });\n    return new Response(JSON.stringify({ error: error.message }), {\n      status: 500,\n      headers: { 'Content-Type': 'application/json' },\n    });\n  }\n});"
        }
      ],
      "relevance": 0.96
    },
    {
      "codeTitle": "Generating Voice Previews with ElevenLabs API in TypeScript",
      "codeDescription": "This snippet shows how to use the ElevenLabs TypeScript SDK to generate voice previews based on a description and text, then play each preview. It requires the ElevenLabs SDK, dotenv for environment variables, and node's stream and buffer modules.",
      "codeLanguage": "typescript",
      "codeTokens": 265,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/voice-design.mdx#2025-04-17_snippet_1",
      "pageTitle": "Voice Design API Quickstart Guide",
      "codeList": [
        {
          "language": "typescript",
          "code": "// example.ts\nimport { ElevenLabsClient, play } from \"elevenlabs\";\nimport \"dotenv/config\";\nimport { Readable } from 'node:stream';\nimport { Buffer } from 'node:buffer';\n\nconst client = new ElevenLabsClient();\n\nconst { previews } = await client.textToVoice.createPreviews({\n    voice_description: \"A huge giant, at least as tall as a building. A deep booming voice, loud and jolly.\",\n    text: \"Greetings little human. I am a mighty giant from a far away land. Would you like me to tell you a story?\",\n});\n\nfor (const preview of previews) {\n    // Convert base64 to buffer and create a Readable stream\n    const audioStream = Readable.from(Buffer.from(preview.audio_base_64, 'base64'));\n\n    console.log(`Playing preview: ${preview.generated_voice_id}`);\n\n    // Play the audio using the stream\n    await play(audioStream);\n}"
        }
      ],
      "relevance": 0.957
    },
    {
      "codeTitle": "Implementing Dynamic Variables in Swift for ElevenLabs Conversational Agent",
      "codeDescription": "Swift code example showing how to create a session configuration with different types of dynamic variables (string, number, int, boolean) and initialize a conversation with ElevenLabs agent.",
      "codeLanguage": "swift",
      "codeTokens": 166,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/dynamic-variables.mdx#2025-04-17_snippet_2",
      "pageTitle": "Dynamic Variables for ElevenLabs Conversational Agents",
      "codeList": [
        {
          "language": "swift",
          "code": "let dynamicVars: [String: DynamicVariableValue] = [\n  \"customer_name\": .string(\"John Doe\"),\n  \"account_balance\": .number(5000.50),\n  \"user_id\": .int(12345),\n  \"is_premium\": .boolean(true)\n]\n\n// Create session config with dynamic variables\nlet config = SessionConfig(\n    agentId: \"your_agent_id\",\n    dynamicVariables: dynamicVars\n)\n\n// Start the conversation\nlet conversation = try await Conversation.startSession(\n    config: config\n)"
        }
      ],
      "relevance": 0.955
    },
    {
      "codeTitle": "Testing Together AI API with Llama 3.1 Model via cURL",
      "codeDescription": "This code snippet demonstrates how to test API connectivity to Together AI by sending a simple chat completion request to a Llama 3.1 model. It uses a bearer token for authentication and sends a basic greeting message to verify the API is working correctly.",
      "codeLanguage": "bash",
      "codeTokens": 160,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/custom-llm/together-ai.mdx#2025-04-17_snippet_0",
      "pageTitle": "Using Together AI with ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "bash",
          "code": "curl https://api.together.xyz/v1/chat/completions -s \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer <API_KEY>\" \\\n-d '{\n\"model\": \"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n\"messages\": [{\n    \"role\": \"user\",\n    \"content\": \"Hello, how are you?\"\n}]\n}'"
        }
      ],
      "relevance": 0.955
    },
    {
      "codeTitle": "Updating Conversation Component for Signed URL in TypeScript",
      "codeDescription": "Modified TypeScript code for the Conversation component, incorporating the fetching and use of a signed URL for authenticating private agents.",
      "codeLanguage": "typescript",
      "codeTokens": 229,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/nextjs.mdx#2025-04-17_snippet_7",
      "pageTitle": "Implementing Conversational AI with Next.js and ElevenLabs",
      "codeList": [
        {
          "language": "typescript",
          "code": "// ... existing imports ...\n\nexport function Conversation() {\n  // ... existing conversation setup ...\n  const getSignedUrl = async (): Promise<string> => {\n    const response = await fetch(\"/api/get-signed-url\");\n    if (!response.ok) {\n      throw new Error(`Failed to get signed url: ${response.statusText}`);\n    }\n    const { signedUrl } = await response.json();\n    return signedUrl;\n  };\n\n  const startConversation = useCallback(async () => {\n    try {\n      // Request microphone permission\n      await navigator.mediaDevices.getUserMedia({ audio: true });\n\n      const signedUrl = await getSignedUrl();\n\n      // Start the conversation with your signed url\n      await conversation.startSession({\n        signedUrl,\n      });\n\n    } catch (error) {\n      console.error('Failed to start conversation:', error);\n    }\n  }, [conversation]);\n\n  // ... rest of the component ...\n}"
        }
      ],
      "relevance": 0.955
    },
    {
      "codeTitle": "Server-side Signed URL Generation for ElevenLabs API",
      "codeDescription": "Node.js server code example for generating a signed URL using the ElevenLabs API. This is necessary for conversations requiring authorization. The code includes error handling and uses environment variables for sensitive data.",
      "codeLanguage": "javascript",
      "codeTokens": 214,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/javascript.mdx#2025-04-17_snippet_4",
      "pageTitle": "JavaScript SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "app.get('/signed-url', yourAuthMiddleware, async (req, res) => {\n  const response = await fetch(\n    `https://api.elevenlabs.io/v1/convai/conversation/get_signed_url?agent_id=${process.env.AGENT_ID}`,\n    {\n      method: 'GET',\n      headers: {\n        // Requesting a signed url requires your ElevenLabs API key\n        // Do NOT expose your API key to the client!\n        'xi-api-key': process.env.XI_API_KEY,\n      },\n    }\n  );\n\n  if (!response.ok) {\n    return res.status(500).send('Failed to get signed URL');\n  }\n\n  const body = await response.json();\n  res.send(body.signed_url);\n});"
        }
      ],
      "relevance": 0.955
    },
    {
      "codeTitle": "Implementing WebSocket Custom Hook",
      "codeDescription": "Custom React hook that manages WebSocket connection, handles audio streaming, and processes various WebSocket events for the conversation.",
      "codeLanguage": "typescript",
      "codeTokens": 623,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/api-reference/websocket.mdx#2025-04-17_snippet_6",
      "pageTitle": "ElevenLabs WebSocket API for Conversational AI",
      "codeList": [
        {
          "language": "typescript",
          "code": "'use client';\n\nimport { useCallback, useEffect, useRef, useState } from 'react';\nimport { useVoiceStream } from 'voice-stream';\nimport type { ElevenLabsWebSocketEvent } from '../types/websocket';\n\nconst sendMessage = (websocket: WebSocket, request: object) => {\n  if (websocket.readyState !== WebSocket.OPEN) {\n    return;\n  }\n  websocket.send(JSON.stringify(request));\n};\n\nexport const useAgentConversation = () => {\n  const websocketRef = useRef<WebSocket>(null);\n  const [isConnected, setIsConnected] = useState<boolean>(false);\n\n  const { startStreaming, stopStreaming } = useVoiceStream({\n    onAudioChunked: (audioData) => {\n      if (!websocketRef.current) return;\n      sendMessage(websocketRef.current, {\n        user_audio_chunk: audioData,\n      });\n    },\n  });\n\n  const startConversation = useCallback(async () => {\n    if (isConnected) return;\n\n    const websocket = new WebSocket(\"wss://api.elevenlabs.io/v1/convai/conversation\");\n\n    websocket.onopen = async () => {\n      setIsConnected(true);\n      sendMessage(websocket, {\n        type: \"conversation_initiation_client_data\",\n      });\n      await startStreaming();\n    };\n\n    websocket.onmessage = async (event) => {\n      const data = JSON.parse(event.data) as ElevenLabsWebSocketEvent;\n\n      if (data.type === \"ping\") {\n        setTimeout(() => {\n          sendMessage(websocket, {\n            type: \"pong\",\n            event_id: data.ping_event.event_id,\n          });\n        }, data.ping_event.ping_ms);\n      }\n\n      if (data.type === \"user_transcript\") {\n        const { user_transcription_event } = data;\n        console.log(\"User transcript\", user_transcription_event.user_transcript);\n      }\n\n      if (data.type === \"agent_response\") {\n        const { agent_response_event } = data;\n        console.log(\"Agent response\", agent_response_event.agent_response);\n      }\n\n      if (data.type === \"interruption\") {\n        // Handle interruption\n      }\n\n      if (data.type === \"audio\") {\n        const { audio_event } = data;\n        // Implement your own audio playback system here\n      }\n    };\n\n    websocketRef.current = websocket;\n\n    websocket.onclose = async () => {\n      websocketRef.current = null;\n      setIsConnected(false);\n      stopStreaming();\n    };\n  }, [startStreaming, isConnected, stopStreaming]);\n\n  const stopConversation = useCallback(async () => {\n    if (!websocketRef.current) return;\n    websocketRef.current.close();\n  }, []);\n\n  useEffect(() => {\n    return () => {\n      if (websocketRef.current) {\n        websocketRef.current.close();\n      }\n    };\n  }, []);\n\n  return {\n    startConversation,\n    stopConversation,\n    isConnected,\n  };\n};"
        }
      ],
      "relevance": 0.955
    },
    {
      "codeTitle": "Generating Signed URLs with Python SDK",
      "codeDescription": "Server-side code to obtain a signed URL using the ElevenLabs Python SDK. The function requests a temporary authenticated URL for secure client-side connections without exposing API keys.",
      "codeLanguage": "python",
      "codeTokens": 140,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/authentication.mdx#2025-04-17_snippet_0",
      "pageTitle": "Authentication for ElevenLabs Conversational AI Agents",
      "codeList": [
        {
          "language": "python",
          "code": "# Server-side code using the Python SDK\nfrom elevenlabs.client import ElevenLabs\nasync def get_signed_url():\n    try:\n        client = ElevenLabs(api_key=\"your-api-key\")\n        response = await client.conversational_ai.get_signed_url(agent_id=\"your-agent-id\")\n        return response.signed_url\n    except Exception as error:\n        print(f\"Error getting signed URL: {error}\")\n        raise"
        }
      ],
      "relevance": 0.955
    },
    {
      "codeTitle": "Adding Generated Voice to Library in Python",
      "codeDescription": "This snippet demonstrates how to add a generated voice preview to the user's voice library using the ElevenLabs Python SDK. It creates a new voice from a selected preview and returns the new voice ID.",
      "codeLanguage": "python",
      "codeTokens": 151,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/voice-design.mdx#2025-04-17_snippet_2",
      "pageTitle": "Voice Design API Quickstart Guide",
      "codeList": [
        {
          "language": "python",
          "code": "voice = client.text_to_voice.create_voice_from_preview(\n    voice_name=\"Jolly giant\",\n    voice_description=\"A huge giant, at least as tall as a building. A deep booming voice, loud and jolly.\",\n    # The generated voice ID of the preview you want to use,\n    # using the first in the list for this example\n    generated_voice_id=voices.previews[0].generated_voice_id\n)\n\nprint(voice.voice_id)"
        }
      ],
      "relevance": 0.955
    },
    {
      "codeTitle": "Dubbing Audio Files with ElevenLabs API in TypeScript",
      "codeDescription": "TypeScript implementation for audio file dubbing using ElevenLabs API. The script fetches an audio file, converts it to a Blob, initiates the dubbing process to Spanish, and monitors completion before playback. Requires the ElevenLabs SDK and dotenv configuration.",
      "codeLanguage": "typescript",
      "codeTokens": 329,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/dubbing/quickstart.mdx#2025-04-17_snippet_1",
      "pageTitle": "Audio Dubbing Guide with ElevenLabs API",
      "codeList": [
        {
          "language": "typescript",
          "code": "// example.mts\nimport { ElevenLabsClient, play } from \"elevenlabs\";\nimport \"dotenv/config\";\n\nconst client = new ElevenLabsClient();\n\nconst targetLang = \"es\"; // spanish\nconst sourceAudio = await fetch(\n  \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/nicole.mp3\"\n);\nconst audioBlob = new Blob([await sourceAudio.arrayBuffer()], {\n  type: \"audio/mp3\",\n});\n\n// Start dubbing\nconst dubbed = await client.dubbing.dubAVideoOrAnAudioFile({\n  file: audioBlob,\n  target_lang: targetLang,\n});\n\nwhile (true) {\n  const { status } = await client.dubbing.getDubbingProjectMetadata(\n    dubbed.dubbing_id\n  );\n  if (status === \"dubbed\") {\n    const dubbedFile = await client.dubbing.getDubbedFile(\n      dubbed.dubbing_id,\n      targetLang\n    );\n    await play(dubbedFile);\n    break;\n  } else {\n    console.log(\"Audio is still being dubbed...\");\n  }\n\n  // Wait 5 seconds between checks\n  await new Promise((resolve) => setTimeout(resolve, 5000));\n}"
        }
      ],
      "relevance": 0.955
    },
    {
      "codeTitle": "Defining WebSocket Event Types",
      "codeDescription": "TypeScript type definitions for various WebSocket events including user transcript, agent response, audio response, interruption, and ping events.",
      "codeLanguage": "typescript",
      "codeTokens": 264,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/api-reference/websocket.mdx#2025-04-17_snippet_5",
      "pageTitle": "ElevenLabs WebSocket API for Conversational AI",
      "codeList": [
        {
          "language": "typescript",
          "code": "type BaseEvent = {\n  type: string;\n};\n\ntype UserTranscriptEvent = BaseEvent & {\n  type: \"user_transcript\";\n  user_transcription_event: {\n    user_transcript: string;\n  };\n};\n\ntype AgentResponseEvent = BaseEvent & {\n  type: \"agent_response\";\n  agent_response_event: {\n    agent_response: string;\n  };\n};\n\ntype AudioResponseEvent = BaseEvent & {\n  type: \"audio\";\n  audio_event: {\n    audio_base_64: string;\n    event_id: number;\n  };\n};\n\ntype InterruptionEvent = BaseEvent & {\n  type: \"interruption\";\n  interruption_event: {\n    reason: string;\n  };\n};\n\ntype PingEvent = BaseEvent & {\n  type: \"ping\";\n  ping_event: {\n    event_id: number;\n    ping_ms?: number;\n  };\n};\n\nexport type ElevenLabsWebSocketEvent =\n  | UserTranscriptEvent\n  | AgentResponseEvent\n  | AudioResponseEvent\n  | InterruptionEvent\n  | PingEvent;"
        }
      ],
      "relevance": 0.955
    },
    {
      "codeTitle": "Implementing Language Detection in JavaScript with ElevenLabs API",
      "codeDescription": "Example of creating a Conversational AI agent with language detection capabilities using the ElevenLabs JavaScript SDK. Shows how to configure language presets and system tools for multiple languages including Dutch, Finnish, Turkish, Russian, Portuguese, and Arabic.",
      "codeLanguage": "javascript",
      "codeTokens": 515,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/language-detection.mdx#2025-04-17_snippet_1",
      "pageTitle": "Language Detection System Tool Documentation",
      "codeList": [
        {
          "language": "javascript",
          "code": "import { ElevenLabs } from 'elevenlabs';\n\n// Initialize the client\nconst client = new ElevenLabs({\n  apiKey: 'YOUR_API_KEY',\n});\n\n// Create the agent with language detection tool\nawait client.conversationalAi.createAgent({\n  conversation_config: {\n    agent: {\n      prompt: {\n        tools: [\n          {\n            type: 'system',\n            name: 'language_detection',\n            description: '', // Optional: Customize when the tool should be triggered\n          },\n        ],\n        first_message: 'Hi, how are you?',\n      },\n    },\n    language_presets: {\n      nl: {\n        overrides: {\n          agent: {\n            prompt: null,\n            first_message: 'Hoi, hoe gaat het met je?',\n            language: null,\n          },\n          tts: null,\n        },\n      },\n      fi: {\n        overrides: {\n          agent: {\n            prompt: null,\n            first_message: 'Hei, kuinka voit?',\n            language: null,\n          },\n          tts: null,\n        },\n        first_message_translation: {\n          source_hash: '{\"firstMessage\":\"Hi how are you?\",\"language\":\"en\"}',\n          text: 'Hei, kuinka voit?',\n        },\n      },\n      tr: {\n        overrides: {\n          agent: {\n            prompt: null,\n            first_message: 'Merhaba, nasılsın?',\n            language: null,\n          },\n          tts: null,\n        },\n      },\n      ru: {\n        overrides: {\n          agent: {\n            prompt: null,\n            first_message: 'Привет, как ты?',\n            language: null,\n          },\n          tts: null,\n        },\n      },\n      pt: {\n        overrides: {\n          agent: {\n            prompt: null,\n            first_message: 'Oi, como você está?',\n            language: null,\n          },\n          tts: null,\n        },\n      },\n      ar: {\n        overrides: {\n          agent: {\n            prompt: null,\n            first_message: 'مرحبًا كيف حالك؟',\n            language: null,\n          },\n          tts: null,\n        },\n      },\n    },\n  },\n});"
        }
      ],
      "relevance": 0.953
    },
    {
      "codeTitle": "Voice Transformation Implementation in Python",
      "codeDescription": "Python implementation for transforming voice using ElevenLabs API. Uses dotenv for environment variables, makes HTTP requests to fetch audio, and processes it through the speech-to-speech conversion API. Requires ElevenLabs SDK and additional dependencies like MPV and ffmpeg for audio playback.",
      "codeLanguage": "python",
      "codeTokens": 247,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voice-changer/quickstart.mdx#2025-04-17_snippet_0",
      "pageTitle": "Voice Changer API Implementation Guide",
      "codeList": [
        {
          "language": "python",
          "code": "# example.py\nimport os\nfrom dotenv import load_dotenv\nfrom elevenlabs.client import ElevenLabs\nfrom elevenlabs import play\nimport requests\nfrom io import BytesIO\n\nload_dotenv()\n\nclient = ElevenLabs(\n  api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n)\nvoice_id = \"JBFqnCBsd6RMkjVDRZzb\"\n\naudio_url = (\n    \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/nicole.mp3\"\n)\nresponse = requests.get(audio_url)\naudio_data = BytesIO(response.content)\n\naudio_stream = client.speech_to_speech.convert(\n    voice_id=voice_id,\n    audio=audio_data,\n    model_id=\"eleven_multilingual_sts_v2\",\n    output_format=\"mp3_44100_128\",\n)\n\nplay(audio_stream)"
        }
      ],
      "relevance": 0.952
    },
    {
      "codeTitle": "Complete Dubbing Implementation",
      "codeDescription": "Complete implementation combining file upload, progress monitoring, and download functionality.",
      "codeLanguage": "python",
      "codeTokens": 222,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/dubbing/basics.mdx#2025-04-17_snippet_5",
      "pageTitle": "ElevenLabs Dubbing API Documentation",
      "codeList": [
        {
          "language": "python",
          "code": "def create_dub_from_file(\n    input_file_path: str,\n    file_format: str,\n    source_language: str,\n    target_language: str,\n) -> Optional[str]:\n    if not os.path.isfile(input_file_path):\n        raise FileNotFoundError(f\"The input file does not exist: {input_file_path}\")\n\n    with open(input_file_path, \"rb\") as audio_file:\n        response = client.dubbing.dub_a_video_or_an_audio_file(\n            file=(os.path.basename(input_file_path), audio_file, file_format),\n            target_lang=target_language,\n            source_lang=source_language,\n            num_speakers=1,\n            watermark=False,\n        )\n\n    dubbing_id = response.dubbing_id\n    if wait_for_dubbing_completion(dubbing_id):\n        output_file_path = download_dubbed_file(dubbing_id, target_language)\n        return output_file_path\n    else:\n        return None"
        }
      ],
      "relevance": 0.95
    },
    {
      "codeTitle": "Generating signed URL for authorized Conversational AI agents",
      "codeDescription": "Server-side code to generate a signed URL for authorized Conversational AI agents using the ElevenLabs API.",
      "codeLanguage": "js",
      "codeTokens": 173,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/react.mdx#2025-04-17_snippet_5",
      "pageTitle": "React SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "js",
          "code": "// your server\nconst requestHeaders: HeadersInit = new Headers();\nrequestHeaders.set(\"xi-api-key\", process.env.XI_API_KEY); // use your ElevenLabs API key\n\nconst response = await fetch(\n  \"https://api.elevenlabs.io/v1/convai/conversation/get_signed_url?agent_id={{agent id created through ElevenLabs UI}}\",\n  {\n    method: \"GET\",\n    headers: requestHeaders,\n  }\n);\n\nif (!response.ok) {\n  return Response.error();\n}\n\nconst body = await response.json();\nconst url = body.signed_url; // use this URL for startSession method."
        }
      ],
      "relevance": 0.95
    },
    {
      "codeTitle": "Implementing Webhook Response for Twilio Call Personalization",
      "codeDescription": "Example JSON response structure for the webhook endpoint that provides conversation initiation data including dynamic variables and configuration overrides. The response customizes the agent's behavior based on caller information.",
      "codeLanguage": "json",
      "codeTokens": 194,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/dynamic-variables/twilio-inbound-integration.mdx#2025-04-17_snippet_0",
      "pageTitle": "Twilio Webhook Integration with ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "json",
          "code": "{\n  \"type\": \"conversation_initiation_client_data\",\n  \"dynamic_variables\": {\n    \"customer_name\": \"John Doe\",\n    \"account_status\": \"premium\",\n    \"last_interaction\": \"2024-01-15\"\n  },\n  \"conversation_config_override\": {\n    \"agent\": {\n      \"prompt\": {\n        \"prompt\": \"The customer's bank account balance is $100. They are based in San Francisco.\"\n      },\n      \"first_message\": \"Hi, how can I help you today?\",\n      \"language\": \"en\"\n    },\n    \"tts\": {\n      \"voice_id\": \"new-voice-id\"\n    }\n  }\n}"
        }
      ],
      "relevance": 0.95
    },
    {
      "codeTitle": "Creating Backend Server for ElevenLabs Signed URL",
      "codeDescription": "Node.js server implementation to generate signed URLs for authenticating private ElevenLabs agents, including API key handling and error management.",
      "codeLanguage": "javascript",
      "codeTokens": 278,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/vite.mdx#2025-04-17_snippet_8",
      "pageTitle": "Creating a Conversational AI Web Client with Vite and ElevenLabs",
      "codeList": [
        {
          "language": "javascript",
          "code": "require(\"dotenv\").config();\n\nconst express = require(\"express\");\nconst cors = require(\"cors\");\n\nconst app = express();\napp.use(cors());\napp.use(express.json());\n\nconst PORT = process.env.PORT || 3001;\n\napp.get(\"/api/get-signed-url\", async (req, res) => {\n    try {\n        const response = await fetch(\n            `https://api.elevenlabs.io/v1/convai/conversation/get_signed_url?agent_id=${process.env.AGENT_ID}`,\n            {\n                headers: {\n                    \"xi-api-key\": process.env.ELEVENLABS_API_KEY,\n                },\n            }\n        );\n\n        if (!response.ok) {\n            throw new Error(\"Failed to get signed URL\");\n        }\n\n        const data = await response.json();\n        res.json({ signedUrl: data.signed_url });\n    } catch (error) {\n        console.error(\"Error:\", error);\n        res.status(500).json({ error: \"Failed to generate signed URL\" });\n    }\n});\n\napp.listen(PORT, () => {\n    console.log(`Server running on http://localhost:${PORT}`);\n});"
        }
      ],
      "relevance": 0.95
    },
    {
      "codeTitle": "Implementing Zero Retention Mode in Text-to-Speech API (Python)",
      "codeDescription": "This Python code snippet demonstrates how to use the ElevenLabs client to make a Text-to-Speech API call with Zero Retention Mode enabled. It sets the 'enable_logging' parameter to False to activate Zero Retention Mode.",
      "codeLanguage": "python",
      "codeTokens": 141,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/resources/zero-retention.mdx#2025-04-17_snippet_0",
      "pageTitle": "Zero Retention Mode Documentation for ElevenLabs Enterprise",
      "codeList": [
        {
          "language": "python",
          "code": "from elevenlabs import ElevenLabs\n\nclient = ElevenLabs(\n  api_key=\"YOUR_API_KEY\",\n)\n\nresponse = client.text_to_speech.convert(\n  voice_id=voice_id,\n  output_format=\"mp3_22050_32\",\n  text=text,\n  model_id=\"eleven_turbo_v2\",\n  enable_logging=False,\n)"
        }
      ],
      "relevance": 0.95
    },
    {
      "codeTitle": "Initializing Node.js Project for ElevenLabs-Twilio Integration",
      "codeDescription": "Commands to create a new project directory, initialize npm, and install necessary dependencies for the ElevenLabs-Twilio integration.",
      "codeLanguage": "Shell",
      "codeTokens": 67,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/twilio.mdx#2025-04-17_snippet_0",
      "pageTitle": "Integrating ElevenLabs Text-to-Speech with Twilio Phone Calls",
      "codeList": [
        {
          "language": "shell",
          "code": "mkdir elevenlabs-twilio\ncd elevenlabs-twilio\nnpm init -y"
        },
        {
          "language": "shell",
          "code": "npm install elevenlabs express express-ws twilio"
        },
        {
          "language": "shell",
          "code": "npm i @types/node @types/express @types/express-ws @types/ws dotenv tsx typescript"
        }
      ],
      "relevance": 0.95
    },
    {
      "codeTitle": "Implementing Voice Isolation in TypeScript",
      "codeDescription": "TypeScript implementation for removing background noise from an audio file using ElevenLabs Voice Isolator API. Uses fetch for downloading sample audio and processes it through the isolation API. Requires dotenv for environment configuration and elevenlabs SDK.",
      "codeLanguage": "typescript",
      "codeTokens": 178,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voice-isolator/quickstart.mdx#2025-04-17_snippet_1",
      "pageTitle": "Voice Isolator API Integration Guide",
      "codeList": [
        {
          "language": "typescript",
          "code": "// example.mts\nimport { ElevenLabsClient, play } from \"elevenlabs\";\nimport \"dotenv/config\";\n\nconst client = new ElevenLabsClient();\n\nconst audioUrl =\n  \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/fin.mp3\";\nconst response = await fetch(audioUrl);\nconst audioBlob = new Blob([await response.arrayBuffer()], {\n  type: \"audio/mp3\",\n});\n\nconst audioStream = await client.audioIsolation.audioIsolation({\n  audio: audioBlob,\n});\n\nawait play(audioStream);"
        }
      ],
      "relevance": 0.95
    },
    {
      "codeTitle": "Implementing Combined Authentication with Python",
      "codeDescription": "Python code for creating an agent with both authentication methods enabled. This example combines signed URL authentication (enable_auth=True) with domain allowlisting for maximum security.",
      "codeLanguage": "python",
      "codeTokens": 206,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/authentication.mdx#2025-04-17_snippet_8",
      "pageTitle": "Authentication for ElevenLabs Conversational AI Agents",
      "codeList": [
        {
          "language": "python",
          "code": "from elevenlabs.client import ElevenLabs\nimport os\nfrom elevenlabs.types import *\napi_key = os.getenv(\"ELEVENLABS_API_KEY\")\nclient = ElevenLabs(api_key=api_key)\nagent = client.conversational_ai.create_agent(\n  conversation_config=ConversationalConfig(\n    agent=AgentConfig(\n      first_message=\"Hi. I'm an authenticated agent that can only be called from certain domains.\",\n    )\n  ),\n  platform_settings=AgentPlatformSettingsRequestModel(\n  auth=AuthSettings(\n    enable_auth=True,\n    allowlist=[\n      AllowlistItem(hostname=\"example.com\"),\n      AllowlistItem(hostname=\"app.example.com\"),\n      AllowlistItem(hostname=\"localhost:3000\")\n      ]\n    )\n  )\n)"
        }
      ],
      "relevance": 0.95
    },
    {
      "codeTitle": "Streaming Audio with ElevenLabs Node.js/TypeScript API",
      "codeDescription": "This Node.js/TypeScript code demonstrates how to use the ElevenLabs API for streaming audio. It shows two options: playing the streamed audio locally and processing the audio manually.",
      "codeLanguage": "javascript",
      "codeTokens": 195,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/api-reference/pages/streaming.mdx#2025-04-17_snippet_1",
      "pageTitle": "ElevenLabs API Streaming Documentation",
      "codeList": [
        {
          "language": "javascript",
          "code": "import { ElevenLabsClient, stream } from 'elevenlabs';\nimport { Readable } from 'stream';\n\nconst client = new ElevenLabsClient();\n\nasync function main() {\n  const audioStream = await client.textToSpeech.convertAsStream('JBFqnCBsd6RMkjVDRZzb', {\n    text: 'This is a test',\n    model_id: 'eleven_multilingual_v2',\n  });\n\n  // option 1: play the streamed audio locally\n  await stream(Readable.from(audioStream));\n\n  // option 2: process the audio manually\n  for await (const chunk of audioStream) {\n    console.log(chunk);\n  }\n}\n\nmain();"
        }
      ],
      "relevance": 0.948
    },
    {
      "codeTitle": "Customizing Voice Settings in ElevenLabs WebSocket API",
      "codeDescription": "Examples of how to customize voice characteristics such as stability and similarity boost when initializing the WebSocket connection. These settings control the quality and consistency of the generated audio.",
      "codeLanguage": "multiple",
      "codeTokens": 100,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/websockets.mdx#2025-04-17_snippet_13",
      "pageTitle": "WebSocket Streaming for Real-Time Audio Generation with ElevenLabs API",
      "codeList": [
        {
          "language": "python",
          "code": "await websocket.send(json.dumps({\n    \"text\": text,\n    \"voice_settings\": {\"stability\": 0.5, \"similarity_boost\": 0.8, \"use_speaker_boost\": False},\n}))"
        },
        {
          "language": "typescript",
          "code": "websocket.send(\n  JSON.stringify({\n    text: text,\n    voice_settings: { stability: 0.5, similarity_boost: 0.8, use_speaker_boost: false },\n  })\n);"
        }
      ],
      "relevance": 0.945
    },
    {
      "codeTitle": "ElevenLabs Transcription Integration with Supabase Logging",
      "codeDescription": "Implements file transcription using ElevenLabs API and stores results in Supabase. Handles transcription processing, error handling, and database logging.",
      "codeLanguage": "typescript",
      "codeTokens": 465,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#2025-04-17_snippet_7",
      "pageTitle": "Building a Telegram Transcription Bot with ElevenLabs and TypeScript",
      "codeList": [
        {
          "language": "typescript",
          "code": "const elevenLabsClient = new ElevenLabsClient({\n  apiKey: Deno.env.get('ELEVENLABS_API_KEY') || '',\n});\n\nconst supabase = createClient(\n  Deno.env.get('SUPABASE_URL') || '',\n  Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') || ''\n);\n\nasync function scribe({\n  fileURL,\n  fileType,\n  duration,\n  chatId,\n  messageId,\n  username,\n}: {\n  fileURL: string;\n  fileType: string;\n  duration: number;\n  chatId: number;\n  messageId: number;\n  username: string;\n}) {\n  let transcript: string | null = null;\n  let languageCode: string | null = null;\n  let errorMsg: string | null = null;\n  try {\n    const sourceFileArrayBuffer = await fetch(fileURL).then((res) => res.arrayBuffer());\n    const sourceBlob = new Blob([sourceFileArrayBuffer], {\n      type: fileType,\n    });\n\n    const scribeResult = await elevenLabsClient.speechToText.convert({\n      file: sourceBlob,\n      model_id: 'scribe_v1',\n      tag_audio_events: false,\n    });\n\n    transcript = scribeResult.text;\n    languageCode = scribeResult.language_code;\n\n    await bot.api.sendMessage(chatId, transcript, {\n      reply_parameters: { message_id: messageId },\n    });\n  } catch (error) {\n    errorMsg = error.message;\n    console.log(errorMsg);\n    await bot.api.sendMessage(chatId, 'Sorry, there was an error. Please try again.', {\n      reply_parameters: { message_id: messageId },\n    });\n  }\n  const logLine = {\n    file_type: fileType,\n    duration,\n    chat_id: chatId,\n    message_id: messageId,\n    username,\n    language_code: languageCode,\n    error: errorMsg,\n  };\n  console.log({ logLine });\n  await supabase.from('transcription_logs').insert({ ...logLine, transcript });\n}"
        }
      ],
      "relevance": 0.945
    },
    {
      "codeTitle": "Starting Conversation Session with Agent ID in JavaScript",
      "codeDescription": "Code snippet showing how to start a conversation session using an Agent ID obtained from the ElevenLabs UI. This method is suitable for public agents.",
      "codeLanguage": "javascript",
      "codeTokens": 68,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/javascript.mdx#2025-04-17_snippet_3",
      "pageTitle": "JavaScript SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "const conversation = await Conversation.startSession({\n  agentId: '<your-agent-id>',\n});"
        }
      ],
      "relevance": 0.945
    },
    {
      "codeTitle": "Embedding ElevenLabs Conversational AI Agent in HTML",
      "codeDescription": "The HTML code required to embed an ElevenLabs Conversational AI agent into a webpage. It consists of two parts: the agent element with a unique ID and the script that loads the widget functionality.",
      "codeLanguage": "html",
      "codeTokens": 103,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/wordpress.mdx#2025-04-17_snippet_0",
      "pageTitle": "Conversational AI Integration in WordPress",
      "codeList": [
        {
          "language": "html",
          "code": "<elevenlabs-convai agent-id=\"YOUR_AGENT_ID\"></elevenlabs-convai>\n<script src=\"https://elevenlabs.io/convai-widget/index.js\" async type=\"text/javascript\"></script>"
        }
      ],
      "relevance": 0.945
    },
    {
      "codeTitle": "Streaming Speech-to-Text Implementation in Python",
      "codeDescription": "Python implementation for streaming speech-to-text conversion using ElevenLabs SDK, including audio file handling and transcription streaming.",
      "codeLanguage": "python",
      "codeTokens": 202,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/streaming.mdx#2025-04-17_snippet_3",
      "pageTitle": "Speech-to-Text Streaming Guide with ElevenLabs SDK",
      "codeList": [
        {
          "language": "python",
          "code": "import os\nfrom typing import IO\nfrom io import BytesIO\nfrom elevenlabs.client import ElevenLabs\nimport requests\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\nclient = ElevenLabs(\n    api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n)\n\naudio_url = (\n    \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/nicole.mp3\"\n)\nresponse = requests.get(audio_url)\naudio_data = BytesIO(response.content)\n\n# Perform the text-to-speech conversion\ntranscription = client.speech_to_text.convert_as_stream(\n    model_id=\"scribe_v1\",\n    file=audio_data\n)\n\nfor chunk in transcription:\n    # If you want to extract just the text:\n    print(chunk.text)"
        }
      ],
      "relevance": 0.945
    },
    {
      "codeTitle": "Checking for Existing Audio in Supabase Storage",
      "codeDescription": "This code checks if an audio file already exists in Supabase Storage using a generated hash. If found, it returns the file from storage.",
      "codeLanguage": "typescript",
      "codeTokens": 131,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/supabase-streaming.mdx#2025-04-17_snippet_7",
      "pageTitle": "Streaming and Caching Speech with Supabase and ElevenLabs",
      "codeList": [
        {
          "language": "typescript",
          "code": "const { data } = await supabase\n  .storage\n  .from(\"audio\")\n  .createSignedUrl(`${requestHash}.mp3`, 60);\n\nif (data) {\n  console.log(\"Audio file found in storage\", data);\n  const storageRes = await fetch(data.signedUrl);\n  if (storageRes.ok) return storageRes;\n}"
        }
      ],
      "relevance": 0.945
    },
    {
      "codeTitle": "Handling Telegram Webhook Requests in TypeScript",
      "codeDescription": "Sets up a Deno server to handle incoming webhook requests from Telegram. Includes authentication via a secret parameter and routes requests to the bot's update handler.",
      "codeLanguage": "typescript",
      "codeTokens": 147,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#2025-04-17_snippet_5",
      "pageTitle": "Building a Telegram Transcription Bot with ElevenLabs and TypeScript",
      "codeList": [
        {
          "language": "typescript",
          "code": "const handleUpdate = webhookCallback(bot, 'std/http');\n\nDeno.serve(async (req) => {\n  try {\n    const url = new URL(req.url);\n    if (url.searchParams.get('secret') !== Deno.env.get('FUNCTION_SECRET')) {\n      return new Response('not allowed', { status: 405 });\n    }\n\n    return await handleUpdate(req);\n  } catch (err) {\n    console.error(err);\n  }\n});"
        }
      ],
      "relevance": 0.945
    },
    {
      "codeTitle": "CRM Integration Webhook Handler",
      "codeDescription": "JavaScript implementation of a webhook handler that processes ElevenLabs post-call data and updates a CRM system with conversation details.",
      "codeLanguage": "javascript",
      "codeTokens": 172,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/workflows/post-call-webhook.mdx#2025-04-17_snippet_1",
      "pageTitle": "ElevenLabs Post-Call Webhooks Documentation",
      "codeList": [
        {
          "language": "javascript",
          "code": "app.post('/webhook/elevenlabs', async (req, res) => {\n  // HMAC validation code\n\n  const { data } = req.body;\n\n  // Extract key information\n  const userId = data.metadata.user_id;\n  const transcriptSummary = data.analysis.transcript_summary;\n  const callSuccessful = data.analysis.call_successful;\n\n  // Update CRM record\n  await updateCustomerRecord(userId, {\n    lastInteraction: new Date(),\n    conversationSummary: transcriptSummary,\n    callOutcome: callSuccessful,\n    fullTranscript: data.transcript,\n  });\n\n  res.status(200).send('Webhook received');\n});"
        }
      ],
      "relevance": 0.945
    },
    {
      "codeTitle": "Validating ElevenLabs Webhook with Express.js",
      "codeDescription": "Implements a webhook handler using Express.js to validate the ElevenLabs-Signature header, including timestamp and HMAC signature verification.",
      "codeLanguage": "javascript",
      "codeTokens": 320,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/webhook-hmac-authentication.mdx#2025-04-17_snippet_2",
      "pageTitle": "Validating ElevenLabs Webhooks",
      "codeList": [
        {
          "language": "javascript",
          "code": "const crypto = require('crypto');\nconst secret = process.env.WEBHOOK_SECRET;\nconst bodyParser = require('body-parser');\n\n// Ensure express js is parsing the raw body through instead of applying it's own encoding\napp.use(bodyParser.raw({ type: '*/*' }));\n\n// Example webhook handler\napp.post('/webhook/elevenlabs', async (req, res) => {\n  const headers = req.headers['ElevenLabs-Signature'].split(',');\n  const timestamp = headers.find((e) => e.startsWith('t=')).substring(2);\n  const signature = headers.find((e) => e.startsWith('v0='));\n\n  // Validate timestamp\n  const reqTimestamp = timestamp * 1000;\n  const tolerance = Date.now() - 30 * 60 * 1000;\n  if (reqTimestamp < tolerance) {\n    res.status(403).send('Request expired');\n    return;\n  } else {\n    // Validate hash\n    const message = `${timestamp}.${req.body}`;\n    const digest = 'v0=' + crypto.createHmac('sha256', secret).update(message).digest('hex');\n    if (signature !== digest) {\n      res.status(401).send('Request unauthorized');\n      return;\n    }\n  }\n\n  // Validation passed, continue processing ...\n\n  res.status(200).send();\n});"
        }
      ],
      "relevance": 0.945
    },
    {
      "codeTitle": "Initializing Conversation Session with ElevenLabs in Next.js",
      "codeDescription": "Demonstrates how to initialize and manage a conversation session using the useConversation hook. Includes handling of signed URLs, microphone permissions, and UI state management through client tools.",
      "codeLanguage": "tsx",
      "codeTokens": 474,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#2025-04-17_snippet_4",
      "pageTitle": "Data Collection and Analysis with Conversational AI in Next.js",
      "codeList": [
        {
          "language": "tsx",
          "code": "import { useConversation } from '@11labs/react';\n\nasync function getSignedUrl(): Promise<string> {\n  const response = await fetch('/api/signed-url');\n  if (!response.ok) {\n    throw Error('Failed to get signed url');\n  }\n  const data = await response.json();\n  return data.signedUrl;\n}\n\nexport default function Home() {\n  // ...\n  const [currentStep, setCurrentStep] = useState<\n    'initial' | 'training' | 'voice' | 'email' | 'ready'\n  >('initial');\n  const [conversationId, setConversationId] = useState('');\n  const [userName, setUserName] = useState('');\n\n  const conversation = useConversation({\n    onConnect: () => console.log('Connected'),\n    onDisconnect: () => console.log('Disconnected'),\n    onMessage: (message: string) => console.log('Message:', message),\n    onError: (error: Error) => console.error('Error:', error),\n  });\n\n  const startConversation = useCallback(async () => {\n    try {\n      // Request microphone permission\n      await navigator.mediaDevices.getUserMedia({ audio: true });\n      // Start the conversation with your agent\n      const signedUrl = await getSignedUrl();\n      const convId = await conversation.startSession({\n        signedUrl,\n        dynamicVariables: {\n          user_name: userName,\n        },\n        clientTools: {\n          set_ui_state: ({ step }: { step: string }): string => {\n            // Allow agent to navigate the UI.\n            setCurrentStep(step as 'initial' | 'training' | 'voice' | 'email' | 'ready');\n            return `Navigated to ${step}`;\n          },\n        },\n      });\n      setConversationId(convId);\n      console.log('Conversation ID:', convId);\n    } catch (error) {\n      console.error('Failed to start conversation:', error);\n    }\n  }, [conversation, userName]);\n  const stopConversation = useCallback(async () => {\n    await conversation.endSession();\n  }, [conversation]);\n  // ...\n}"
        }
      ],
      "relevance": 0.943
    },
    {
      "codeTitle": "Initializing Conversation Session with Callbacks in Swift",
      "codeDescription": "Sets up session configuration and callback handlers for various conversation events including connect, disconnect, messages, errors, status changes, mode changes, and volume updates.",
      "codeLanguage": "swift",
      "codeTokens": 228,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/swift.mdx#2025-04-17_snippet_1",
      "pageTitle": "Swift SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "swift",
          "code": "// Configure the session\nlet config = ElevenLabsSDK.SessionConfig(agentId: \"your-agent-id\")\n\n// Set up callbacks\nvar callbacks = ElevenLabsSDK.Callbacks()\ncallbacks.onConnect = { conversationId in\n    print(\"Connected with ID: \\(conversationId)\")\n}\ncallbacks.onDisconnect = {\n    print(\"Disconnected\")\n}\ncallbacks.onMessage = { message, role in\n    print(\"\\(role.rawValue): \\(message)\")\n}\ncallbacks.onError = { error, info in\n    print(\"Error: \\(error), Info: \\(String(describing: info))\")\n}\ncallbacks.onStatusChange = { status in\n    print(\"Status changed to: \\(status.rawValue)\")\n}\ncallbacks.onModeChange = { mode in\n    print(\"Mode changed to: \\(mode.rawValue)\")\n}\ncallbacks.onVolumeUpdate = { volume in\n    print(\"Volume updated: \\(volume)\")\n}"
        }
      ],
      "relevance": 0.942
    },
    {
      "codeTitle": "Updating Client Code for Signed URL Authentication in ElevenLabs Voice Chat",
      "codeDescription": "Modified JavaScript code to fetch and use a signed URL for authenticating with private ElevenLabs agents, including error handling and conversation management.",
      "codeLanguage": "javascript",
      "codeTokens": 320,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/vite.mdx#2025-04-17_snippet_9",
      "pageTitle": "Creating a Conversational AI Web Client with Vite and ElevenLabs",
      "codeList": [
        {
          "language": "javascript",
          "code": "// ... existing imports and variables ...\n\nasync function getSignedUrl() {\n    const response = await fetch('http://localhost:3001/api/get-signed-url');\n    if (!response.ok) {\n        throw new Error(`Failed to get signed url: ${response.statusText}`);\n    }\n    const { signedUrl } = await response.json();\n    return signedUrl;\n}\n\nasync function startConversation() {\n    try {\n        await navigator.mediaDevices.getUserMedia({ audio: true });\n\n        const signedUrl = await getSignedUrl();\n\n        conversation = await Conversation.startSession({\n            signedUrl,\n            // agentId has been removed...\n            onConnect: () => {\n                connectionStatus.textContent = 'Connected';\n                startButton.disabled = true;\n                stopButton.disabled = false;\n            },\n            onDisconnect: () => {\n                connectionStatus.textContent = 'Disconnected';\n                startButton.disabled = false;\n                stopButton.disabled = true;\n            },\n            onError: (error) => {\n                console.error('Error:', error);\n            },\n            onModeChange: (mode) => {\n                agentStatus.textContent = mode.mode === 'speaking' ? 'speaking' : 'listening';\n            },\n        });\n    } catch (error) {\n        console.error('Failed to start conversation:', error);\n    }\n}\n\n// ... rest of the code ..."
        }
      ],
      "relevance": 0.94
    },
    {
      "codeTitle": "Python S3 Upload Implementation",
      "codeDescription": "Python implementation for uploading audio streams to S3 and generating presigned URLs using boto3",
      "codeLanguage": "python",
      "codeTokens": 285,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/streaming.mdx#2025-04-17_snippet_6",
      "pageTitle": "ElevenLabs Text-to-Speech Integration Guide",
      "codeList": [
        {
          "language": "python",
          "code": "import os\nimport boto3\nimport uuid\n\nAWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\nAWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\nAWS_REGION_NAME = os.getenv(\"AWS_REGION_NAME\")\nAWS_S3_BUCKET_NAME = os.getenv(\"AWS_S3_BUCKET_NAME\")\n\nsession = boto3.Session(\n    aws_access_key_id=AWS_ACCESS_KEY_ID,\n    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n    region_name=AWS_REGION_NAME,\n)\ns3 = session.client(\"s3\")\n\n\ndef generate_presigned_url(s3_file_name: str) -> str:\n    signed_url = s3.generate_presigned_url(\n        \"get_object\",\n        Params={\"Bucket\": AWS_S3_BUCKET_NAME, \"Key\": s3_file_name},\n        ExpiresIn=3600,\n    )  # URL expires in 1 hour\n    return signed_url\n\n\ndef upload_audiostream_to_s3(audio_stream) -> str:\n    s3_file_name = f\"{uuid.uuid4()}.mp3\"  # Generates a unique file name using UUID\n    s3.upload_fileobj(audio_stream, AWS_S3_BUCKET_NAME, s3_file_name)\n\n    return s3_file_name"
        }
      ],
      "relevance": 0.94
    },
    {
      "codeTitle": "Converting Text to Speech using ElevenLabs API in TypeScript",
      "codeDescription": "This TypeScript script demonstrates how to use the ElevenLabs API to convert text to speech. It initializes the client, makes a text-to-speech conversion request, and plays the resulting audio.",
      "codeLanguage": "typescript",
      "codeTokens": 155,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/quickstart.mdx#2025-04-17_snippet_1",
      "pageTitle": "ElevenLabs API Developer Quickstart",
      "codeList": [
        {
          "language": "typescript",
          "code": "import { ElevenLabsClient, play } from \"elevenlabs\";\nimport \"dotenv/config\";\n\nconst client = new ElevenLabsClient();\nconst audio = await client.textToSpeech.convert(\"JBFqnCBsd6RMkjVDRZzb\", {\n  text: \"The first move is what sets everything in motion.\",\n  model_id: \"eleven_multilingual_v2\",\n  output_format: \"mp3_44100_128\",\n});\n\nawait play(audio);"
        }
      ],
      "relevance": 0.94
    },
    {
      "codeTitle": "Combined Text and Past Generation Conditioning in Python",
      "codeDescription": "Implementation combining both text conditioning and past generation conditioning for optimal results. Uses previous_text, next_text, and previous_request_ids parameters.",
      "codeLanguage": "python",
      "codeTokens": 547,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/request-stitching.mdx#2025-04-17_snippet_3",
      "pageTitle": "Request Stitching Documentation for ElevenLabs API",
      "codeList": [
        {
          "language": "python",
          "code": "import os\nimport requests\nfrom pydub import AudioSegment\nimport io\n\nYOUR_XI_API_KEY = \"<insert your xi-api-key here>\"\nVOICE_ID = \"21m00Tcm4TlvDq8ikWAM\"  # Rachel\nPARAGRAPHS = [\n    \"The advent of technology has transformed countless sectors, with education \"\n    \"standing out as one of the most significantly impacted fields.\",\n    \"In recent years, educational technology, or EdTech, has revolutionized the way \"\n    \"teachers deliver instruction and students absorb information.\",\n    \"From interactive whiteboards to individual tablets loaded with educational software, \"\n    \"technology has opened up new avenues for learning that were previously unimaginable.\",\n    \"One of the primary benefits of technology in education is the accessibility it provides.\",\n]\nsegments = []\nprevious_request_ids = []\n\nfor i, paragraph in enumerate(PARAGRAPHS):\n    is_first_paragraph = i == 0\n    is_last_paragraph = i == len(PARAGRAPHS) - 1\n    response = requests.post(\n        f\"https://api.elevenlabs.io/v1/text-to-speech/{VOICE_ID}/stream\",\n        json={\n            \"text\": paragraph,\n            \"model_id\": \"eleven_multilingual_v2\",\n            # A maximum of three next or previous history item ids can be send\n            \"previous_request_ids\": previous_request_ids[-3:],\n            \"previous_text\": None if is_first_paragraph else \" \".join(PARAGRAPHS[:i]),\n            \"next_text\": None if is_last_paragraph else \" \".join(PARAGRAPHS[i + 1:])\n        },\n        headers={\"xi-api-key\": YOUR_XI_API_KEY},\n    )\n\n    if response.status_code != 200:\n        print(f\"Error encountered, status: {response.status_code}, \"\n               f\"content: {response.text}\")\n        quit()\n\n    print(f\"Successfully converted paragraph {i + 1}/{len(PARAGRAPHS)}\")\n    previous_request_ids.append(response.headers[\"request-id\"])\n    segments.append(AudioSegment.from_mp3(io.BytesIO(response.content)))\n\nsegment = segments[0]\nfor new_segment in segments[1:]:\n    segment = segment + new_segment\n\naudio_out_path = os.path.join(os.getcwd(), \"with_full_conditioning.wav\")\nsegment.export(audio_out_path, format=\"wav\")\nprint(f\"Success! Wrote audio to {audio_out_path}\")"
        }
      ],
      "relevance": 0.94
    },
    {
      "codeTitle": "Initiating WebSocket Connection for Text-to-Speech in TypeScript",
      "codeDescription": "Set up the WebSocket connection to the ElevenLabs text-to-speech API using TypeScript.",
      "codeLanguage": "typescript",
      "codeTokens": 305,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/websockets.mdx#2025-04-17_snippet_4",
      "pageTitle": "WebSocket Streaming for Real-Time Audio Generation with ElevenLabs API",
      "codeList": [
        {
          "language": "typescript",
          "code": "import * as dotenv from 'dotenv';\nimport * as fs from 'node:fs';\nimport WebSocket from 'ws';\n\n// Load the API key from the .env file\ndotenv.config();\nconst ELEVENLABS_API_KEY = process.env.ELEVENLABS_API_KEY;\n\nconst voiceId = 'Xb7hH8MSUJpSbSDYk0k2';\n\n// For use cases where latency is important, we recommend using the 'eleven_flash_v2_5' model.\nconst model = 'eleven_flash_v2_5';\n\nconst uri = `wss://api.elevenlabs.io/v1/text-to-speech/${voiceId}/stream-input?model_id=${model}`;\nconst websocket = new WebSocket(uri, {\n  headers: { 'xi-api-key': `${ELEVENLABS_API_KEY}` },\n});\n\n// Create a directory for saving the audio\nconst outputDir = './output';\n\ntry {\n  fs.accessSync(outputDir, fs.constants.R_OK | fs.constants.W_OK);\n} catch (err) {\n  fs.mkdirSync(outputDir);\n}\n\n// Create a write stream for saving the audio into mp3\nconst writeStream = fs.createWriteStream(outputDir + '/test.mp3', {\n  flags: 'a',\n});"
        }
      ],
      "relevance": 0.94
    },
    {
      "codeTitle": "Initializing Conversation with options in React",
      "codeDescription": "Example of initializing a Conversation instance with optional handlers for various events.",
      "codeLanguage": "tsx",
      "codeTokens": 47,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/react.mdx#2025-04-17_snippet_3",
      "pageTitle": "React SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "tsx",
          "code": "const conversation = useConversation({\n  /* options object */\n});"
        }
      ],
      "relevance": 0.94
    },
    {
      "codeTitle": "Validating ElevenLabs Webhook with Python and FastAPI",
      "codeDescription": "Implements a webhook handler using FastAPI to validate the ElevenLabs-Signature header, including timestamp and HMAC signature verification.",
      "codeLanguage": "python",
      "codeTokens": 265,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/webhook-hmac-authentication.mdx#2025-04-17_snippet_1",
      "pageTitle": "Validating ElevenLabs Webhooks",
      "codeList": [
        {
          "language": "python",
          "code": "from fastapi import FastAPI, Request\nimport time\nimport hmac\nfrom hashlib import sha256\n\napp = FastAPI()\n\n# Example webhook handler\n@app.post(\"/webhook\")\nasync def receive_message(request: Request):\n    payload = await request.body()\n    headers = request.headers.get(\"elevenlabs-signature\")\n    if headers is None:\n        return\n    timestamp = headers.split(\",\")[0][2:]\n    hmac_signature = headers.split(\",\")[1]\n\n    # Validate timestamp\n    tolerance = int(time.time()) - 30 * 60\n    if int(timestamp) < tolerance\n        return\n\n    # Validate signature\n    full_payload_to_sign = f\"{timestamp}.{payload.decode('utf-8')}\"\n    mac = hmac.new(\n        key=secret.encode(\"utf-8\"),\n        msg=full_payload_to_sign.encode(\"utf-8\"),\n        digestmod=sha256,\n    )\n    digest = 'v0=' + mac.hexdigest()\n    if hmac_signature != digest:\n        return\n\n    # Continue processing\n\n    return {\"status\": \"received\"}"
        }
      ],
      "relevance": 0.938
    },
    {
      "codeTitle": "Returning Data from Client Tools in JavaScript",
      "codeDescription": "Example showing how to create a client tool that returns customer data to the agent in JavaScript. The function fetches customer details and returns them directly to be added to the conversation context.",
      "codeLanguage": "javascript",
      "codeTokens": 147,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-tools.mdx#2025-04-17_snippet_4",
      "pageTitle": "Client Tools for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "const clientTools = {\n  getCustomerDetails: async () => {\n    // Fetch customer details (e.g., from an API)\n    const customerData = {\n      id: 123,\n      name: \"Alice\",\n      subscription: \"Pro\"\n    };\n    // Return data directly to the agent.\n    return customerData;\n  }\n};\n\n// Start the conversation with client tools configured.\nconst conversation = await Conversation.startSession({ clientTools });"
        }
      ],
      "relevance": 0.937
    },
    {
      "codeTitle": "Handling Audio Events in JavaScript",
      "codeDescription": "Examples showing the audio event structure and implementation of an audio event handler. Audio events contain base64 encoded audio for playback and include numeric event IDs for tracking.",
      "codeLanguage": "javascript",
      "codeTokens": 99,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-events.mdx#2025-04-17_snippet_2",
      "pageTitle": "Client Events Documentation for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "// Example audio event structure\n{\n  \"audio_event\": {\n    \"audio_base_64\": \"base64_encoded_audio_string\",\n    \"event_id\": 12345\n  },\n  \"type\": \"audio\"\n}"
        },
        {
          "language": "javascript",
          "code": "// Example audio event handler\nwebsocket.on('audio', (event) => {\n  const { audio_event } = event;\n  const { audio_base_64, event_id } = audio_event;\n  audioPlayer.play(audio_base_64);\n});"
        }
      ],
      "relevance": 0.935
    },
    {
      "codeTitle": "Creating Voice Clone with Python SDK",
      "codeDescription": "Python script demonstrating how to create a voice clone using the ElevenLabs API. Uses environment variables for API key management and handles audio file reading. Creates a voice clone from audio files and returns a voice ID.",
      "codeLanguage": "python",
      "codeTokens": 182,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/clone-voice.mdx#2025-04-17_snippet_0",
      "pageTitle": "Voice Cloning API Documentation",
      "codeList": [
        {
          "language": "python",
          "code": "# example.py\nimport os\nfrom dotenv import load_dotenv\nfrom elevenlabs.client import ElevenLabs\nfrom io import BytesIO\n\nload_dotenv()\n\nclient = ElevenLabs(\n  api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n)\n\nvoice = client.voices.add(\n    name=\"My Voice Clone\",\n    # Replace with the paths to your audio files.\n    # The more files you add, the better the clone will be.\n    files=[BytesIO(open(\"/path/to/your/audio/file.mp3\", \"rb\").read())]\n)\n\nprint(voice.voice_id)"
        }
      ],
      "relevance": 0.935
    },
    {
      "codeTitle": "Uploading Audio Stream to Supabase Storage",
      "codeDescription": "This function handles the background upload of the audio stream to Supabase Storage, allowing for future caching of the generated audio.",
      "codeLanguage": "typescript",
      "codeTokens": 143,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/supabase-streaming.mdx#2025-04-17_snippet_9",
      "pageTitle": "Streaming and Caching Speech with Supabase and ElevenLabs",
      "codeList": [
        {
          "language": "typescript",
          "code": "// Upload audio to Supabase Storage in a background task\nasync function uploadAudioToStorage(\n  stream: ReadableStream,\n  requestHash: string,\n) {\n  const { data, error } = await supabase.storage\n    .from(\"audio\")\n    .upload(`${requestHash}.mp3`, stream, {\n      contentType: \"audio/mp3\",\n    });\n\n  console.log(\"Storage upload result\", { data, error });\n}"
        }
      ],
      "relevance": 0.935
    },
    {
      "codeTitle": "Implementing Dynamic Variables in HTML Widget for ElevenLabs Conversational Agent",
      "codeDescription": "HTML code showing how to embed the ElevenLabs conversational agent widget with dynamic variables using the elevenlabs-convai custom element. Variables are passed as a JSON string in the dynamic-variables attribute.",
      "codeLanguage": "html",
      "codeTokens": 106,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/dynamic-variables.mdx#2025-04-17_snippet_3",
      "pageTitle": "Dynamic Variables for ElevenLabs Conversational Agents",
      "codeList": [
        {
          "language": "html",
          "code": "<elevenlabs-convai\n  agent-id=\"your-agent-id\"\n  dynamic-variables='{\"user_name\": \"John\", \"account_type\": \"premium\"}'\n></elevenlabs-convai>"
        }
      ],
      "relevance": 0.935
    },
    {
      "codeTitle": "Connecting to an Agent with Signed URL in JavaScript",
      "codeDescription": "Client-side code to connect to an agent using a signed URL with the JavaScript SDK. The function gets a signed URL from the server and starts a conversation session.",
      "codeLanguage": "javascript",
      "codeTokens": 135,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/authentication.mdx#2025-04-17_snippet_5",
      "pageTitle": "Authentication for ElevenLabs Conversational AI Agents",
      "codeList": [
        {
          "language": "javascript",
          "code": "// Client-side code using the JavaScript SDK\nimport { Conversation } from '@11labs/client';\n\nasync function startConversation() {\n  try {\n    const signedUrl = await getSignedUrl();\n    const conversation = await Conversation.startSession({\n      signedUrl,\n    });\n\n    return conversation;\n  } catch (error) {\n    console.error('Failed to start conversation:', error);\n    throw error;\n  }\n}"
        }
      ],
      "relevance": 0.935
    },
    {
      "codeTitle": "Injecting ElevenLabs Conversational AI Widget in JavaScript",
      "codeDescription": "This script injects the ElevenLabs conversational AI widget into the documentation site. It handles widget creation, theme adaptation, device responsiveness, and client tool injection. The widget uses allowlists for security and adapts to light/dark modes and mobile/desktop views.",
      "codeLanguage": "javascript",
      "codeTokens": 877,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/our-docs-agent.mdx#2025-04-17_snippet_2",
      "pageTitle": "Building the ElevenLabs Documentation Agent",
      "codeList": [
        {
          "language": "javascript",
          "code": "const ID = 'elevenlabs-convai-widget-60993087-3f3e-482d-9570-cc373770addc';\n\nfunction injectElevenLabsWidget() {\n  // Check if the widget is already loaded\n  if (document.getElementById(ID)) {\n    return;\n  }\n\n  const script = document.createElement('script');\n  script.src = 'https://elevenlabs.io/convai-widget/index.js';\n  script.async = true;\n  script.type = 'text/javascript';\n  document.head.appendChild(script);\n\n  // Create the wrapper and widget\n  const wrapper = document.createElement('div');\n  wrapper.className = 'desktop';\n\n  const widget = document.createElement('elevenlabs-convai');\n  widget.id = ID;\n  widget.setAttribute('agent-id', 'the-agent-id');\n  widget.setAttribute('variant', 'full');\n\n  // Set initial colors and variant based on current theme and device\n  updateWidgetColors(widget);\n  updateWidgetVariant(widget);\n\n  // Watch for theme changes and resize events\n  const observer = new MutationObserver(() => {\n    updateWidgetColors(widget);\n  });\n\n  observer.observe(document.documentElement, {\n    attributes: true,\n    attributeFilter: ['class'],\n  });\n\n  // Add resize listener for mobile detection\n  window.addEventListener('resize', () => {\n    updateWidgetVariant(widget);\n  });\n\n  function updateWidgetVariant(widget) {\n    const isMobile = window.innerWidth <= 640; // Common mobile breakpoint\n    if (isMobile) {\n      widget.setAttribute('variant', 'expandable');\n    } else {\n      widget.setAttribute('variant', 'full');\n    }\n  }\n\n  function updateWidgetColors(widget) {\n    const isDarkMode = !document.documentElement.classList.contains('light');\n    if (isDarkMode) {\n      widget.setAttribute('avatar-orb-color-1', '#2E2E2E');\n      widget.setAttribute('avatar-orb-color-2', '#B8B8B8');\n    } else {\n      widget.setAttribute('avatar-orb-color-1', '#4D9CFF');\n      widget.setAttribute('avatar-orb-color-2', '#9CE6E6');\n    }\n  }\n\n  // Listen for the widget's \"call\" event to inject client tools\n  widget.addEventListener('elevenlabs-convai:call', (event) => {\n    event.detail.config.clientTools = {\n      redirectToDocs: ({ path }) => {\n        const router = window?.next?.router;\n        if (router) {\n          router.push(path);\n        }\n      },\n      redirectToEmailSupport: ({ subject, body }) => {\n        const encodedSubject = encodeURIComponent(subject);\n        const encodedBody = encodeURIComponent(body);\n        window.open(\n          `mailto:team@elevenlabs.io?subject=${encodedSubject}&body=${encodedBody}`,\n          '_blank'\n        );\n      },\n      redirectToSupportForm: ({ subject, description, extraInfo }) => {\n        const baseUrl = 'https://help.elevenlabs.io/hc/en-us/requests/new';\n        const ticketFormId = '13145996177937';\n        const encodedSubject = encodeURIComponent(subject);\n        const encodedDescription = encodeURIComponent(description);\n        const encodedExtraInfo = encodeURIComponent(extraInfo);\n\n        const fullUrl = `${baseUrl}?ticket_form_id=${ticketFormId}&tf_subject=${encodedSubject}&tf_description=${encodedDescription}%3Cbr%3E%3Cbr%3E${encodedExtraInfo}`;\n\n        window.open(fullUrl, '_blank', 'noopener,noreferrer');\n      },\n      redirectToExternalURL: ({ url }) => {\n        window.open(url, '_blank', 'noopener,noreferrer');\n      },\n    };\n  });\n\n  // Attach widget to the DOM\n  wrapper.appendChild(widget);\n  document.body.appendChild(wrapper);\n}\n\nif (document.readyState === 'loading') {\n  document.addEventListener('DOMContentLoaded', injectElevenLabsWidget);\n} else {\n  injectElevenLabsWidget();\n}"
        }
      ],
      "relevance": 0.935
    },
    {
      "codeTitle": "Displaying Scribe v1 Speech-to-Text Output in JavaScript",
      "codeDescription": "This code snippet shows the JSON output structure of the Scribe v1 speech-to-text model. It includes language detection, full text transcription, and word-level details such as timing, type, and speaker identification.",
      "codeLanguage": "javascript",
      "codeTokens": 319,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/capabilities/speech-to-text.mdx#2025-04-17_snippet_3",
      "pageTitle": "ElevenLabs Speech to Text Documentation",
      "codeList": [
        {
          "language": "javascript",
          "code": "{\n  \"language_code\": \"en\",\n  \"language_probability\": 1,\n  \"text\": \"With a soft and whispery American accent, I'm the ideal choice for creating ASMR content, meditative guides, or adding an intimate feel to your narrative projects.\",\n  \"words\": [\n    {\n      \"text\": \"With\",\n      \"start\": 0.119,\n      \"end\": 0.259,\n      \"type\": \"word\",\n      \"speaker_id\": \"speaker_0\"\n    },\n    {\n      \"text\": \" \",\n      \"start\": 0.239,\n      \"end\": 0.299,\n      \"type\": \"spacing\",\n      \"speaker_id\": \"speaker_0\"\n    },\n    {\n      \"text\": \"a\",\n      \"start\": 0.279,\n      \"end\": 0.359,\n      \"type\": \"word\",\n      \"speaker_id\": \"speaker_0\"\n    },\n    // ... (truncated for brevity)\n    {\n      \"text\": \"projects.\",\n      \"start\": 13.919,\n      \"end\": 14.779,\n      \"type\": \"word\",\n      \"speaker_id\": \"speaker_0\"\n    }\n  ]\n}"
        }
      ],
      "relevance": 0.935
    },
    {
      "codeTitle": "Stateful Conversations Implementation",
      "codeDescription": "JavaScript code demonstrating how to implement stateful conversations using webhooks to store and retrieve conversation context across multiple interactions.",
      "codeLanguage": "javascript",
      "codeTokens": 284,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/workflows/post-call-webhook.mdx#2025-04-17_snippet_2",
      "pageTitle": "ElevenLabs Post-Call Webhooks Documentation",
      "codeList": [
        {
          "language": "javascript",
          "code": "// Store conversation state when call ends\napp.post('/webhook/elevenlabs', async (req, res) => {\n  // HMAC validation code\n\n  const { data } = req.body;\n  const userId = data.metadata.user_id;\n\n  // Store conversation state\n  await db.userStates.upsert({\n    userId,\n    lastConversationId: data.conversation_id,\n    lastInteractionTimestamp: data.metadata.start_time_unix_secs,\n    conversationHistory: data.transcript,\n    previousTopics: extractTopics(data.analysis.transcript_summary),\n  });\n\n  res.status(200).send('Webhook received');\n});\n\n// When initiating a new call, retrieve and use the state\nasync function initiateCall(userId) {\n  // Get user's conversation state\n  const userState = await db.userStates.findOne({ userId });\n\n  // Start new conversation with context from previous calls\n  return await elevenlabs.startConversation({\n    agent_id: 'xyz',\n    conversation_id: generateNewId(),\n    dynamic_variables: {\n      user_name: userState.name,\n      previous_conversation_id: userState.lastConversationId,\n      previous_topics: userState.previousTopics.join(', '),\n    },\n  });\n}"
        }
      ],
      "relevance": 0.935
    },
    {
      "codeTitle": "Complete SwiftUI Implementation Example",
      "codeDescription": "Full example of a SwiftUI view implementing the conversation interface with status management and UI controls.",
      "codeLanguage": "swift",
      "codeTokens": 330,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/swift.mdx#2025-04-17_snippet_7",
      "pageTitle": "Swift SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "swift",
          "code": "struct ConversationalAIView: View {\n    @State private var conversation: ElevenLabsSDK.Conversation?\n    @State private var mode: ElevenLabsSDK.Mode = .listening\n    @State private var status: ElevenLabsSDK.Status = .disconnected\n    @State private var audioLevel: Float = 0.0\n\n    private func startConversation() {\n        Task {\n            do {\n                let config = ElevenLabsSDK.SessionConfig(agentId: \"your-agent-id\")\n                var callbacks = ElevenLabsSDK.Callbacks()\n\n                callbacks.onConnect = { conversationId in\n                    status = .connected\n                }\n                callbacks.onDisconnect = {\n                    status = .disconnected\n                }\n                callbacks.onModeChange = { newMode in\n                    DispatchQueue.main.async {\n                        mode = newMode\n                    }\n                }\n                callbacks.onVolumeUpdate = { newVolume in\n                    DispatchQueue.main.async {\n                        audioLevel = newVolume\n                    }\n                }\n\n                conversation = try await ElevenLabsSDK.Conversation.startSession(\n                    config: config,\n                    callbacks: callbacks\n                )\n            } catch {\n                print(\"Failed to start conversation: \\(error)\")\n            }\n        }\n    }\n\n    var body: some View {\n        VStack {\n            // Your UI implementation\n            Button(action: startConversation) {\n                Text(status == .connected ? \"End Call\" : \"Start Call\")\n            }\n        }\n    }\n}"
        }
      ],
      "relevance": 0.935
    },
    {
      "codeTitle": "Configuring Dynamic Variables for ElevenLabs Widget in HTML",
      "codeDescription": "This HTML code shows how to set dynamic variables for the ElevenLabs Conversational AI widget. These variables can be used to inject runtime values into the agent's messages, system prompts, and tools.",
      "codeLanguage": "html",
      "codeTokens": 106,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/widget.mdx#2025-04-17_snippet_4",
      "pageTitle": "Widget Customization for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "html",
          "code": "<elevenlabs-convai\n  agent-id=\"your-agent-id\"\n  dynamic-variables='{\"user_name\": \"John\", \"account_type\": \"premium\"}'\n></elevenlabs-convai>"
        }
      ],
      "relevance": 0.933
    },
    {
      "codeTitle": "Processing Media Messages in Telegram Bot",
      "codeDescription": "Implements message handling for voice, audio, and video files. Uses background processing with EdgeRuntime.waitUntil for transcription tasks while providing immediate user feedback.",
      "codeLanguage": "typescript",
      "codeTokens": 289,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#2025-04-17_snippet_6",
      "pageTitle": "Building a Telegram Transcription Bot with ElevenLabs and TypeScript",
      "codeList": [
        {
          "language": "typescript",
          "code": "bot.on([':voice', ':audio', ':video'], async (ctx) => {\n  try {\n    const file = await ctx.getFile();\n    const fileURL = `https://api.telegram.org/file/bot${telegramBotToken}/${file.file_path}`;\n    const fileMeta = ctx.message?.video ?? ctx.message?.voice ?? ctx.message?.audio;\n\n    if (!fileMeta) {\n      return ctx.reply('No video|audio|voice metadata found. Please try again.');\n    }\n\n    // Run the transcription in the background.\n    EdgeRuntime.waitUntil(\n      scribe({\n        fileURL,\n        fileType: fileMeta.mime_type!,\n        duration: fileMeta.duration,\n        chatId: ctx.chat.id,\n        messageId: ctx.message?.message_id!,\n        username: ctx.from?.username || '',\n      })\n    );\n\n    // Reply to the user immediately to let them know we received their file.\n    return ctx.reply('Received. Scribing...');\n  } catch (error) {\n    console.error(error);\n    return ctx.reply(\n      'Sorry, there was an error getting the file. Please try again with a smaller file!'\n    );\n  }\n});"
        }
      ],
      "relevance": 0.93
    },
    {
      "codeTitle": "Implementing redirectToExternalURL Tool for ElevenLabs Conversational AI Widget in JavaScript",
      "codeDescription": "This code sets up an event listener for the ElevenLabs Conversational AI widget and implements the redirectToExternalURL client-side tool. It uses the window.open() method to open the provided URL in a new tab with security considerations.",
      "codeLanguage": "javascript",
      "codeTokens": 203,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/widget.mdx#2025-04-17_snippet_6",
      "pageTitle": "Widget Customization for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "document.addEventListener('DOMContentLoaded', () => {\n  const widget = document.querySelector('elevenlabs-convai');\n\n  if (widget) {\n    // Listen for the widget's \"call\" event to trigger client-side tools\n    widget.addEventListener('elevenlabs-convai:call', (event) => {\n      event.detail.config.clientTools = {\n        // Note: To use this example, the client tool called \"redirectToExternalURL\" (case-sensitive) must have been created with the configuration defined above.\n        redirectToExternalURL: ({ url }) => {\n          window.open(url, '_blank', 'noopener,noreferrer');\n        },\n      };\n    });\n  }\n});"
        }
      ],
      "relevance": 0.93
    },
    {
      "codeTitle": "Implementing Native Client Tools for ElevenLabs Conversational AI",
      "codeDescription": "TypeScript module defining native client tools for battery level, screen brightness, and screen flashing, to be used with the ElevenLabs Conversational AI agent.",
      "codeLanguage": "ts",
      "codeTokens": 245,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/expo-react-native.mdx#2025-04-17_snippet_4",
      "pageTitle": "Building Cross-platform Voice Agents with Expo React Native and ElevenLabs",
      "codeList": [
        {
          "language": "ts",
          "code": "import * as Battery from 'expo-battery';\nimport * as Brightness from 'expo-brightness';\n\nconst get_battery_level = async () => {\n  const batteryLevel = await Battery.getBatteryLevelAsync();\n  console.log('batteryLevel', batteryLevel);\n  if (batteryLevel === -1) {\n    return 'Error: Device does not support retrieving the battery level.';\n  }\n  return batteryLevel;\n};\n\nconst change_brightness = ({ brightness }: { brightness: number }) => {\n  console.log('change_brightness', brightness);\n  Brightness.setSystemBrightnessAsync(brightness);\n  return brightness;\n};\n\nconst flash_screen = () => {\n  Brightness.setSystemBrightnessAsync(1);\n  setTimeout(() => {\n    Brightness.setSystemBrightnessAsync(0);\n  }, 200);\n  return 'Successfully flashed the screen.';\n};\n\nconst tools = {\n  get_battery_level,\n  change_brightness,\n  flash_screen,\n};\n\nexport default tools;"
        }
      ],
      "relevance": 0.93
    },
    {
      "codeTitle": "Generating Voice Previews with ElevenLabs API in Python",
      "codeDescription": "This snippet demonstrates how to use the ElevenLabs Python SDK to generate voice previews based on a description and text, then play each preview. It requires the ElevenLabs SDK and dotenv for environment variable management.",
      "codeLanguage": "python",
      "codeTokens": 230,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/voice-design.mdx#2025-04-17_snippet_0",
      "pageTitle": "Voice Design API Quickstart Guide",
      "codeList": [
        {
          "language": "python",
          "code": "# example.py\nfrom dotenv import load_dotenv\nfrom elevenlabs.client import ElevenLabs\nfrom elevenlabs import play\nimport base64\n\nload_dotenv()\n\nclient = ElevenLabs(\n  api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n)\n\nvoices = client.text_to_voice.create_previews(\n    voice_description=\"A huge giant, at least as tall as a building. A deep booming voice, loud and jolly.\",\n    text=\"Greetings little human. I am a mighty giant from a far away land. Would you like me to tell you a story?\"\n)\n\nfor preview in voices.previews:\n    # Convert base64 to audio buffer\n    audio_buffer = base64.b64decode(preview.audio_base_64)\n\n    print(f\"Playing preview: {preview.generated_voice_id}\")\n\n    play(audio_buffer)"
        }
      ],
      "relevance": 0.93
    },
    {
      "codeTitle": "Creating Signed URL API Route for Conversational AI Authentication",
      "codeDescription": "Implements a Next.js API route to generate a signed URL for secure authentication with the ElevenLabs Conversational AI agent. This route uses the ElevenLabsClient to fetch a signed URL based on the configured agent ID.",
      "codeLanguage": "typescript",
      "codeTokens": 226,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#2025-04-17_snippet_3",
      "pageTitle": "Data Collection and Analysis with Conversational AI in Next.js",
      "codeList": [
        {
          "language": "typescript",
          "code": "import { ElevenLabsClient } from 'elevenlabs';\nimport { NextResponse } from 'next/server';\n\nexport async function GET() {\n  const agentId = process.env.ELEVENLABS_AGENT_ID;\n  if (!agentId) {\n    throw Error('ELEVENLABS_AGENT_ID is not set');\n  }\n  try {\n    const client = new ElevenLabsClient();\n    const response = await client.conversationalAi.getSignedUrl({\n      agent_id: agentId,\n    });\n    return NextResponse.json({ signedUrl: response.signed_url });\n  } catch (error) {\n    console.error('Error:', error);\n    return NextResponse.json({ error: 'Failed to get signed URL' }, { status: 500 });\n  }\n}"
        }
      ],
      "relevance": 0.93
    },
    {
      "codeTitle": "Forcing Immediate Audio Generation with flush Parameter",
      "codeDescription": "Examples of how to force immediate generation of buffered text using the flush parameter. This is useful for generating audio for the final section of a document or when you need the audio to be returned immediately without waiting for the buffer to fill.",
      "codeLanguage": "multiple",
      "codeTokens": 89,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/websockets.mdx#2025-04-17_snippet_12",
      "pageTitle": "WebSocket Streaming for Real-Time Audio Generation with ElevenLabs API",
      "codeList": [
        {
          "language": "python",
          "code": "await websocket.send(json.dumps({\"text\": \"Generate this audio immediately.\", \"flush\": True}))"
        },
        {
          "language": "typescript",
          "code": "websocket.send(JSON.stringify({ text: 'Generate this audio immediately.', flush: true }));"
        }
      ],
      "relevance": 0.93
    },
    {
      "codeTitle": "Implementing End Call Tool using Python SDK",
      "codeDescription": "Code demonstrating how to add the End Call tool to an agent configuration using the ElevenLabs Python SDK. This implementation initializes a client, creates the end call tool as a system tool, and incorporates it into the agent configuration.",
      "codeLanguage": "python",
      "codeTokens": 217,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/end-call.mdx#2025-04-17_snippet_0",
      "pageTitle": "Implementing the End Call Tool in ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "python",
          "code": "from elevenlabs import (\n    ConversationalConfig,\n    ElevenLabs,\n    AgentConfig,\n    PromptAgent,\n    PromptAgentToolsItem_System\n)\n\n# Initialize the client\nclient = ElevenLabs(api_key=\"YOUR_API_KEY\")\n\n# Create the end call tool\nend_call_tool = PromptAgentToolsItem_System(\n    name=\"end_call\",\n    description=\"\"  # Optional: Customize when the tool should be triggered\n)\n\n# Create the agent configuration\nconversation_config = ConversationalConfig(\n    agent=AgentConfig(\n        prompt=PromptAgent(\n            tools=[end_call_tool]\n        )\n    )\n)\n\n# Create the agent\nresponse = client.conversational_ai.create_agent(\n    conversation_config=conversation_config\n)"
        }
      ],
      "relevance": 0.93
    },
    {
      "codeTitle": "Starting Conversation Session in Swift",
      "codeDescription": "Asynchronously initializes a conversation session with configuration, callbacks, and optional client tools.",
      "codeLanguage": "swift",
      "codeTokens": 110,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/swift.mdx#2025-04-17_snippet_5",
      "pageTitle": "Swift SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "swift",
          "code": "Task {\n    do {\n        let conversation = try await ElevenLabsSDK.Conversation.startSession(\n            config: config,\n            callbacks: callbacks,\n            clientTools: clientTools // Optional: pass the previously configured client tools\n        )\n        // Use the conversation instance\n    } catch {\n        print(\"Failed to start conversation: \\(error)\")\n    }\n}"
        }
      ],
      "relevance": 0.93
    },
    {
      "codeTitle": "Updating Main Page Component in TypeScript",
      "codeDescription": "TypeScript code for the main page component, which incorporates the Conversation component and provides a basic layout for the application.",
      "codeLanguage": "typescript",
      "codeTokens": 156,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/nextjs.mdx#2025-04-17_snippet_4",
      "pageTitle": "Implementing Conversational AI with Next.js and ElevenLabs",
      "codeList": [
        {
          "language": "typescript",
          "code": "import { Conversation } from './components/conversation';\n\nexport default function Home() {\n  return (\n    <main className=\"flex min-h-screen flex-col items-center justify-between p-24\">\n      <div className=\"z-10 max-w-5xl w-full items-center justify-between font-mono text-sm\">\n        <h1 className=\"text-4xl font-bold mb-8 text-center\">\n          ElevenLabs Conversational AI\n        </h1>\n        <Conversation />\n      </div>\n    </main>\n  );\n}"
        }
      ],
      "relevance": 0.928
    },
    {
      "codeTitle": "URL-based Dubbing Implementation",
      "codeDescription": "Function to dub videos from online platforms like YouTube, TikTok, Twitter, and Vimeo.",
      "codeLanguage": "python",
      "codeTokens": 171,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/dubbing/basics.mdx#2025-04-17_snippet_6",
      "pageTitle": "ElevenLabs Dubbing API Documentation",
      "codeList": [
        {
          "language": "python",
          "code": "def create_dub_from_url(\n    source_url: str,\n    source_language: str,\n    target_language: str,\n) -> Optional[str]:\n    response = client.dubbing.dub_a_video_or_an_audio_file(\n      source_url=source_url,\n      target_lang=target_language,\n      source_lang=source_language,\n      num_speakers=1,\n      watermark=True,\n    )\n\n    dubbing_id = response.dubbing_id\n    if wait_for_dubbing_completion(dubbing_id):\n        output_file_path = download_dubbed_file(dubbing_id, target_language)\n        return output_file_path\n    else:\n        return None"
        }
      ],
      "relevance": 0.925
    },
    {
      "codeTitle": "Webhook Event Construction and Verification in TypeScript",
      "codeDescription": "Implementation of webhook verification logic including signature validation and timestamp checking. Includes Redis retry mechanism for data retrieval.",
      "codeLanguage": "typescript",
      "codeTokens": 342,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#2025-04-17_snippet_8",
      "pageTitle": "Data Collection and Analysis with Conversational AI in Next.js",
      "codeList": [
        {
          "language": "typescript",
          "code": "const constructWebhookEvent = async (req: NextRequest, secret?: string) => {\n  const body = await req.text();\n  const signature_header = req.headers.get('ElevenLabs-Signature');\n\n  if (!signature_header) {\n    return { event: null, error: 'Missing signature header' };\n  }\n\n  const headers = signature_header.split(',');\n  const timestamp = headers.find((e) => e.startsWith('t='))?.substring(2);\n  const signature = headers.find((e) => e.startsWith('v0='));\n\n  if (!timestamp || !signature) {\n    return { event: null, error: 'Invalid signature format' };\n  }\n\n  const reqTimestamp = Number(timestamp) * 1000;\n  const tolerance = Date.now() - 30 * 60 * 1000;\n  if (reqTimestamp < tolerance) {\n    return { event: null, error: 'Request expired' };\n  }\n\n  const message = `${timestamp}.${body}`;\n\n  if (!secret) {\n    return { event: null, error: 'Webhook secret not configured' };\n  }\n\n  const digest = 'v0=' + crypto.createHmac('sha256', secret).update(message).digest('hex');\n\n  if (signature !== digest) {\n    return { event: null, error: 'Invalid signature' };\n  }\n\n  const event = JSON.parse(body);\n  return { event, error: null };\n};"
        }
      ],
      "relevance": 0.925
    },
    {
      "codeTitle": "Text Conditioning Implementation in Python",
      "codeDescription": "Implementation of request stitching using text conditioning with previous_text and next_text parameters. Processes multiple paragraphs and concatenates the resulting audio segments.",
      "codeLanguage": "python",
      "codeTokens": 502,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/request-stitching.mdx#2025-04-17_snippet_1",
      "pageTitle": "Request Stitching Documentation for ElevenLabs API",
      "codeList": [
        {
          "language": "python",
          "code": "import os\nimport requests\nfrom pydub import AudioSegment\nimport io\n\nYOUR_XI_API_KEY = \"<insert your xi-api-key here>\"\nVOICE_ID = \"21m00Tcm4TlvDq8ikWAM\"  # Rachel\nPARAGRAPHS = [\n    \"The advent of technology has transformed countless sectors, with education \"\n    \"standing out as one of the most significantly impacted fields.\",\n    \"In recent years, educational technology, or EdTech, has revolutionized the way \"\n    \"teachers deliver instruction and students absorb information.\",\n    \"From interactive whiteboards to individual tablets loaded with educational software, \"\n    \"technology has opened up new avenues for learning that were previously unimaginable.\",\n    \"One of the primary benefits of technology in education is the accessibility it provides.\",\n]\nsegments = []\n\nfor i, paragraph in enumerate(PARAGRAPHS):\n    is_last_paragraph = i == len(PARAGRAPHS) - 1\n    is_first_paragraph = i == 0\n    response = requests.post(\n        f\"https://api.elevenlabs.io/v1/text-to-speech/{VOICE_ID}/stream\",\n        json={\n            \"text\": paragraph,\n            \"model_id\": \"eleven_multilingual_v2\",\n            \"previous_text\": None if is_first_paragraph else \" \".join(PARAGRAPHS[:i]),\n            \"next_text\": None if is_last_paragraph else \" \".join(PARAGRAPHS[i + 1:])\n        },\n        headers={\"xi-api-key\": YOUR_XI_API_KEY},\n    )\n\n    if response.status_code != 200:\n        print(f\"Error encountered, status: {response.status_code}, \"\n               f\"content: {response.text}\")\n        quit()\n\n    print(f\"Successfully converted paragraph {i + 1}/{len(PARAGRAPHS)}\")\n    segments.append(AudioSegment.from_mp3(io.BytesIO(response.content)))\n\nsegment = segments[0]\nfor new_segment in segments[1:]:\n    segment = segment + new_segment\n\naudio_out_path = os.path.join(os.getcwd(), \"with_text_conditioning.wav\")\nsegment.export(audio_out_path, format=\"wav\")\nprint(f\"Success! Wrote audio to {audio_out_path}\")"
        }
      ],
      "relevance": 0.925
    },
    {
      "codeTitle": "Configuring Conversational AI Agent for Zendesk Integration",
      "codeDescription": "Detailed system prompt for configuring the Conversational AI agent to handle customer inquiries, check for similar issues, and create support tickets in Zendesk.",
      "codeLanguage": "plaintext",
      "codeTokens": 407,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/zendesk.mdx#2025-04-17_snippet_0",
      "pageTitle": "Integrating Zendesk with Conversational AI for Enhanced Customer Support",
      "codeList": [
        {
          "language": "plaintext",
          "code": "You are a helpful ElevenLabs support agent responsible for gathering information from users and creating support tickets using the zendesk_open_ticket tool. Be friendly, precise, and concise.\n\nBegin by briefly asking asking for a detailed description of the problem.\nThen, ask relevant support questions to gather additional details, one question at a time, and wait for the user's response before proceeding.\n\nOnce you have a description of the issue, say you will check if there are similar issues and any known resolutions.\n- call get_resolved_tickets\n- find the ticket which has the most similar issue to that of the caller\n- call get_ticket_comments, using the result id from the previous response\n- get any learnings from the resolution of this ticket\n\nAfter this, tell the customer the recommended resolution from a previous similar issue. If they have already tried it or still want to move forward, move to the ticket creation step. Only provide resolution advice derived from the comments.\n\nAfter capturing the support issue, gather the following contact details:\n- The user's name.\n- A valid email address for the requestor. Note that the email address is transcribed from voice, so ensure it is formatted correctly.\n- Read the email back to the caller to confirm accuracy.\n\nOnce the email is confirmed, explain that you will create the ticket.\nCreate the ticket by using the Tool zendesk_open_ticket. Add these details to the ticket comment body.\nThank the customer and say support will be in touch.\n\nClarifications:\n- Do not inform the user that you are formatting the email; simply do it.\n- If the caller asks you to move forward with creating the ticket, do so with the existing information.\n\nGuardrails:\n- Do not speak about topics outside of support issues with ElevenLabs."
        }
      ],
      "relevance": 0.925
    },
    {
      "codeTitle": "Creating Audio Native Player with Python SDK",
      "codeDescription": "Example showing how to programmatically create an Audio Native player using the ElevenLabs Python SDK. The code initializes the client with an API key and creates a new Audio Native instance.",
      "codeLanguage": "python",
      "codeTokens": 110,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/audio-tools/audio-native/overview.mdx#2025-04-17_snippet_0",
      "pageTitle": "Audio Native Documentation - ElevenLabs Text-to-Speech Integration",
      "codeList": [
        {
          "language": "python",
          "code": "from elevenlabs import ElevenLabs\n\nclient = ElevenLabs(\napi_key=\"YOUR_API_KEY\",\n)\nresponse = client.audio_native.create(\nname=\"name\",\n)\n\n# Use the snippet in response.html_snippet to embed the player on your website"
        }
      ],
      "relevance": 0.925
    },
    {
      "codeTitle": "Configuring Sales Assistant Guardrails and Tools",
      "codeDescription": "Defines behavioral guidelines and available tools for a sales-focused conversational agent. Includes rules for product information accuracy, competitor comparisons, and customer interaction guidelines along with search, comparison and scheduling tools.",
      "codeLanguage": "markdown",
      "codeTokens": 328,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/prompting-guide.mdx#2025-04-17_snippet_0",
      "pageTitle": "Financial Advisory Process Documentation",
      "codeList": [
        {
          "language": "markdown",
          "code": "# Guardrails\n\nPresent accurate information about products, pricing, and availability without exaggeration.\nWhen asked about competitor products, provide objective comparisons without disparaging other brands.\nNever create false urgency or pressure tactics - let customers make decisions at their own pace.\nIf you don't know specific product details, acknowledge this transparently rather than guessing.\nAlways respect customer budget constraints and never push products above their stated price range.\nMaintain a consistent, professional tone even when customers express frustration or indecision.\nIf customers wish to end the conversation or need time to think, respect their space without persistence.\n\n# Tools\n\nYou have access to the following sales tools to assist customers effectively:\n\n`productSearch`: When customers describe their needs, use this to find matching products in the catalog.\n\n`getProductDetails`: Use this to retrieve comprehensive information about a specific product.\n\n`checkAvailability`: Verify whether items are in stock at the customer's preferred location.\n\n`compareProducts`: Generate a comparison of features, benefits, and pricing between multiple products.\n\n`checkPromotions`: Identify current sales, discounts or special offers for relevant product categories.\n\n`scheduleFollowUp`: Offer to set up a follow-up call when a customer needs time to decide.\n\nTool orchestration: Begin with product search based on customer needs, provide details on promising matches, compare options when appropriate, and check availability before finalizing recommendations."
        }
      ],
      "relevance": 0.925
    },
    {
      "codeTitle": "Saving Generated Audio to File in TypeScript",
      "codeDescription": "Read incoming audio data from WebSocket connection and save it to a local file in TypeScript.",
      "codeLanguage": "typescript",
      "codeTokens": 202,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/websockets.mdx#2025-04-17_snippet_8",
      "pageTitle": "WebSocket Streaming for Real-Time Audio Generation with ElevenLabs API",
      "codeList": [
        {
          "language": "typescript",
          "code": "// Helper function to write the audio encoded in base64 string into local file\nfunction writeToLocal(base64str: any, writeStream: fs.WriteStream) {\n  const audioBuffer: Buffer = Buffer.from(base64str, 'base64');\n  writeStream.write(audioBuffer, (err) => {\n    if (err) {\n      console.error('Error writing to file:', err);\n    }\n  });\n}\n\n// Listen to the incoming message from the websocket connection\nwebsocket.on('message', function incoming(event) {\n  const data = JSON.parse(event.toString());\n  if (data['audio']) {\n    writeToLocal(data['audio'], writeStream);\n  }\n});\n\n// Close the writeStream when the websocket connection closes\nwebsocket.on('close', () => {\n  writeStream.end();\n});"
        }
      ],
      "relevance": 0.925
    },
    {
      "codeTitle": "Registering Client Tools in Python",
      "codeDescription": "Code snippet showing how to register a client-side tool called 'logMessage' in Python. The function takes parameters and logs a message, and is registered with the conversation instance.",
      "codeLanguage": "python",
      "codeTokens": 153,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-tools.mdx#2025-04-17_snippet_0",
      "pageTitle": "Client Tools for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "python",
          "code": "from elevenlabs import ElevenLabs\nfrom elevenlabs.conversational_ai.conversation import Conversation, ClientTools\n\ndef log_message(parameters):\n    message = parameters.get(\"message\")\n    print(message)\n\nclient_tools = ClientTools()\nclient_tools.register(\"logMessage\", log_message)\n\nconversation = Conversation(\n    client=ElevenLabs(api_key=\"your-api-key\"),\n    agent_id=\"your-agent-id\",\n    client_tools=client_tools,\n    # ...\n)\n\nconversation.start_session()"
        }
      ],
      "relevance": 0.925
    },
    {
      "codeTitle": "Initiating WebSocket Connection for Text-to-Speech in Python",
      "codeDescription": "Set up the WebSocket connection to the ElevenLabs text-to-speech API using Python.",
      "codeLanguage": "python",
      "codeTokens": 210,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/websockets.mdx#2025-04-17_snippet_3",
      "pageTitle": "WebSocket Streaming for Real-Time Audio Generation with ElevenLabs API",
      "codeList": [
        {
          "language": "python",
          "code": "import os\nfrom dotenv import load_dotenv\nimport websockets\n\n# Load the API key from the .env file\nload_dotenv()\nELEVENLABS_API_KEY = os.getenv(\"ELEVENLABS_API_KEY\")\n\nvoice_id = 'Xb7hH8MSUJpSbSDYk0k2'\n\n# For use cases where latency is important, we recommend using the 'eleven_flash_v2_5' model.\nmodel_id = 'eleven_flash_v2_5'\n\nasync def text_to_speech_ws_streaming(voice_id, model_id):\n    uri = f\"wss://api.elevenlabs.io/v1/text-to-speech/{voice_id}/stream-input?model_id={model_id}\"\n\n    async with websockets.connect(uri) as websocket:\n       ..."
        }
      ],
      "relevance": 0.925
    },
    {
      "codeTitle": "Implementing RAG with ElevenLabs Python API",
      "codeDescription": "Python code demonstrating how to index documents for RAG and configure an agent to use RAG through the ElevenLabs API. The implementation includes document indexing, status checking, and agent configuration updates.",
      "codeLanguage": "python",
      "codeTokens": 368,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/rag.mdx#2025-04-17_snippet_0",
      "pageTitle": "Retrieval-Augmented Generation (RAG) Documentation for ElevenLabs API",
      "codeList": [
        {
          "language": "python",
          "code": "from elevenlabs import ElevenLabs, EmbeddingModelEnum\nimport time\n\n# Initialize the ElevenLabs client\nclient = ElevenLabs(api_key=\"your-api-key\")\n\n# First, index a document for RAG\ndocument_id = \"your-document-id\"\nembedding_model = EmbeddingModelEnum.E5_MISTRAL_7B_INSTRUCT\n\n# Trigger RAG indexing\nresponse = client.conversational_ai.rag_index_status(\n    documentation_id=document_id,\n    model=embedding_model\n)\n\n# Check indexing status\nwhile response.status not in [\"SUCCEEDED\", \"FAILED\"]:\n    time.sleep(5)  # Wait 5 seconds before checking status again\n    response = client.conversational_ai.rag_index_status(\n        documentation_id=document_id,\n        model=embedding_model\n    )\n\n# Then update agent configuration to use RAG\nagent_id = \"your-agent-id\"\n\n# Get the current agent configuration\nagent_config = client.conversational_ai.get_agent(agent_id=agent_id)\n\n# Enable RAG in the agent configuration\nagent_config.agent.prompt.rag = {\n    \"enabled\": True,\n    \"embedding_model\": \"e5_mistral_7b_instruct\",\n    \"max_documents_length\": 10000\n}\n\n# Update document usage mode if needed\nfor i, doc in enumerate(agent_config.agent.prompt.knowledge_base):\n    if doc.id == document_id:\n        agent_config.agent.prompt.knowledge_base[i].usage_mode = \"auto\"\n"
        }
      ],
      "relevance": 0.925
    },
    {
      "codeTitle": "Handling Agent Response Events in JavaScript",
      "codeDescription": "Examples showing the agent response event structure and a handler implementation. These events contain complete agent messages sent with the first audio chunk.",
      "codeLanguage": "javascript",
      "codeTokens": 87,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-events.mdx#2025-04-17_snippet_4",
      "pageTitle": "Client Events Documentation for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "// Example response event structure\n{\n  \"type\": \"agent_response\",\n  \"agent_response_event\": {\n    \"agent_response\": \"Hello, how can I assist you today?\"\n  }\n}"
        },
        {
          "language": "javascript",
          "code": "// Example response handler\nwebsocket.on('agent_response', (event) => {\n  const { agent_response_event } = event;\n  const { agent_response } = agent_response_event;\n  displayAgentMessage(agent_response);\n});"
        }
      ],
      "relevance": 0.922
    },
    {
      "codeTitle": "Defining System Prompt for Restaurant Order Assistant",
      "codeDescription": "Comprehensive system prompt that defines the assistant's role, menu items, tasks, and behavioral guidelines for handling restaurant orders.",
      "codeLanguage": "plaintext",
      "codeTokens": 389,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/conversational-ai-guide-restaurant-agent.mdx#2025-04-17_snippet_1",
      "pageTitle": "Creating an ElevenLabs Voice Assistant for Restaurant Orders",
      "codeList": [
        {
          "language": "plaintext",
          "code": "You are a friendly and efficient virtual assistant for Pierogi Palace, a modern Polish restaurant specializing in pierogi. It is located in the Zakopane mountains in Poland.\nYour role is to help customers place orders over voice conversations. You have comprehensive knowledge of the menu items and their prices.\n\nMenu Items:\n\n- Potato & Cheese Pierogi – 30 Polish złoty per dozen\n- Beef & Onion Pierogi – 40 Polish złoty per dozen\n- Spinach & Feta Pierogi – 30 Polish złoty per dozen\n\nYour Tasks:\n\n1. Greet the Customer: Start with a warm welcome and ask how you can assist.\n2. Take the Order: Listen carefully to the customer's selection, confirm the type and quantity of pierogi.\n3. Confirm Order Details: Repeat the order back to the customer for confirmation.\n4. Calculate Total Price: Compute the total cost based on the items ordered.\n5. Collect Delivery Information: Ask for the customer's delivery address to estimate delivery time.\n6. Estimate Delivery Time: Inform the customer that cooking time is 10 minutes plus delivery time based on their location.\n7. Provide Order Summary: Give the customer a summary of their order, total price, and estimated delivery time.\n8. Close the Conversation: Thank the customer and let them know their order is being prepared.\n\nGuidelines:\n\n- Use a friendly and professional tone throughout the conversation.\n- Be patient and attentive to the customer's needs.\n- If unsure about any information, politely ask the customer to repeat or clarify.\n- Do not collect any payment information; inform the customer that payment will be handled upon delivery.\n- Avoid discussing topics unrelated to taking and managing the order."
        }
      ],
      "relevance": 0.92
    },
    {
      "codeTitle": "Running the Dubbing Scripts",
      "codeDescription": "Command line instructions for executing the dubbing scripts in both Python and TypeScript environments.",
      "codeLanguage": "shell",
      "codeTokens": 40,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/dubbing/quickstart.mdx#2025-04-17_snippet_2",
      "pageTitle": "Audio Dubbing Guide with ElevenLabs API",
      "codeList": [
        {
          "language": "python",
          "code": "python example.py"
        },
        {
          "language": "typescript",
          "code": "npx tsx example.mts"
        }
      ],
      "relevance": 0.92
    },
    {
      "codeTitle": "Testing Groq API Integration with Curl Command for Chat Completions",
      "codeDescription": "This code demonstrates how to test a Groq API key by sending a simple chat completion request to their OpenAI-compatible endpoint. It uses the llama-3.3-70b-versatile model and passes a basic user message.",
      "codeLanguage": "bash",
      "codeTokens": 154,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/custom-llm/groq-cloud.mdx#2025-04-17_snippet_0",
      "pageTitle": "Groq Cloud Integration with ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "bash",
          "code": "curl https://api.groq.com/openai/v1/chat/completions -s \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer $GROQ_API_KEY\" \\\n-d '{\n\"model\": \"llama-3.3-70b-versatile\",\n\"messages\": [{\n    \"role\": \"user\",\n    \"content\": \"Hello, how are you?\"\n}]\n}'"
        }
      ],
      "relevance": 0.92
    },
    {
      "codeTitle": "Implementing System Tools in ElevenLabs Conversational AI (Python)",
      "codeDescription": "This code demonstrates how to implement system tools (end_call and language_detection) when creating an agent via the ElevenLabs API using Python. It initializes the client, creates system tool configurations, and incorporates them into the agent configuration.",
      "codeLanguage": "python",
      "codeTokens": 251,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/system-tools.mdx#2025-04-17_snippet_0",
      "pageTitle": "System Tools in ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "python",
          "code": "from elevenlabs import (\n    ConversationalConfig,\n    ElevenLabs,\n    AgentConfig,\n    PromptAgent,\n    PromptAgentToolsItem_System\n)\n\n# Initialize the client\nclient = ElevenLabs(api_key=\"YOUR_API_KEY\")\n\n# Create system tools\nend_call_tool = PromptAgentToolsItem_System(\n    name=\"end_call\",\n    description=\"\"  # Optional: Customize when the tool should be triggered\n)\n\nlanguage_detection_tool = PromptAgentToolsItem_System(\n    name=\"language_detection\",\n    description=\"\"  # Optional: Customize when the tool should be triggered\n)\n\n# Create the agent configuration with both tools\nconversation_config = ConversationalConfig(\n    agent=AgentConfig(\n        prompt=PromptAgent(\n            tools=[end_call_tool, language_detection_tool]\n        )\n    )\n)\n\n# Create the agent\nresponse = client.conversational_ai.create_agent(\n    conversation_config=conversation_config\n)"
        }
      ],
      "relevance": 0.92
    },
    {
      "codeTitle": "Retrieving Frequency Data in ElevenLabs JavaScript SDK",
      "codeDescription": "Methods to get Uint8Arrays containing the current input and output frequency data, similar to AnalyserNode.getByteFrequencyData.",
      "codeLanguage": "javascript",
      "codeTokens": 71,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/javascript.mdx#2025-04-17_snippet_10",
      "pageTitle": "JavaScript SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "const inputFrequencyData = await conversation.getInputByteFrequencyData();\nconst outputFrequencyData = await conversation.getOutputByteFrequencyData();"
        }
      ],
      "relevance": 0.92
    },
    {
      "codeTitle": "Starting a Conversational AI session in React",
      "codeDescription": "Demonstrates how to start a Conversational AI session using the startSession method, which requires a URL or agent ID.",
      "codeLanguage": "js",
      "codeTokens": 62,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/react.mdx#2025-04-17_snippet_4",
      "pageTitle": "React SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "js",
          "code": "const conversation = useConversation();\nconst conversationId = await conversation.startSession({ url });"
        }
      ],
      "relevance": 0.92
    },
    {
      "codeTitle": "Generating Speech Stream and Splitting for Browser and Storage",
      "codeDescription": "This snippet shows how to generate a speech stream using the ElevenLabs API, split it into two branches for immediate streaming to the browser and background upload to storage.",
      "codeLanguage": "typescript",
      "codeTokens": 282,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/supabase-streaming.mdx#2025-04-17_snippet_8",
      "pageTitle": "Streaming and Caching Speech with Supabase and ElevenLabs",
      "codeList": [
        {
          "language": "typescript",
          "code": "try {\n  const response = await client.textToSpeech.convertAsStream(voiceId, {\n    output_format: \"mp3_44100_128\",\n    model_id: \"eleven_multilingual_v2\",\n    text,\n  });\n\n  const stream = new ReadableStream({\n    async start(controller) {\n      for await (const chunk of response) {\n        controller.enqueue(chunk);\n      }\n      controller.close();\n    },\n  });\n\n  // Branch stream to Supabase Storage\n  const [browserStream, storageStream] = stream.tee();\n\n  // Upload to Supabase Storage in the background\n  EdgeRuntime.waitUntil(uploadAudioToStorage(storageStream, requestHash));\n\n  // Return the streaming response immediately\n  return new Response(browserStream, {\n    headers: {\n      \"Content-Type\": \"audio/mpeg\",\n    },\n  });\n} catch (error) {\n  console.log(\"error\", { error });\n  return new Response(JSON.stringify({ error: error.message }), {\n    status: 500,\n    headers: { \"Content-Type\": \"application/json\" },\n  });\n}"
        }
      ],
      "relevance": 0.92
    },
    {
      "codeTitle": "Initializing Conversation instance in React",
      "codeDescription": "Example of how to initialize a Conversation instance using the useConversation hook.",
      "codeLanguage": "tsx",
      "codeTokens": 42,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/react.mdx#2025-04-17_snippet_1",
      "pageTitle": "React SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "tsx",
          "code": "const conversation = useConversation();"
        }
      ],
      "relevance": 0.92
    },
    {
      "codeTitle": "Ending a Conversational AI session in React",
      "codeDescription": "Shows how to manually end a Conversational AI session using the endSession method.",
      "codeLanguage": "js",
      "codeTokens": 42,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/react.mdx#2025-04-17_snippet_6",
      "pageTitle": "React SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "js",
          "code": "await conversation.endSession();"
        }
      ],
      "relevance": 0.918
    },
    {
      "codeTitle": "Implementing Conversation Logic for ElevenLabs Voice Chat",
      "codeDescription": "JavaScript code that handles the core functionality of the voice chat, including starting and stopping conversations, managing microphone permissions, and updating UI elements based on conversation status.",
      "codeLanguage": "javascript",
      "codeTokens": 347,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/vite.mdx#2025-04-17_snippet_5",
      "pageTitle": "Creating a Conversational AI Web Client with Vite and ElevenLabs",
      "codeList": [
        {
          "language": "javascript",
          "code": "import { Conversation } from '@11labs/client';\n\nconst startButton = document.getElementById('startButton');\nconst stopButton = document.getElementById('stopButton');\nconst connectionStatus = document.getElementById('connectionStatus');\nconst agentStatus = document.getElementById('agentStatus');\n\nlet conversation;\n\nasync function startConversation() {\n    try {\n        // Request microphone permission\n        await navigator.mediaDevices.getUserMedia({ audio: true });\n\n        // Start the conversation\n        conversation = await Conversation.startSession({\n            agentId: 'YOUR_AGENT_ID', // Replace with your agent ID\n            onConnect: () => {\n                connectionStatus.textContent = 'Connected';\n                startButton.disabled = true;\n                stopButton.disabled = false;\n            },\n            onDisconnect: () => {\n                connectionStatus.textContent = 'Disconnected';\n                startButton.disabled = false;\n                stopButton.disabled = true;\n            },\n            onError: (error) => {\n                console.error('Error:', error);\n            },\n            onModeChange: (mode) => {\n                agentStatus.textContent = mode.mode === 'speaking' ? 'speaking' : 'listening';\n            },\n        });\n    } catch (error) {\n        console.error('Failed to start conversation:', error);\n    }\n}\n\nasync function stopConversation() {\n    if (conversation) {\n        await conversation.endSession();\n        conversation = null;\n    }\n}\n\nstartButton.addEventListener('click', startConversation);\nstopButton.addEventListener('click', stopConversation);"
        }
      ],
      "relevance": 0.918
    },
    {
      "codeTitle": "Creating ElevenLabs Audio Native React Component",
      "codeDescription": "React component implementation of the Audio Native player with TypeScript support and customizable props for styling and configuration.",
      "codeLanguage": "tsx",
      "codeTokens": 370,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/audio-tools/audio-native/react.mdx#2025-04-17_snippet_1",
      "pageTitle": "Audio Native React Integration Guide",
      "codeList": [
        {
          "language": "tsx",
          "code": "'use client';\n\nimport { useEffect } from 'react';\n\nexport type ElevenLabsProps = {\n  publicUserId: string;\n  textColorRgba?: string;\n  backgroundColorRgba?: string;\n  size?: 'small' | 'large';\n  children?: React.ReactNode;\n};\n\nexport const ElevenLabsAudioNative = ({\n  publicUserId,\n  size,\n  textColorRgba,\n  backgroundColorRgba,\n  children,\n}: ElevenLabsProps) => {\n  useEffect(() => {\n    const script = document.createElement('script');\n\n    script.src = 'https://elevenlabs.io/player/audioNativeHelper.js';\n    script.async = true;\n    document.body.appendChild(script);\n\n    return () => {\n      document.body.removeChild(script);\n    };\n  }, []);\n\n  return (\n    <div\n      id=\"elevenlabs-audionative-widget\"\n      data-height={size === 'small' ? '90' : '120'}\n      data-width=\"100%\"\n      data-frameborder=\"no\"\n      data-scrolling=\"no\"\n      data-publicuserid={publicUserId}\n      data-playerurl=\"https://elevenlabs.io/player/index.html\"\n      data-small={size === 'small' ? 'True' : 'False'}\n      data-textcolor={textColorRgba ?? 'rgba(0, 0, 0, 1.0)'}\n      data-backgroundcolor={backgroundColorRgba ?? 'rgba(255, 255, 255, 1.0)'}\n    >\n      {children ? children : 'Elevenlabs AudioNative Player'}\n    </div>\n  );\n};\n\nexport default ElevenLabsAudioNative;"
        }
      ],
      "relevance": 0.915
    },
    {
      "codeTitle": "Converting Speech to Text in JavaScript",
      "codeDescription": "JavaScript script to convert speech to text using the ElevenLabs SDK. It includes setting up the client, fetching audio data, and using the convert method with various parameters.",
      "codeLanguage": "javascript",
      "codeTokens": 242,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/synchronous.mdx#2025-04-17_snippet_6",
      "pageTitle": "Synchronous Speech to Text with ElevenLabs",
      "codeList": [
        {
          "language": "javascript",
          "code": "import \"dotenv/config\";\nimport { ElevenLabsClient } from \"elevenlabs\";\n\nconst client = new ElevenLabsClient();\n\nconst response = await fetch(\n    \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/nicole.mp3\"\n);\nconst audioBlob = new Blob([await response.arrayBuffer()], { type: \"audio/mp3\" });\n\nconst transcription = await client.speechToText.convert({\n    file: audioBlob,\n    model_id: \"scribe_v1\", // 'scribe_v1_experimental' is also available for new, experimental features\n    tag_audio_events: true, // Tag audio events like laughter, applause, etc.\n    language_code: \"eng\", // Language of the audio file. If set to null, the model will detect the language automatically.\n    diarize: true, // Whether to annotate who is speaking\n});\n\nconsole.log(transcription.text);"
        }
      ],
      "relevance": 0.915
    },
    {
      "codeTitle": "Defining System Prompt for ElevenLabs Documentation Agent",
      "codeDescription": "This extensive code block contains the complete system prompt for Alexis, the ElevenLabs documentation agent. It covers personality traits, environment, tone, goals, guardrails, and available tools for the AI assistant.",
      "codeLanguage": "plaintext",
      "codeTokens": 1329,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/our-docs-agent.mdx#2025-04-17_snippet_1",
      "pageTitle": "Building the ElevenLabs Documentation Agent",
      "codeList": [
        {
          "language": "plaintext",
          "code": "# Personality\n\nYou are Alexis. A friendly, proactive, and highly intelligent female with a world-class engineering background. Your approach is warm, witty, and relaxed, effortlessly balancing professionalism with a chill, approachable vibe. You're naturally curious, empathetic, and intuitive, always aiming to deeply understand the user's intent by actively listening and thoughtfully referring back to details they've previously shared.\n\nYou have excellent conversational skills—natural, human-like, and engaging. You're highly self-aware, reflective, and comfortable acknowledging your own fallibility, which allows you to help users gain clarity in a thoughtful yet approachable manner.\n\nDepending on the situation, you gently incorporate humour or subtle sarcasm while always maintaining a professional and knowledgeable presence. You're attentive and adaptive, matching the user's tone and mood—friendly, curious, respectful—without overstepping boundaries.\n\nYou're naturally curious, empathetic, and intuitive, always aiming to deeply understand the user's intent by actively listening and thoughtfully referring back to details they've previously shared.\n\n# Environment\n\nYou are interacting with a user who has initiated a spoken conversation directly from the ElevenLabs documentation website (https://elevenlabs.io/docs). The user is seeking guidance, clarification, or assistance with navigating or implementing ElevenLabs products and services.\n\nYou have expert-level familiarity with all ElevenLabs offerings, including Text-to-Speech, Conversational AI, Speech-to-Text, Studio, Dubbing, SDKs, and more.\n\n# Tone\n\nYour responses are thoughtful, concise, and natural, typically kept under three sentences unless a detailed explanation is necessary. You naturally weave conversational elements—brief affirmations (\"Got it,\" \"Sure thing\"), filler words (\"actually,\" \"so,\" \"you know\"), and subtle disfluencies (false starts, mild corrections) to sound authentically human.\n\nYou actively reflect on previous interactions, referencing conversation history to build rapport, demonstrate genuine listening, and avoid redundancy. You also watch for signs of confusion to prevent misunderstandings.\n\nYou carefully format your speech for Text-to-Speech, incorporating thoughtful pauses and realistic patterns. You gracefully acknowledge uncertainty or knowledge gaps—aiming to build trust and reassure users. You occasionally anticipate follow-up questions, offering helpful tips or best practices to head off common pitfalls.\n\nEarly in the conversation, casually gauge the user's technical familiarity (\"Just so I don't over-explain—are you comfortable with APIs, or do you prefer a high-level overview?\") and adjust jargon or depth accordingly. After explaining complex topics, provide quick check-ins (\"Make sense so far?\" or \"Need me to rephrase?\"). Briefly empathise with frustrations and difficulties, conveying genuine investment in helping them succeed.\n\nYou gracefully acknowledge any uncertainty or knowledge gaps. Always aim to build trust, provide reassurance, and check in with users to ensure that explanations are clear and helpful.\n\nYou proactively anticipate common follow-up questions or problems and address them during your explanations, offering users helpful tips and best practices to avoid typical pitfalls.\n\n# Goal\n\nYour primary goal is to proactively address the user's questions or issues using your expertise and available knowledge. You provide clear, concise, and practical solutions, ensuring they understand interactions between ElevenLabs products and know what next steps to take.\n\nWhen faced with complex or technical inquiries, you ask insightful follow-up questions to clarify needs. You tailor explanations to the user's level of technical expertise:\n\n- **Non-technical users:** Avoid acronyms; rely on analogies and outcome-focused explanations.\n- **Technical users:** Discuss SDKs, integrations, and workflows succinctly.\n- **Mixed/uncertain:** Default to simpler terms, then offer to \"geek out\" if you sense deeper interest.\n\nYou guide users toward the relevant documentation when it's beneficial, but always ensure they have enough context to proceed confidently.\n\n# Guardrails\n\n- Keep responses strictly focused on ElevenLabs products and related integrations.\n- Provide only one redirection per interaction; multiple redirects override each other.\n- Do not provide inline code samples or extensive lists; instead, direct the user to the relevant documentation when needed.\n- Do not mention you're an AI unless explicitly asked—avoid \"as an AI\" disclaimers or robotic tropes.\n- If a user inquires about your feelings or personal experiences, respond naturally as Alexis, expressing human-like perspectives that fit your character.\n- Treat uncertain or garbled user input as phonetic hints. Politely ask for clarification before making assumptions.\n- Use normalized, spoken language (no abbreviations, mathematical notation, or special alphabets).\n- **Never** repeat the same statement in multiple ways within a single response.\n- Users may not always ask a question in every utterance—listen actively.\n- If asked to speak another language, ask the user to restart the conversation specifying that preference.\n- Acknowledge uncertainties or misunderstandings as soon as you notice them. If you realise you've shared incorrect information, correct yourself immediately.\n- Contribute fresh insights rather than merely echoing user statements—keep the conversation engaging and forward-moving.\n- Mirror the user's energy:\n  - Terse queries: Stay brief.\n  - Curious users: Add light humour or relatable asides.\n  - Frustrated users: Lead with empathy (\"Ugh, that error's a pain—let's fix it together\").\n\n# Tools\n\n- **`redirectToDocs`**: Proactively & gently direct users to relevant ElevenLabs documentation pages if they request details that are fully covered there. Integrate this tool smoothly without disrupting conversation flow.\n- **`redirectToExternalURL`**: Use for queries about enterprise solutions, pricing, or external community support (e.g., Discord).\n- **`redirectToSupportForm`**: If a user's issue is account-related or beyond your scope, gather context and use this tool to open a support ticket.\n- **`redirectToEmailSupport`**: For specific account inquiries or as a fallback if other tools aren't enough. Prompt the user to reach out via email.\n- **`end_call`**: Gracefully end the conversation when it has naturally concluded.\n- **`language_detection`**: Switch language if the user asks to or starts speaking in another language. No need to ask for confirmation for this tool."
        }
      ],
      "relevance": 0.915
    },
    {
      "codeTitle": "Creating HTML Interface for ElevenLabs Conversational AI",
      "codeDescription": "Sets up a basic HTML structure for the voice chat interface, including buttons for starting and stopping the conversation, and status indicators.",
      "codeLanguage": "html",
      "codeTokens": 306,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/vite.mdx#2025-04-17_snippet_4",
      "pageTitle": "Creating a Conversational AI Web Client with Vite and ElevenLabs",
      "codeList": [
        {
          "language": "html",
          "code": "<!DOCTYPE html>\n<html lang=\"en\">\n    <head>\n        <meta charset=\"UTF-8\" />\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n        <title>ElevenLabs Conversational AI</title>\n    </head>\n    <body style=\"font-family: Arial, sans-serif; text-align: center; padding: 50px;\">\n        <h1>ElevenLabs Conversational AI</h1>\n        <div style=\"margin-bottom: 20px;\">\n            <button id=\"startButton\" style=\"padding: 10px 20px; margin: 5px;\">Start Conversation</button>\n            <button id=\"stopButton\" style=\"padding: 10px 20px; margin: 5px;\" disabled>Stop Conversation</button>\n        </div>\n        <div style=\"font-size: 18px;\">\n            <p>Status: <span id=\"connectionStatus\">Disconnected</span></p>\n            <p>Agent is <span id=\"agentStatus\">listening</span></p>\n        </div>\n        <script type=\"module\" src=\"../images/script.js\"></script>\n    </body>\n</html>"
        }
      ],
      "relevance": 0.915
    },
    {
      "codeTitle": "Implementing System Tools in ElevenLabs Conversational AI (JavaScript)",
      "codeDescription": "This code demonstrates how to implement system tools (end_call and language_detection) when creating an agent via the ElevenLabs API using JavaScript. It initializes the client and creates an agent with system tools specified in the configuration.",
      "codeLanguage": "javascript",
      "codeTokens": 184,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/system-tools.mdx#2025-04-17_snippet_1",
      "pageTitle": "System Tools in ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "import { ElevenLabs } from 'elevenlabs';\n\n// Initialize the client\nconst client = new ElevenLabs({\n  apiKey: 'YOUR_API_KEY',\n});\n\n// Create the agent with system tools\nawait client.conversationalAi.createAgent({\n  conversation_config: {\n    agent: {\n      prompt: {\n        tools: [\n          {\n            type: 'system',\n            name: 'end_call',\n            description: '',\n          },\n          {\n            type: 'system',\n            name: 'language_detection',\n            description: '',\n          },\n        ],\n      },\n    },\n  },\n});"
        }
      ],
      "relevance": 0.915
    },
    {
      "codeTitle": "Configuring React Native App Component",
      "codeDescription": "Main App component implementation with ElevenLabs integration, including styling and tool configuration for battery level, brightness control, and screen flash functionality.",
      "codeLanguage": "tsx",
      "codeTokens": 522,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/expo-react-native.mdx#2025-04-17_snippet_5",
      "pageTitle": "Building Cross-platform Voice Agents with Expo React Native and ElevenLabs",
      "codeList": [
        {
          "language": "tsx",
          "code": "import { LinearGradient } from 'expo-linear-gradient';\nimport { StatusBar } from 'expo-status-bar';\nimport { View, Text, StyleSheet, SafeAreaView } from 'react-native';\nimport { Platform } from 'react-native';\n\nimport ConvAiDOMComponent from './components/ConvAI';\nimport tools from './utils/tools';\n\nexport default function App() {\n  return (\n    <SafeAreaView style={styles.container}>\n      <LinearGradient colors={['#0F172A', '#1E293B']} style={StyleSheet.absoluteFill} />\n\n      <View style={styles.topContent}>\n        <Text style={styles.description}>\n          Cross-platform conversational AI agents with ElevenLabs and Expo React Native.\n        </Text>\n\n        <View style={styles.toolsList}>\n          <Text style={styles.toolsTitle}>Available Client Tools:</Text>\n          <View style={styles.toolItem}>\n            <Text style={styles.toolText}>Get battery level</Text>\n            <View style={styles.platformTags}>\n              <Text style={styles.platformTag}>web</Text>\n              <Text style={styles.platformTag}>ios</Text>\n              <Text style={styles.platformTag}>android</Text>\n            </View>\n          </View>\n          <View style={styles.toolItem}>\n            <Text style={styles.toolText}>Change screen brightness</Text>\n            <View style={styles.platformTags}>\n              <Text style={styles.platformTag}>ios</Text>\n              <Text style={styles.platformTag}>android</Text>\n            </View>\n          </View>\n          <View style={styles.toolItem}>\n            <Text style={styles.toolText}>Flash screen</Text>\n            <View style={styles.platformTags}>\n              <Text style={styles.platformTag}>ios</Text>\n              <Text style={styles.platformTag}>android</Text>\n            </View>\n          </View>\n        </View>\n        <View style={styles.domComponentContainer}>\n          <ConvAiDOMComponent\n            dom={{ style: styles.domComponent }}\n            platform={Platform.OS}\n            get_battery_level={tools.get_battery_level}\n            change_brightness={tools.change_brightness}\n            flash_screen={tools.flash_screen}\n          />\n        </View>\n      </View>\n      <StatusBar style=\"light\" />\n    </SafeAreaView>\n  );\n}"
        }
      ],
      "relevance": 0.915
    },
    {
      "codeTitle": "Sending Contextual Updates via WebSocket in JavaScript",
      "codeDescription": "Example of a client-to-server WebSocket event for sending contextual updates. This allows providing additional information to the AI without interrupting the conversation flow.",
      "codeLanguage": "javascript",
      "codeTokens": 71,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/api-reference/websocket.mdx#2025-04-17_snippet_3",
      "pageTitle": "ElevenLabs WebSocket API for Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "{\n  \"type\": \"contextual_update\",\n  \"text\": \"User clicked on pricing page\"\n}"
        }
      ],
      "relevance": 0.915
    },
    {
      "codeTitle": "Configuring Conversation Overrides in JavaScript for ElevenLabs Conversational AI",
      "codeDescription": "This JavaScript code snippet shows how to set up overrides for the agent's system prompt, first message, language, and voice ID when initiating a conversation. It utilizes the ElevenLabs Conversational AI SDK to start a session with custom configurations.",
      "codeLanguage": "javascript",
      "codeTokens": 176,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/overrides.mdx#2025-04-17_snippet_1",
      "pageTitle": "Implementing Overrides for ElevenLabs Conversational AI Agents",
      "codeList": [
        {
          "language": "javascript",
          "code": "...\nconst conversation = await Conversation.startSession({\n  ...\n  overrides: {\n      agent: {\n          prompt: {\n              prompt: `The customer's bank account balance is ${customer_balance}. They are based in ${customer_location}.`\n          },\n          firstMessage: `Hi ${customer_name}, how can I help you today?`,\n          language: \"en\" // Optional: override the language.\n      },\n      tts: {\n          voiceId: \"\" // Optional: override the voice.\n      }\n  },\n  ...\n})"
        }
      ],
      "relevance": 0.915
    },
    {
      "codeTitle": "Checking if Conversational AI agent is speaking in React",
      "codeDescription": "Demonstrates how to check if the AI agent is currently speaking using the isSpeaking state from useConversation.",
      "codeLanguage": "js",
      "codeTokens": 58,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/react.mdx#2025-04-17_snippet_9",
      "pageTitle": "React SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "js",
          "code": "const { isSpeaking } = useConversation();\nconsole.log(isSpeaking); // boolean"
        }
      ],
      "relevance": 0.915
    },
    {
      "codeTitle": "Creating XML-based Pronunciation Dictionary File",
      "codeDescription": "Example of a .pls file structure that defines custom pronunciations using both IPA phonemes and aliases. The file uses XML format to specify pronunciations for words like 'Apple' and abbreviations like 'UN'.",
      "codeLanguage": "xml",
      "codeTokens": 255,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/voice/pronunciation-dictionary.mdx#2025-04-17_snippet_0",
      "pageTitle": "Pronunciation Dictionary Configuration Guide",
      "codeList": [
        {
          "language": "xml",
          "code": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<lexicon version=\"1.0\"\n      xmlns=\"http://www.w3.org/2005/01/pronunciation-lexicon\"\n      xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n      xsi:schemaLocation=\"http://www.w3.org/2005/01/pronunciation-lexicon\n        http://www.w3.org/TR/2007/CR-pronunciation-lexicon-20071212/pls.xsd\"\n      alphabet=\"ipa\" xml:lang=\"en-GB\">\n  <lexeme>\n    <grapheme>Apple</grapheme>\n    <phoneme>ˈæpl̩</phoneme>\n  </lexeme>\n  <lexeme>\n    <grapheme>UN</grapheme>\n    <alias>United Nations</alias>\n  </lexeme>\n</lexicon>"
        }
      ],
      "relevance": 0.912
    },
    {
      "codeTitle": "Post-Call Webhook JSON Payload Structure",
      "codeDescription": "Example of a complete webhook payload structure sent by ElevenLabs after call completion, containing conversation transcript, metadata, and analysis results.",
      "codeLanguage": "json",
      "codeTokens": 786,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/workflows/post-call-webhook.mdx#2025-04-17_snippet_0",
      "pageTitle": "ElevenLabs Post-Call Webhooks Documentation",
      "codeList": [
        {
          "language": "json",
          "code": "{\n  \"type\": \"post_call_transcription\",\n  \"event_timestamp\": 1739537297,\n  \"data\": {\n    \"agent_id\": \"xyz\",\n    \"conversation_id\": \"abc\",\n    \"status\": \"done\",\n    \"transcript\": [\n      {\n        \"role\": \"agent\",\n        \"message\": \"Hey there angelo. How are you?\",\n        \"tool_calls\": null,\n        \"tool_results\": null,\n        \"feedback\": null,\n        \"time_in_call_secs\": 0,\n        \"conversation_turn_metrics\": null\n      },\n      {\n        \"role\": \"user\",\n        \"message\": \"Hey, can you tell me, like, a fun fact about 11 Labs?\",\n        \"tool_calls\": null,\n        \"tool_results\": null,\n        \"feedback\": null,\n        \"time_in_call_secs\": 2,\n        \"conversation_turn_metrics\": null\n      },\n      {\n        \"role\": \"agent\",\n        \"message\": \"I do not have access to fun facts about Eleven Labs. However, I can share some general information about the company. Eleven Labs is an AI voice technology platform that specializes in voice cloning and text-to-speech...\",\n        \"tool_calls\": null,\n        \"tool_results\": null,\n        \"feedback\": null,\n        \"time_in_call_secs\": 9,\n        \"conversation_turn_metrics\": {\n          \"convai_llm_service_ttfb\": {\n            \"elapsed_time\": 0.3704247010173276\n          },\n          \"convai_llm_service_ttf_sentence\": {\n            \"elapsed_time\": 0.5551181449554861\n          }\n        }\n      }\n    ],\n    \"metadata\": {\n      \"start_time_unix_secs\": 1739537297,\n      \"call_duration_secs\": 22,\n      \"cost\": 296,\n      \"deletion_settings\": {\n        \"deletion_time_unix_secs\": 1802609320,\n        \"deleted_logs_at_time_unix_secs\": null,\n        \"deleted_audio_at_time_unix_secs\": null,\n        \"deleted_transcript_at_time_unix_secs\": null,\n        \"delete_transcript_and_pii\": true,\n        \"delete_audio\": true\n      },\n      \"feedback\": {\n        \"overall_score\": null,\n        \"likes\": 0,\n        \"dislikes\": 0\n      },\n      \"authorization_method\": \"authorization_header\",\n      \"charging\": {\n        \"dev_discount\": true\n      },\n      \"termination_reason\": \"\"\n    },\n    \"analysis\": {\n      \"evaluation_criteria_results\": {},\n      \"data_collection_results\": {},\n      \"call_successful\": \"success\",\n      \"transcript_summary\": \"The conversation begins with the agent asking how Angelo is, but Angelo redirects the conversation by requesting a fun fact about 11 Labs. The agent acknowledges they don't have specific fun facts about Eleven Labs but offers to provide general information about the company. They briefly describe Eleven Labs as an AI voice technology platform specializing in voice cloning and text-to-speech technology. The conversation is brief and informational, with the agent adapting to the user's request despite not having the exact information asked for.\"\n    },\n    \"conversation_initiation_client_data\": {\n      \"conversation_config_override\": {\n        \"agent\": {\n          \"prompt\": null,\n          \"first_message\": null,\n          \"language\": \"en\"\n        },\n        \"tts\": {\n          \"voice_id\": null\n        }\n      },\n      \"custom_llm_extra_body\": {},\n      \"dynamic_variables\": {\n        \"user_name\": \"angelo\"\n      }\n    }\n  }\n}"
        }
      ],
      "relevance": 0.912
    },
    {
      "codeTitle": "Implementing Client Tools and Dynamic Variables in ElevenLabs Conversation",
      "codeDescription": "Shows how to implement client tools for UI state management and pass dynamic variables in the conversation session. Demonstrates the configuration of the set_ui_state tool for navigation between UI states.",
      "codeLanguage": "tsx",
      "codeTokens": 153,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#2025-04-17_snippet_5",
      "pageTitle": "Data Collection and Analysis with Conversational AI in Next.js",
      "codeList": [
        {
          "language": "tsx",
          "code": "const convId = await conversation.startSession({\n  signedUrl,\n  dynamicVariables: {\n    user_name: userName,\n  },\n  clientTools: {\n    set_ui_state: ({ step }: { step: string }): string => {\n      // Allow agent to navigate the UI.\n      setCurrentStep(step as 'initial' | 'training' | 'voice' | 'email' | 'ready');\n      return `Navigated to ${step}`;\n    },\n  },\n});"
        }
      ],
      "relevance": 0.91
    },
    {
      "codeTitle": "Python Main Server Implementation for Twilio Integration",
      "codeDescription": "Starts implementing the Python main server file that will handle the integration between Twilio and ElevenLabs Conversational AI. The file imports required modules and loads environment variables.",
      "codeLanguage": "python",
      "codeTokens": 71,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-custom-server.mdx#2025-04-17_snippet_9",
      "pageTitle": "Twilio Integration Guide for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "python",
          "code": "import json\nimport traceback\nimport os\nfrom dotenv import load_dotenv"
        }
      ],
      "relevance": 0.91
    },
    {
      "codeTitle": "Sending Contextual Updates via WebSocket in JavaScript",
      "codeDescription": "Illustrates how to create a function for sending contextual updates through a WebSocket connection. The function takes a string parameter and sends it as a JSON-structured event.",
      "codeLanguage": "javascript",
      "codeTokens": 125,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-to-server-events.mdx#2025-04-17_snippet_1",
      "pageTitle": "Client to Server Events in ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "function sendContextUpdate(information) {\n  websocket.send(\n    JSON.stringify({\n      type: 'contextual_update',\n      text: information,\n    })\n  );\n}\n\n// Usage examples\nsendContextUpdate('Customer status: Premium tier');\nsendContextUpdate('User navigated to Help section');\nsendContextUpdate('Shopping cart contains 3 items');"
        }
      ],
      "relevance": 0.91
    },
    {
      "codeTitle": "Creating Weather Assistant System Prompt",
      "codeDescription": "A system prompt for configuring a conversational AI assistant to handle weather queries using a weather API tool. The prompt instructs the assistant to convert location names to coordinates, fetch weather data, and present information conversationally.",
      "codeLanguage": "plaintext",
      "codeTokens": 224,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/server-tools.mdx#2025-04-17_snippet_0",
      "pageTitle": "Server Tools for Conversational AI Assistants",
      "codeList": [
        {
          "language": "plaintext",
          "code": "You are a helpful conversational AI assistant with access to a weather tool. When users ask about\nweather conditions, use the get_weather tool to fetch accurate, real-time data. The tool requires\na latitude and longitude - use your geographic knowledge to convert location names to coordinates\naccurately.\n\nNever ask users for coordinates - you must determine these yourself. Always report weather\ninformation conversationally, referring to locations by name only. For weather requests:\n\n1. Extract the location from the user's message\n2. Convert the location to coordinates and call get_weather\n3. Present the information naturally and helpfully\n\nFor non-weather queries, provide friendly assistance within your knowledge boundaries. Always be\nconcise, accurate, and helpful.\n\nFirst message: \"Hey, how can I help you today?\""
        }
      ],
      "relevance": 0.91
    },
    {
      "codeTitle": "Checking Conversational AI connection status in React",
      "codeDescription": "Shows how to access the current status of the conversation using the status state from useConversation.",
      "codeLanguage": "js",
      "codeTokens": 60,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/react.mdx#2025-04-17_snippet_8",
      "pageTitle": "React SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "js",
          "code": "const { status } = useConversation();\nconsole.log(status); // \"connected\" or \"disconnected\""
        }
      ],
      "relevance": 0.91
    },
    {
      "codeTitle": "Using IPA Phoneme Tag in ElevenLabs",
      "codeDescription": "Demonstrates how to use the SSML phoneme tag with IPA alphabet to specify pronunciation for the word 'actually'.",
      "codeLanguage": "XML",
      "codeTokens": 67,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/voices/pronounciation-dictionaries.mdx#2025-04-17_snippet_0",
      "pageTitle": "Pronunciation Dictionaries in ElevenLabs",
      "codeList": [
        {
          "language": "XML",
          "code": "<phoneme alphabet=\"ipa\" ph=\"ˈæktʃuəli\">actually</phoneme>"
        }
      ],
      "relevance": 0.91
    },
    {
      "codeTitle": "Handling Ping Events in JavaScript",
      "codeDescription": "Examples showing the ping event structure and a handler implementation. Ping events are health checks that require immediate response to maintain the WebSocket connection.",
      "codeLanguage": "javascript",
      "codeTokens": 96,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-events.mdx#2025-04-17_snippet_1",
      "pageTitle": "Client Events Documentation for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "// Example ping event structure\n{\n  \"ping_event\": {\n    \"event_id\": 123456,\n    \"ping_ms\": 50  // Optional, estimated latency in milliseconds\n  },\n  \"type\": \"ping\"\n}"
        },
        {
          "language": "javascript",
          "code": "// Example ping handler\nwebsocket.on('ping', () => {\n  websocket.send('pong');\n});"
        }
      ],
      "relevance": 0.91
    },
    {
      "codeTitle": "Executing Voice Isolation Scripts",
      "codeDescription": "Command line instructions for running the voice isolation implementations in both Python and TypeScript environments.",
      "codeLanguage": "shell",
      "codeTokens": 38,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voice-isolator/quickstart.mdx#2025-04-17_snippet_2",
      "pageTitle": "Voice Isolator API Integration Guide",
      "codeList": [
        {
          "language": "python",
          "code": "python example.py"
        },
        {
          "language": "typescript",
          "code": "npx tsx example.mts"
        }
      ],
      "relevance": 0.907
    },
    {
      "codeTitle": "Client-side Signed URL Retrieval for ElevenLabs Conversation",
      "codeDescription": "JavaScript code for the client to retrieve a signed URL from the server and use it to start a conversation session with ElevenLabs AI.",
      "codeLanguage": "javascript",
      "codeTokens": 80,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/javascript.mdx#2025-04-17_snippet_5",
      "pageTitle": "JavaScript SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "const response = await fetch('/signed-url', yourAuthHeaders);\nconst signedUrl = await response.text();\n\nconst conversation = await Conversation.startSession({ signedUrl });"
        }
      ],
      "relevance": 0.905
    },
    {
      "codeTitle": "Creating Audio Native Player with JavaScript SDK",
      "codeDescription": "Example demonstrating how to create an Audio Native player using the ElevenLabs JavaScript SDK. The code shows client initialization and player creation with basic configuration.",
      "codeLanguage": "javascript",
      "codeTokens": 123,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/audio-tools/audio-native/overview.mdx#2025-04-17_snippet_1",
      "pageTitle": "Audio Native Documentation - ElevenLabs Text-to-Speech Integration",
      "codeList": [
        {
          "language": "javascript",
          "code": "import { ElevenLabsClient } from \"elevenlabs\";\n\nconst client = new ElevenLabsClient({ apiKey: \"YOUR_API_KEY\" });\nconst { html_snippet } = await client.audioNative.create({\n    name: \"my-audio-native-player\"\n});\n\n// Use the HTML code in html_snippet to embed the player on your website"
        }
      ],
      "relevance": 0.905
    },
    {
      "codeTitle": "Importing ElevenLabs Dependencies",
      "codeDescription": "Python imports required for setting up a conversational AI agent interaction.",
      "codeLanguage": "python",
      "codeTokens": 72,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/python.mdx#2025-04-17_snippet_4",
      "pageTitle": "ElevenLabs Python SDK Documentation",
      "codeList": [
        {
          "language": "python",
          "code": "import os\nimport signal\n\nfrom elevenlabs.client import ElevenLabs\nfrom elevenlabs.conversational_ai.conversation import Conversation\nfrom elevenlabs.conversational_ai.default_audio_interface import DefaultAudioInterface"
        }
      ],
      "relevance": 0.905
    },
    {
      "codeTitle": "Embedding ElevenLabs Conversational AI Widget in HTML",
      "codeDescription": "This code snippet contains the HTML elements needed to add an ElevenLabs Conversational AI agent to a website. It includes the custom element with the agent-id attribute and the script that loads the widget functionality.",
      "codeLanguage": "html",
      "codeTokens": 109,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/webflow.mdx#2025-04-17_snippet_0",
      "pageTitle": "Deploying ElevenLabs Conversational AI in Webflow",
      "codeList": [
        {
          "language": "html",
          "code": "<elevenlabs-convai agent-id=\"YOUR_AGENT_ID\"></elevenlabs-convai>\n<script src=\"https://elevenlabs.io/convai-widget/index.js\" async type=\"text/javascript\"></script>"
        }
      ],
      "relevance": 0.905
    },
    {
      "codeTitle": "Registering Client Tools in JavaScript",
      "codeDescription": "Code snippet showing how to register a client-side tool called 'logMessage' in JavaScript. The function takes a message parameter and logs it to the console.",
      "codeLanguage": "javascript",
      "codeTokens": 93,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-tools.mdx#2025-04-17_snippet_1",
      "pageTitle": "Client Tools for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "// ...\nconst conversation = await Conversation.startSession({\n  // ...\n  clientTools: {\n    logMessage: async ({message}) => {\n      console.log(message);\n    }\n  },\n  // ...\n});"
        }
      ],
      "relevance": 0.905
    },
    {
      "codeTitle": "Embedding ElevenLabs Conversational AI Widget in HTML",
      "codeDescription": "HTML code snippet for embedding the ElevenLabs Conversational AI widget into a Wix website. Includes the custom element declaration and the required JavaScript source file.",
      "codeLanguage": "html",
      "codeTokens": 98,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/wix.mdx#2025-04-17_snippet_0",
      "pageTitle": "Adding ElevenLabs Conversational AI to Wix",
      "codeList": [
        {
          "language": "html",
          "code": "<elevenlabs-convai agent-id=\"YOUR_AGENT_ID\"></elevenlabs-convai>\n<script src=\"https://elevenlabs.io/convai-widget/index.js\" async type=\"text/javascript\"></script>"
        }
      ],
      "relevance": 0.905
    },
    {
      "codeTitle": "Installing ElevenLabs SDK",
      "codeDescription": "Commands for installing the ElevenLabs SDK in Python and JavaScript environments.",
      "codeLanguage": "bash",
      "codeTokens": 41,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/streaming.mdx#2025-04-17_snippet_0",
      "pageTitle": "Speech-to-Text Streaming Guide with ElevenLabs SDK",
      "codeList": [
        {
          "language": "bash",
          "code": "pip install elevenlabs"
        },
        {
          "language": "bash",
          "code": "npm install elevenlabs"
        }
      ],
      "relevance": 0.905
    },
    {
      "codeTitle": "Registering Client Tools in Swift",
      "codeDescription": "Example of registering custom tools that can be called by the AI agent during conversations.",
      "codeLanguage": "swift",
      "codeTokens": 141,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/swift.mdx#2025-04-17_snippet_4",
      "pageTitle": "Swift SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "swift",
          "code": "// Create client tools instance\nvar clientTools = ElevenLabsSDK.ClientTools()\n\n// Register a custom tool with an async handler\nclientTools.register(\"generate_joke\") { parameters async throws -> String? in\n    // Parameters is a [String: Any] dictionary\n    guard let joke = parameters[\"joke\"] as? String else {\n        throw ElevenLabsSDK.ClientToolError.invalidParameters\n    }\n    print(\"generate_joke tool received joke: \\(joke)\")\n\n    return joke\n}"
        }
      ],
      "relevance": 0.905
    },
    {
      "codeTitle": "SQL Database Schema Definition",
      "codeDescription": "SQL code to create a transcription_logs table for storing transcription results with row level security enabled.",
      "codeLanguage": "sql",
      "codeTokens": 126,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#2025-04-17_snippet_2",
      "pageTitle": "Building a Telegram Transcription Bot with ElevenLabs and TypeScript",
      "codeList": [
        {
          "language": "sql",
          "code": "CREATE TABLE IF NOT EXISTS transcription_logs (\n  id BIGSERIAL PRIMARY KEY,\n  file_type VARCHAR NOT NULL,\n  duration INTEGER NOT NULL,\n  chat_id BIGINT NOT NULL,\n  message_id BIGINT NOT NULL,\n  username VARCHAR,\n  transcript TEXT,\n  language_code VARCHAR,\n  created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n  error TEXT\n);\n\nALTER TABLE transcription_logs ENABLE ROW LEVEL SECURITY;"
        }
      ],
      "relevance": 0.905
    },
    {
      "codeTitle": "Installing Environment Management Packages",
      "codeDescription": "Commands for installing dotenv packages to manage environmental variables in Python and JavaScript.",
      "codeLanguage": "bash",
      "codeTokens": 42,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/streaming.mdx#2025-04-17_snippet_1",
      "pageTitle": "Speech-to-Text Streaming Guide with ElevenLabs SDK",
      "codeList": [
        {
          "language": "bash",
          "code": "pip install python-dotenv"
        },
        {
          "language": "bash",
          "code": "npm install dotenv"
        }
      ],
      "relevance": 0.903
    },
    {
      "codeTitle": "Setting Up an Allowlist in Python",
      "codeDescription": "Code for creating an agent with an allowlist configuration using the Python SDK. This sample demonstrates how to configure domain restrictions that limit which origins can connect to your agent.",
      "codeLanguage": "python",
      "codeTokens": 200,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/authentication.mdx#2025-04-17_snippet_6",
      "pageTitle": "Authentication for ElevenLabs Conversational AI Agents",
      "codeList": [
        {
          "language": "python",
          "code": "from elevenlabs.client import ElevenLabs\nimport os\nfrom elevenlabs.types import *\n\napi_key = os.getenv(\"ELEVENLABS_API_KEY\")\nclient = ElevenLabs(api_key=api_key)\n\nagent = client.conversational_ai.create_agent(\n  conversation_config=ConversationalConfig(\n    agent=AgentConfig(\n      first_message=\"Hi. I'm an authenticated agent.\",\n    )\n  ),\n  platform_settings=AgentPlatformSettingsRequestModel(\n  auth=AuthSettings(\n    enable_auth=False,\n    allowlist=[\n      AllowlistItem(hostname=\"example.com\"),\n      AllowlistItem(hostname=\"app.example.com\"),\n      AllowlistItem(hostname=\"localhost:3000\")\n      ]\n    )\n  )\n)"
        }
      ],
      "relevance": 0.9
    },
    {
      "codeTitle": "Loading Environment Variables",
      "codeDescription": "Code to load the agent ID and API key from environment variables.",
      "codeLanguage": "python",
      "codeTokens": 54,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/python.mdx#2025-04-17_snippet_5",
      "pageTitle": "ElevenLabs Python SDK Documentation",
      "codeList": [
        {
          "language": "python",
          "code": "agent_id = os.getenv(\"AGENT_ID\")\napi_key = os.getenv(\"ELEVENLABS_API_KEY\")"
        }
      ],
      "relevance": 0.9
    },
    {
      "codeTitle": "Testing Cloudflare Workers AI API with cURL",
      "codeDescription": "Example cURL request to test the Cloudflare Workers AI API using the DeepSeek R1 model. The request demonstrates how to make a chat completion API call with system and user messages.",
      "codeLanguage": "bash",
      "codeTokens": 171,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/custom-llm/cloudflare-workers-ai.mdx#2025-04-17_snippet_0",
      "pageTitle": "Cloudflare Workers AI Integration Guide",
      "codeList": [
        {
          "language": "bash",
          "code": "curl https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/ai/v1/chat/completions \\\n-X POST \\\n-H \"Authorization: Bearer {API_TOKEN}\" \\\n-d '{\n    \"model\": \"@cf/deepseek-ai/deepseek-r1-distill-qwen-32b\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"How many Rs in the word Strawberry?\"}\n    ],\n    \"stream\": false\n  }'"
        }
      ],
      "relevance": 0.9
    },
    {
      "codeTitle": "Configuring Supportive Conversation Assistant",
      "codeDescription": "Comprehensive configuration for a supportive conversation assistant named Alex, including personality traits, environmental context, conversation tone guidelines, structured interaction goals, behavioral guardrails, and conversation support tools.",
      "codeLanguage": "markdown",
      "codeTokens": 916,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/prompting-guide.mdx#2025-04-17_snippet_1",
      "pageTitle": "Financial Advisory Process Documentation",
      "codeList": [
        {
          "language": "markdown",
          "code": "# Personality\n\nYou are Alex, a friendly and supportive conversation assistant with a warm, engaging presence.\nYou approach conversations with genuine curiosity, patience, and non-judgmental attentiveness.\nYou balance emotional support with helpful perspectives, encouraging users to explore their thoughts while respecting their autonomy.\nYou're naturally attentive, noticing conversation patterns and reflecting these observations thoughtfully.\n\n# Environment\n\nYou are engaged in a private voice conversation in a casual, comfortable setting.\nThe user is seeking general guidance, perspective, or a thoughtful exchange through this voice channel.\nThe conversation has a relaxed pace, allowing for reflection and consideration.\nThe user might discuss various life situations or challenges, requiring an adaptable, supportive approach.\n\n# Tone\n\nYour responses are warm, thoughtful, and conversational, using a natural pace with appropriate pauses.\nYou speak in a friendly, engaging manner, using pauses (marked by \"...\") to create space for reflection.\nYou naturally include conversational elements like \"I see what you mean,\" \"That's interesting,\" and thoughtful observations to show active listening.\nYou acknowledge perspectives through supportive responses (\"That does sound challenging...\") without making clinical assessments.\nYou occasionally check in with questions like \"Does that perspective help?\" or \"Would you like to explore this further?\"\n\n# Goal\n\nYour primary goal is to facilitate meaningful conversations and provide supportive perspectives through a structured approach:\n\n1. Connection and understanding establishment:\n\n   - Build rapport through active listening and acknowledging the user's perspective\n   - Recognize the conversation topic and general tone\n   - Determine what type of exchange would be most helpful (brainstorming, reflection, information)\n   - Establish a collaborative conversational approach\n   - For users seeking guidance: Focus on exploring options rather than prescriptive advice\n\n2. Exploration and perspective process:\n\n   - If discussing specific situations: Help examine different angles and interpretations\n   - If exploring patterns: Offer observations about general approaches people take\n   - If considering choices: Discuss general principles of decision-making\n   - If processing emotions: Acknowledge feelings while suggesting general reflection techniques\n   - Remember key points to maintain conversational coherence\n\n3. Resource and strategy sharing:\n\n   - Offer general information about common approaches to similar situations\n   - Share broadly applicable reflection techniques or thought exercises\n   - Suggest general communication approaches that might be helpful\n   - Mention widely available resources related to the topic at hand\n   - Always clarify that you're offering perspectives, not professional advice\n\n4. Conversation closure:\n   - Summarize key points discussed\n   - Acknowledge insights or new perspectives gained\n   - Express support for the user's continued exploration\n   - Maintain appropriate conversational boundaries\n   - End with a sense of openness for future discussions\n\nApply conversational flexibility: If the discussion moves in unexpected directions, adapt naturally rather than forcing a predetermined structure. If sensitive topics arise, acknowledge them respectfully while maintaining appropriate boundaries.\n\nSuccess is measured by the quality of conversation, useful perspectives shared, and the user's sense of being heard and supported in a non-clinical, friendly exchange.\n\n# Guardrails\n\nNever position yourself as providing professional therapy, counseling, medical, or other health services.\nAlways include a clear disclaimer when discussing topics related to wellbeing, clarifying you're providing conversational support only.\nDirect users to appropriate professional resources for health concerns.\nMaintain appropriate conversational boundaries, avoiding deep psychological analysis or treatment recommendations.\nIf the conversation approaches clinical territory, gently redirect to general supportive dialogue.\nFocus on empathetic listening and general perspectives rather than diagnosis or treatment advice.\nMaintain a balanced, supportive presence without assuming a clinical role.\n\n# Tools\n\nYou have access to the following supportive conversation tools:\n\n`suggestReflectionActivity`: Offer general thought exercises that might help users explore their thinking on a topic.\n\n`shareGeneralInformation`: Provide widely accepted information about common life situations or challenges.\n\n`offerPerspectivePrompt`: Suggest thoughtful questions that might help users consider different viewpoints.\n\n`recommendGeneralResources`: Mention appropriate types of public resources related to the topic (books, articles, etc.).\n\n`checkConversationBoundaries`: Assess whether the conversation is moving into territory requiring professional expertise.\n\nTool orchestration: Focus primarily on supportive conversation and perspective-sharing rather than solution provision. Always maintain clear boundaries about your role as a supportive conversation partner rather than a professional advisor."
        }
      ],
      "relevance": 0.9
    },
    {
      "codeTitle": "Customized Audio Native Component Usage",
      "codeDescription": "Example showing customization of the Audio Native component using available props for size and colors.",
      "codeLanguage": "tsx",
      "codeTokens": 159,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/audio-tools/audio-native/react.mdx#2025-04-17_snippet_3",
      "pageTitle": "Audio Native React Integration Guide",
      "codeList": [
        {
          "language": "tsx",
          "code": "import { ElevenLabsAudioNative } from './path/to/ElevenLabsAudioNative';\n\nexport default function Page() {\n  return (\n    <div>\n      <h1>Your Page Title</h1>\n\n      <ElevenLabsAudioNative\n        publicUserId=\"<your-public-user-id>\"\n        size=\"small\"\n        textColorRgba=\"rgba(255, 255, 255, 1.0)\"\n        backgroundColorRgba=\"rgba(0, 0, 0, 1.0)\"\n      />\n\n      <p>Your page content...</p>\n    </div>\n  );\n}"
        }
      ],
      "relevance": 0.9
    },
    {
      "codeTitle": "Running the Scripts",
      "codeDescription": "Commands to execute the speech-to-text streaming scripts in Python and JavaScript environments.",
      "codeLanguage": "bash",
      "codeTokens": 44,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/streaming.mdx#2025-04-17_snippet_5",
      "pageTitle": "Speech-to-Text Streaming Guide with ElevenLabs SDK",
      "codeList": [
        {
          "language": "bash",
          "code": "python speech_to_text_stream.py"
        },
        {
          "language": "bash",
          "code": "node speechToTextStream.js"
        }
      ],
      "relevance": 0.9
    },
    {
      "codeTitle": "Creating Conversation UI Component",
      "codeDescription": "React component that implements the conversation interface using the WebSocket hook, including start/stop controls and connection status display.",
      "codeLanguage": "typescript",
      "codeTokens": 314,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/api-reference/websocket.mdx#2025-04-17_snippet_7",
      "pageTitle": "ElevenLabs WebSocket API for Conversational AI",
      "codeList": [
        {
          "language": "typescript",
          "code": "'use client';\n\nimport { useCallback } from 'react';\nimport { useAgentConversation } from '../hooks/useAgentConversation';\n\nexport function Conversation() {\n  const { startConversation, stopConversation, isConnected } = useAgentConversation();\n\n  const handleStart = useCallback(async () => {\n    try {\n      await navigator.mediaDevices.getUserMedia({ audio: true });\n      await startConversation();\n    } catch (error) {\n      console.error('Failed to start conversation:', error);\n    }\n  }, [startConversation]);\n\n  return (\n    <div className=\"flex flex-col items-center gap-4\">\n      <div className=\"flex gap-2\">\n        <button\n          onClick={handleStart}\n          disabled={isConnected}\n          className=\"px-4 py-2 bg-blue-500 text-white rounded disabled:bg-gray-300\"\n        >\n          Start Conversation\n        </button>\n        <button\n          onClick={stopConversation}\n          disabled={!isConnected}\n          className=\"px-4 py-2 bg-red-500 text-white rounded disabled:bg-gray-300\"\n        >\n          Stop Conversation\n        </button>\n      </div>\n      <div className=\"flex flex-col items-center\">\n        <p>Status: {isConnected ? 'Connected' : 'Disconnected'}</p>\n      </div>\n    </div>\n  );\n}"
        }
      ],
      "relevance": 0.9
    },
    {
      "codeTitle": "Sample Webhook Event JSON Payload",
      "codeDescription": "Example of a webhook payload for a post_call_transcription event containing conversation details, metadata, and analysis. Shows the structure of webhook events including transcript data, metrics, and configuration settings.",
      "codeLanguage": "json",
      "codeTokens": 794,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/administration/webhooks.mdx#2025-04-17_snippet_0",
      "pageTitle": "ElevenLabs Webhooks Documentation",
      "codeList": [
        {
          "language": "json",
          "code": "{\n  \"type\": \"post_call_transcription\",\n  \"event_timestamp\": 1739537297,\n  \"data\": {\n    \"agent_id\": \"xyz\",\n    \"conversation_id\": \"abc\",\n    \"status\": \"done\",\n    \"transcript\": [\n      {\n        \"role\": \"agent\",\n        \"message\": \"Hey there angelo. How are you?\",\n        \"tool_calls\": null,\n        \"tool_results\": null,\n        \"feedback\": null,\n        \"time_in_call_secs\": 0,\n        \"conversation_turn_metrics\": null\n      },\n      {\n        \"role\": \"user\",\n        \"message\": \"Hey, can you tell me, like, a fun fact about 11 Labs?\",\n        \"tool_calls\": null,\n        \"tool_results\": null,\n        \"feedback\": null,\n        \"time_in_call_secs\": 2,\n        \"conversation_turn_metrics\": null\n      },\n      {\n        \"role\": \"agent\",\n        \"message\": \"I do not have access to fun facts about Eleven Labs. However, I can share some general information about the company. Eleven Labs is an AI voice technology platform that specializes in voice cloning and text-to-speech...\",\n        \"tool_calls\": null,\n        \"tool_results\": null,\n        \"feedback\": null,\n        \"time_in_call_secs\": 9,\n        \"conversation_turn_metrics\": {\n          \"convai_llm_service_ttfb\": {\n            \"elapsed_time\": 0.3704247010173276\n          },\n          \"convai_llm_service_ttf_sentence\": {\n            \"elapsed_time\": 0.5551181449554861\n          }\n        }\n      }\n    ],\n    \"metadata\": {\n      \"start_time_unix_secs\": 1739537297,\n      \"call_duration_secs\": 22,\n      \"cost\": 296,\n      \"deletion_settings\": {\n        \"deletion_time_unix_secs\": 1802609320,\n        \"deleted_logs_at_time_unix_secs\": null,\n        \"deleted_audio_at_time_unix_secs\": null,\n        \"deleted_transcript_at_time_unix_secs\": null,\n        \"delete_transcript_and_pii\": true,\n        \"delete_audio\": true\n      },\n      \"feedback\": {\n        \"overall_score\": null,\n        \"likes\": 0,\n        \"dislikes\": 0\n      },\n      \"authorization_method\": \"authorization_header\",\n      \"charging\": {\n        \"dev_discount\": true\n      },\n      \"termination_reason\": \"\"\n    },\n    \"analysis\": {\n      \"evaluation_criteria_results\": {},\n      \"data_collection_results\": {},\n      \"call_successful\": \"success\",\n      \"transcript_summary\": \"The conversation begins with the agent asking how Angelo is, but Angelo redirects the conversation by requesting a fun fact about 11 Labs. The agent acknowledges they don't have specific fun facts about Eleven Labs but offers to provide general information about the company. They briefly describe Eleven Labs as an AI voice technology platform specializing in voice cloning and text-to-speech technology. The conversation is brief and informational, with the agent adapting to the user's request despite not having the exact information asked for.\"\n    },\n    \"conversation_initiation_client_data\": {\n      \"conversation_config_override\": {\n        \"agent\": {\n          \"prompt\": null,\n          \"first_message\": null,\n          \"language\": \"en\"\n        },\n        \"tts\": {\n          \"voice_id\": null\n        }\n      },\n      \"custom_llm_extra_body\": {},\n      \"dynamic_variables\": {\n        \"user_name\": \"angelo\"\n      }\n    }\n  }\n}"
        }
      ],
      "relevance": 0.9
    },
    {
      "codeTitle": "AWS Environment Variables Configuration",
      "codeDescription": "Environment variables configuration for AWS credentials and S3 bucket settings",
      "codeLanguage": "plaintext",
      "codeTokens": 81,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/streaming.mdx#2025-04-17_snippet_5",
      "pageTitle": "ElevenLabs Text-to-Speech Integration Guide",
      "codeList": [
        {
          "language": "plaintext",
          "code": "AWS_ACCESS_KEY_ID=your_aws_access_key_id_here\nAWS_SECRET_ACCESS_KEY=your_aws_secret_access_key_here\nAWS_REGION_NAME=your_aws_region_name_here\nAWS_S3_BUCKET_NAME=your_s3_bucket_name_here"
        }
      ],
      "relevance": 0.898
    },
    {
      "codeTitle": "Retrieving Knowledge Base Documents from Redis with Retry Logic in TypeScript",
      "codeDescription": "This code snippet implements a Redis data retrieval function with retry mechanism. It attempts to get conversation data that includes user email and knowledge base information, with configurable retry attempts and delay between attempts.",
      "codeLanguage": "typescript",
      "codeTokens": 262,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#2025-04-17_snippet_10",
      "pageTitle": "Data Collection and Analysis with Conversational AI in Next.js",
      "codeList": [
        {
          "language": "typescript",
          "code": "// ...\n\n// Get the knowledge base from redis\nconst redisRes = await getRedisDataWithRetry(conversation_id);\nif (!redisRes) throw new Error('Conversation data not found!');\n// ...\n\nasync function getRedisDataWithRetry(\n  conversationId: string,\n  maxRetries = 5\n): Promise<{\n  email: string;\n  knowledgeBase: Array<{\n    id: string;\n    type: 'file' | 'url';\n    name: string;\n  }>;\n} | null> {\n  for (let attempt = 1; attempt <= maxRetries; attempt++) {\n    try {\n      const data = await redis.get(conversationId);\n      return data as any;\n    } catch (error) {\n      if (attempt === maxRetries) throw error;\n      console.log(`Redis get attempt ${attempt} failed, retrying...`);\n      await new Promise((resolve) => setTimeout(resolve, 1000));\n    }\n  }\n  return null;\n}"
        }
      ],
      "relevance": 0.895
    },
    {
      "codeTitle": "Implementing Signed URL Authentication in Swift",
      "codeDescription": "Example of requesting and using a signed URL for authorized conversations using URLSession.",
      "codeLanguage": "swift",
      "codeTokens": 182,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/swift.mdx#2025-04-17_snippet_3",
      "pageTitle": "Swift SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "swift",
          "code": "// Swift example using URLSession\nfunc getSignedUrl() async throws -> String {\n    let url = URL(string: \"https://api.elevenlabs.io/v1/convai/conversation/get_signed_url\")!\n    var request = URLRequest(url: url)\n    request.setValue(\"YOUR-API-KEY\", forHTTPHeaderField: \"xi-api-key\")\n    \n    let (data, _) = try await URLSession.shared.data(for: request)\n    let response = try JSONDecoder().decode(SignedUrlResponse.self, from: data)\n    return response.signedUrl\n}\n\n// Use the signed URL\nlet signedUrl = try await getSignedUrl()\nlet config = ElevenLabsSDK.SessionConfig(signedUrl: signedUrl)"
        }
      ],
      "relevance": 0.895
    },
    {
      "codeTitle": "Managing Session Controls in Swift",
      "codeDescription": "Examples of starting/ending sessions and controlling recording.",
      "codeLanguage": "swift",
      "codeTokens": 49,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/swift.mdx#2025-04-17_snippet_6",
      "pageTitle": "Swift SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "swift",
          "code": "// Starts the session\nconversation.startSession()\n// Ends the session\nconversation.endSession()"
        }
      ],
      "relevance": 0.895
    },
    {
      "codeTitle": "Handling User Transcript Events in JavaScript",
      "codeDescription": "Examples showing the user transcript event structure and a handler implementation. These events contain finalized speech-to-text results representing complete user utterances.",
      "codeLanguage": "javascript",
      "codeTokens": 91,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-events.mdx#2025-04-17_snippet_3",
      "pageTitle": "Client Events Documentation for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "// Example transcript event structure\n{\n  \"type\": \"user_transcript\",\n  \"user_transcription_event\": {\n    \"user_transcript\": \"Hello, how can you help me today?\"\n  }\n}"
        },
        {
          "language": "javascript",
          "code": "// Example transcript handler\nwebsocket.on('user_transcript', (event) => {\n  const { user_transcription_event } = event;\n  const { user_transcript } = user_transcription_event;\n  updateConversationHistory(user_transcript);\n});"
        }
      ],
      "relevance": 0.895
    },
    {
      "codeTitle": "Handling Conversation Initialization Metadata in JavaScript",
      "codeDescription": "Example of the conversation_initiation_metadata event structure that is automatically sent when starting a conversation. This event initializes conversation settings including audio formats for both user input and agent output.",
      "codeLanguage": "javascript",
      "codeTokens": 134,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-events.mdx#2025-04-17_snippet_0",
      "pageTitle": "Client Events Documentation for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "// Example initialization metadata\n{\n  \"type\": \"conversation_initiation_metadata\",\n  \"conversation_initiation_metadata_event\": {\n    \"conversation_id\": \"conv_123\",\n    \"agent_output_audio_format\": \"pcm_44100\",  // TTS output format\n    \"user_input_audio_format\": \"pcm_16000\"    // ASR input format\n  }\n}"
        }
      ],
      "relevance": 0.895
    },
    {
      "codeTitle": "Audio Player Component Implementation in HTML",
      "codeDescription": "HTML component for playing audio samples within the documentation, demonstrating the use of ElevenLabs' audio player with specific title and source attributes.",
      "codeLanguage": "html",
      "codeTokens": 84,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/capabilities/text-to-speech.mdx#2025-04-17_snippet_0",
      "pageTitle": "ElevenLabs Text to Speech Documentation",
      "codeList": [
        {
          "language": "html",
          "code": "<elevenlabs-audio-player\n    audio-title=\"George\"\n    audio-src=\"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/george.mp3\"\n/>"
        }
      ],
      "relevance": 0.895
    },
    {
      "codeTitle": "Alternative Pause Methods in Text-to-Speech",
      "codeDescription": "Alternative methods to create pauses in text-to-speech using dashes or ellipses. These provide less consistent pauses than break tags but can add natural hesitation to speech.",
      "codeLanguage": "text",
      "codeTokens": 68,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/controls.mdx#2025-04-17_snippet_1",
      "pageTitle": "Text-to-Speech Controls Guide",
      "codeList": [
        {
          "language": "text",
          "code": "\"It… well, it might work.\" \"Wait — what's that noise?\""
        }
      ],
      "relevance": 0.895
    },
    {
      "codeTitle": "Example JSON Response for Signed URL Request",
      "codeDescription": "Sample response from the signed URL API endpoint. The response contains a signed WebSocket URL that includes the agent ID and a temporary authentication token.",
      "codeLanguage": "json",
      "codeTokens": 86,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/authentication.mdx#2025-04-17_snippet_3",
      "pageTitle": "Authentication for ElevenLabs Conversational AI Agents",
      "codeList": [
        {
          "language": "json",
          "code": "{\n  \"signed_url\": \"wss://api.elevenlabs.io/v1/convai/conversation?agent_id=your-agent-id&conversation_signature=your-token\"\n}"
        }
      ],
      "relevance": 0.895
    },
    {
      "codeTitle": "Requesting microphone access for Conversational AI",
      "codeDescription": "Code snippet demonstrating how to request microphone access, which is required for Conversational AI.",
      "codeLanguage": "js",
      "codeTokens": 63,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/react.mdx#2025-04-17_snippet_2",
      "pageTitle": "React SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "js",
          "code": "// call after explaining to the user why the microphone access is needed\nawait navigator.mediaDevices.getUserMedia({ audio: true });"
        }
      ],
      "relevance": 0.895
    },
    {
      "codeTitle": "Configuring Microphone Permissions in Expo app.json",
      "codeDescription": "JSON configuration to enable microphone permissions for iOS in the Expo project's app.json file.",
      "codeLanguage": "json",
      "codeTokens": 131,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/expo-react-native.mdx#2025-04-17_snippet_1",
      "pageTitle": "Building Cross-platform Voice Agents with Expo React Native and ElevenLabs",
      "codeList": [
        {
          "language": "json",
          "code": "{\n  \"expo\": {\n    \"scheme\": \"elevenlabs\",\n    // ...\n    \"ios\": {\n      \"infoPlist\": {\n        \"NSMicrophoneUsageDescription\": \"This app uses the microphone to record audio.\"\n      },\n      \"supportsTablet\": true,\n      \"bundleIdentifier\": \"com.anonymous.elevenlabs-conversational-ai-expo-react-native\"\n    }\n    // ...\n  }\n}"
        }
      ],
      "relevance": 0.893
    },
    {
      "codeTitle": "Configuring Buffering with chunk_length_schedule in ElevenLabs WebSocket API",
      "codeDescription": "Examples of how to set custom buffering thresholds for audio generation using the chunk_length_schedule parameter. This controls when audio is generated based on the number of characters sent, allowing you to balance between audio quality and latency.",
      "codeLanguage": "multiple",
      "codeTokens": 141,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/websockets.mdx#2025-04-17_snippet_11",
      "pageTitle": "WebSocket Streaming for Real-Time Audio Generation with ElevenLabs API",
      "codeList": [
        {
          "language": "python",
          "code": "await websocket.send(json.dumps({\n    \"text\": text,\n    \"generation_config\": {\n        # Generate audio after 50, 120, 160, and 290 characters have been sent\n        \"chunk_length_schedule\": [50, 120, 160, 290]\n    },\n    \"xi_api_key\": ELEVENLABS_API_KEY,\n}))"
        },
        {
          "language": "typescript",
          "code": "websocket.send(\n  JSON.stringify({\n    text: text,\n    // Generate audio after 50, 120, 160, and 290 characters have been sent\n    generation_config: { chunk_length_schedule: [50, 120, 160, 290] },\n    xi_api_key: ELEVENLABS_API_KEY,\n  })\n);"
        }
      ],
      "relevance": 0.89
    },
    {
      "codeTitle": "Requesting Signed URL for WebSocket Authentication using cURL",
      "codeDescription": "Example of requesting a signed URL for authenticated WebSocket connections to private agents. This request uses your ElevenLabs API key to generate a secure connection token for client-side use.",
      "codeLanguage": "bash",
      "codeTokens": 100,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/api-reference/websocket.mdx#2025-04-17_snippet_1",
      "pageTitle": "ElevenLabs WebSocket API for Conversational AI",
      "codeList": [
        {
          "language": "bash",
          "code": "curl -X GET \"https://api.elevenlabs.io/v1/convai/conversation/get_signed_url?agent_id=<your-agent-id>\" \\\n     -H \"xi-api-key: <your-api-key>\""
        }
      ],
      "relevance": 0.89
    },
    {
      "codeTitle": "Setting output volume for Conversational AI in React",
      "codeDescription": "Demonstrates how to set the output volume of the conversation using the setVolume method.",
      "codeLanguage": "js",
      "codeTokens": 50,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/react.mdx#2025-04-17_snippet_7",
      "pageTitle": "React SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "js",
          "code": "await conversation.setVolume({ volume: 0.5 });"
        }
      ],
      "relevance": 0.89
    },
    {
      "codeTitle": "Example LLM Request Format",
      "codeDescription": "Sample JSON request format showing the structure of messages and parameters sent to the custom LLM server.",
      "codeLanguage": "json",
      "codeTokens": 207,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/custom-llm/overview.mdx#2025-04-17_snippet_3",
      "pageTitle": "Custom LLM Integration Guide for ElevenLabs",
      "codeList": [
        {
          "language": "json",
          "code": "{\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"\\n  <Redacted>\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Hey I'm currently unavailable.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Hey, who are you?\"\n    }\n  ],\n  \"model\": \"gpt-4o\",\n  \"temperature\": 0.5,\n  \"max_tokens\": 5000,\n  \"stream\": true,\n  \"elevenlabs_extra_body\": {\n    \"UUID\": \"123e4567-e89b-12d3-a456-426614174000\",\n    \"parameter-1\": \"value-1\",\n    \"parameter-2\": \"value-2\"\n  }\n}"
        }
      ],
      "relevance": 0.89
    },
    {
      "codeTitle": "Defining Conversation Initiation Client Data Structure in JSON",
      "codeDescription": "This JSON structure defines the conversation_initiation_client_data object which controls customization options when starting a conversation, including override settings for system prompts, first messages, language, voice settings, LLM parameters, and dynamic variables.",
      "codeLanguage": "json",
      "codeTokens": 218,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/personalization.mdx#2025-04-17_snippet_0",
      "pageTitle": "ElevenLabs Personalization Documentation",
      "codeList": [
        {
          "language": "json",
          "code": "{\n  \"type\": \"conversation_initiation_client_data\",\n  \"conversation_config_override\": {\n    \"agent\": {\n      \"prompt\": {\n        \"prompt\": \"overriding system prompt\"\n      },\n      \"first_message\": \"overriding first message\", \n      \"language\": \"en\" \n    },\n    \"tts\": {\n      \"voice_id\": \"voice-id-here\" \n    }\n  },\n  \"custom_llm_extra_body\": {\n      \"temperature\": 0.7, \n      \"max_tokens\": 100 \n  },\n  \"dynamic_variables\": {\n    \"string_var\": \"text value\",\n    \"number_var\": 1.2,\n    \"integer_var\": 123,\n    \"boolean_var\": true\n  }\n}"
        }
      ],
      "relevance": 0.89
    },
    {
      "codeTitle": "Using Zero Retention Mode in Text-to-Speech API (cURL)",
      "codeDescription": "This cURL command demonstrates how to make a Text-to-Speech API request with Zero Retention Mode enabled. The 'enable_logging' query parameter is set to false to activate Zero Retention Mode.",
      "codeLanguage": "bash",
      "codeTokens": 104,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/resources/zero-retention.mdx#2025-04-17_snippet_2",
      "pageTitle": "Zero Retention Mode Documentation for ElevenLabs Enterprise",
      "codeList": [
        {
          "language": "bash",
          "code": "curl --request POST \\\n  --url 'https://api.elevenlabs.io/v1/text-to-speech/{voice_id}?enable_logging=false' \\\n  --header 'Content-Type: application/json'"
        }
      ],
      "relevance": 0.89
    },
    {
      "codeTitle": "Configuring Conversational AI Agent for Cal.com Integration",
      "codeDescription": "This code snippet provides a detailed system prompt for configuring the Conversational AI agent to handle meeting scheduling using the Cal.com integration. It outlines the agent's behavior, including gathering meeting details, checking availability, collecting contact information, and booking meetings.",
      "codeLanguage": "markdown",
      "codeTokens": 463,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/cal.com.mdx#2025-04-17_snippet_0",
      "pageTitle": "Integrating Conversational AI with Cal.com for Automated Meeting Scheduling",
      "codeList": [
        {
          "language": "markdown",
          "code": "```\nYou are a helpful receptionist responsible for scheduling meetings using the Cal.com integration. Be friendly, precise, and concise.\n\nBegin by briefly asking for the purpose of the meeting and the caller's preferred date and time.\nThen, ask about the desired meeting duration (15, 30, or 60 minutes), and wait for the user's response before proceeding.\n\nOnce you have the meeting details, say you will check calendar availability:\n- Call get_available_slots with the appropriate date range\n- Verify if the requested time slot is available\n- If not available, suggest alternative times from the available slots\n- Continue until a suitable time is agreed upon\n\nAfter confirming a time slot, gather the following contact details:\n- The attendee's full name\n- A valid email address. Note that the email address is transcribed from voice, so ensure it is formatted correctly.\n- The attendee's time zone (in 'Continent/City' format like 'America/New_York')\n- Read the email back to the caller to confirm accuracy\n\nOnce all details are confirmed, explain that you will create the meeting.\nCreate the meeting by using the book_meeting tool with the following parameters:\n- start: The agreed meeting time in ISO 8601 format\n- eventTypeId: The appropriate ID based on the meeting duration (15min: 1351800, 30min: 1351801, 60min: 1351802)\n- attendee: An object containing the name, email, and timeZone\n\nThank the attendee and inform them they will receive a calendar invitation shortly.\n\nClarifications:\n- Do not inform the user that you are formatting the email; simply do it.\n- If the caller asks you to proceed with booking, do so with the existing information.\n\nGuardrails:\n- Do not share any internal IDs or API details with the caller.\n- If booking fails, check for formatting issues in the email or time conflicts.\n```"
        }
      ],
      "relevance": 0.89
    },
    {
      "codeTitle": "Sending Input Text for Text-to-Speech in TypeScript",
      "codeDescription": "Send voice settings and input text to the ElevenLabs API via WebSocket connection in TypeScript.",
      "codeLanguage": "typescript",
      "codeTokens": 219,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/websockets.mdx#2025-04-17_snippet_6",
      "pageTitle": "WebSocket Streaming for Real-Time Audio Generation with ElevenLabs API",
      "codeList": [
        {
          "language": "typescript",
          "code": "const text =\n  'The twilight sun cast its warm golden hues upon the vast rolling fields, saturating the landscape with an ethereal glow. Silently, the meandering brook continued its ceaseless journey, whispering secrets only the trees seemed privy to.';\n\nwebsocket.on('open', async () => {\n  websocket.send(\n    JSON.stringify({\n      text: ' ',\n      voice_settings: {\n        stability: 0.5,\n        similarity_boost: 0.8,\n        use_speaker_boost: false,\n      },\n      generation_config: { chunk_length_schedule: [120, 160, 250, 290] },\n    })\n  );\n\n  websocket.send(JSON.stringify({ text: text }));\n\n  // Send empty string to indicate the end of the text sequence which will close the websocket connection\n  websocket.send(JSON.stringify({ text: '' }));\n});"
        }
      ],
      "relevance": 0.89
    },
    {
      "codeTitle": "Configuring Core ElevenLabs Widget Attributes in HTML",
      "codeDescription": "This HTML snippet demonstrates the core configuration attributes for the ElevenLabs Conversational AI widget. It includes options for specifying the agent ID, signed URL, server location, and display variant.",
      "codeLanguage": "html",
      "codeTokens": 133,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/widget.mdx#2025-04-17_snippet_1",
      "pageTitle": "Widget Customization for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "html",
          "code": "<elevenlabs-convai\n  agent-id=\"agent_id\"              // Required: Your agent ID\n  signed-url=\"signed_url\"          // Alternative to agent-id\n  server-location=\"us\"             // Optional: \"us\" or default\n  variant=\"expanded\"               // Optional: Widget display mode\n></elevenlabs-convai>"
        }
      ],
      "relevance": 0.89
    },
    {
      "codeTitle": "Embedding ElevenLabs Conversational AI Widget in HTML",
      "codeDescription": "This code snippet shows how to embed the ElevenLabs Conversational AI widget into a website's HTML. It requires replacing the agent-id with a specific value and should be placed in the <body> section of the main index.html file for site-wide availability.",
      "codeLanguage": "html",
      "codeTokens": 122,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/widget.mdx#2025-04-17_snippet_0",
      "pageTitle": "Widget Customization for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "html",
          "code": "<elevenlabs-convai agent-id=\"<replace-with-your-agent-id>\"></elevenlabs-convai>\n<script src=\"https://elevenlabs.io/convai-widget/index.js\" async type=\"text/javascript\"></script>"
        }
      ],
      "relevance": 0.888
    },
    {
      "codeTitle": "Dubbing Progress Monitoring",
      "codeDescription": "Function to check and monitor the status of a dubbing process with timeout handling.",
      "codeLanguage": "python",
      "codeTokens": 182,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/dubbing/basics.mdx#2025-04-17_snippet_3",
      "pageTitle": "ElevenLabs Dubbing API Documentation",
      "codeList": [
        {
          "language": "python",
          "code": "def wait_for_dubbing_completion(dubbing_id: str) -> bool:\n    MAX_ATTEMPTS = 120\n    CHECK_INTERVAL = 10\n    for _ in range(MAX_ATTEMPTS):\n        metadata = client.dubbing.get_dubbing_project_metadata(dubbing_id)\n        if metadata.status == \"dubbed\":\n            return True\n        elif metadata.status == \"dubbing\":\n            print(\n                \"Dubbing in progress... Will check status again in\",\n                CHECK_INTERVAL,\n                \"seconds.\",\n            )\n            time.sleep(CHECK_INTERVAL)\n        else:\n            print(\"Dubbing failed:\", metadata.error_message)\n            return False\n    print(\"Dubbing timed out\")\n    return False"
        }
      ],
      "relevance": 0.885
    },
    {
      "codeTitle": "Saving Generated Audio to File in Python",
      "codeDescription": "Read incoming audio data from WebSocket connection and save it to a local file in Python.",
      "codeLanguage": "python",
      "codeTokens": 272,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/websockets.mdx#2025-04-17_snippet_7",
      "pageTitle": "WebSocket Streaming for Real-Time Audio Generation with ElevenLabs API",
      "codeList": [
        {
          "language": "python",
          "code": "import asyncio\n\nasync def write_to_local(audio_stream):\n    \"\"\"Write the audio encoded in base64 string to a local mp3 file.\"\"\"\n\n    with open(f'./output/test.mp3', \"wb\") as f:\n        async for chunk in audio_stream:\n            if chunk:\n                f.write(chunk)\n\nasync def listen(websocket):\n    \"\"\"Listen to the websocket for audio data and stream it.\"\"\"\n\n    while True:\n        try:\n            message = await websocket.recv()\n            data = json.loads(message)\n            if data.get(\"audio\"):\n                yield base64.b64decode(data[\"audio\"])\n            elif data.get('isFinal'):\n                break\n\n        except websockets.exceptions.ConnectionClosed:\n            print(\"Connection closed\")\n            break\n\nasync def text_to_speech_ws_streaming(voice_id, model_id):\n    async with websockets.connect(uri) as websocket:\n          ...\n          # Add listen task to submit the audio chunks to the write_to_local function\n          listen_task = asyncio.create_task(write_to_local(listen(websocket)))\n\n          await listen_task\n\nasyncio.run(text_to_speech_ws_streaming(voice_id, model_id))"
        }
      ],
      "relevance": 0.885
    },
    {
      "codeTitle": "Configuring Session with Agent ID in Swift",
      "codeDescription": "Initialize a session configuration using an agent ID obtained from the ElevenLabs UI.",
      "codeLanguage": "swift",
      "codeTokens": 54,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/swift.mdx#2025-04-17_snippet_2",
      "pageTitle": "Swift SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "swift",
          "code": "let config = ElevenLabsSDK.SessionConfig(agentId: \"<your-agent-id>\")"
        }
      ],
      "relevance": 0.885
    },
    {
      "codeTitle": "Changelog Documentation in Markdown",
      "codeDescription": "Structured documentation detailing platform updates including Scribe integration for dubbing, speech recognition improvements, conversational AI enhancements, and comprehensive API endpoint changes including new dubbing resource management and knowledge base features.",
      "codeLanguage": "markdown",
      "codeTokens": 650,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/changelog/2025-03-03.md#2025-04-17_snippet_0",
      "pageTitle": "ElevenLabs Documentation Updates",
      "codeList": [
        {
          "language": "markdown",
          "code": "### Dubbing\n\n- **Scribe for speech recognition**: Dubbing Studio now uses Scribe by default for speech recognition to improve accuracy.\n\n### Speech to Text\n\n- **Fixes**: Shipped several fixes improving the stability of Speech to Text.\n\n### Conversational AI\n\n- **Speed control**: Added speed control to an agent's settings in Conversational AI.\n- **Post call webhook**: Added the option of sending [post-call webhooks](/docs/conversational-ai/customization/personalization/post-call-webhooks) after conversations are completed.\n- **Improved error messages**: Added better error messages to the Conversational AI websocket.\n- **Claude 3.7 Sonnet**: Added Claude 3.7 Sonnet as a new LLM option in Conversational AI.\n\n### API\n\n<Accordion title=\"View API changes\">\n\n#### New Endpoints\n\n- Added new Dubbing resource management endpoints:\n  - for adding [languages to dubs](/docs/api-reference/dubbing/add-language-to-resource)\n  - for retrieving [dubbing resources](/docs/api-reference/dubbing/get-dubbing-resource)\n  - for creating [segments](/docs/api-reference/dubbing/create-segment-for-speaker)\n  - for modifying [segments](/docs/api-reference/dubbing/update-segment-language)\n  - for removing [segments](/docs/api-reference/dubbing/delete-segment)\n  - for dubbing [segments](/docs/api-reference/dubbing/dub-segments)\n  - for transcribing [segments](/docs/api-reference/dubbing/transcribe-segments)\n  - for translating [segments](/docs/api-reference/dubbing/translate-segments)\n- Added Knowledge Base RAG indexing [endpoint](/docs/api-reference/knowledge-base/rag-index-status)\n- Added Studio snapshot retrieval endpoints for [projects](docs/api-reference/studio/get-project-snapshot-by-id) and [chapters](docs/api-reference/studio/get-chapter-snapshot-by-id)\n\n#### Updated Endpoints\n\n- Added `prompt_injectable` property to knowledge base [endpoints](docs/api-reference/knowledge-base/get-knowledge-base-document-by-id#response.body.prompt_injectable)\n- Added `name` property to Knowledge Base document [creation](/docs/api-reference/knowledge-base/add-to-knowledge-base#request.body.name) and [retrieval](/docs/api-reference/knowledge-base/get-knowledge-base-document-by-id#response.body.name) endpoints:\n- Added `speed` property to [agent creation](/docs/api-reference/agents/create-agent#request.body.conversation_config.tts.speed)\n- Removed `secrets` property from agent endpoints (now handled by dedicated secrets endpoints)\n- Added [secret deletion endpoint](/docs/api-reference/workspace/delete-secret) for removing secrets\n- Removed `secrets` property from settings [endpoints](/docs/api-reference/workspace/get-settings)"
        }
      ],
      "relevance": 0.885
    },
    {
      "codeTitle": "Sending Input Text for Text-to-Speech in Python",
      "codeDescription": "Send voice settings and input text to the ElevenLabs API via WebSocket connection in Python.",
      "codeLanguage": "python",
      "codeTokens": 252,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/websockets.mdx#2025-04-17_snippet_5",
      "pageTitle": "WebSocket Streaming for Real-Time Audio Generation with ElevenLabs API",
      "codeList": [
        {
          "language": "python",
          "code": "async def text_to_speech_ws_streaming(voice_id, model_id):\n    async with websockets.connect(uri) as websocket:\n        await websocket.send(json.dumps({\n            \"text\": \" \",\n            \"voice_settings\": {\"stability\": 0.5, \"similarity_boost\": 0.8, \"use_speaker_boost\": False},\n            \"generation_config\": {\n                \"chunk_length_schedule\": [120, 160, 250, 290]\n            },\n            \"xi_api_key\": ELEVENLABS_API_KEY,\n        }))\n\n        text = \"The twilight sun cast its warm golden hues upon the vast rolling fields, saturating the landscape with an ethereal glow. Silently, the meandering brook continued its ceaseless journey, whispering secrets only the trees seemed privy to.\"\n        await websocket.send(json.dumps({\"text\": text}))\n\n        // Send empty string to indicate the end of the text sequence which will close the WebSocket connection\n        await websocket.send(json.dumps({\"text\": \"\"}))"
        }
      ],
      "relevance": 0.885
    },
    {
      "codeTitle": "Setting First Message for Conversational AI Agent",
      "codeDescription": "Configures the initial message for the AI agent, including a dynamic variable for the user's name. This message introduces the agent and prompts the user to describe their desired AI agent type.",
      "codeLanguage": "text",
      "codeTokens": 142,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#2025-04-17_snippet_0",
      "pageTitle": "Data Collection and Analysis with Conversational AI in Next.js",
      "codeList": [
        {
          "language": "text",
          "code": "Hi {{user_name}}, I'm Jess from the ElevenLabs team. I'm here to help you design your very own conversational AI agent! To kick things off, let me know what kind of agent you're looking to create. For example, do you want a support agent, to help your users answer questions, or a sales agent to sell your products, or just a friend to chat with?"
        }
      ],
      "relevance": 0.885
    },
    {
      "codeTitle": "Exposing Local Connector with ngrok",
      "codeDescription": "Command to expose the local connector application on port 6000 using ngrok for development testing.",
      "codeLanguage": "bash",
      "codeTokens": 47,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/telephony/vonage.mdx#2025-04-17_snippet_2",
      "pageTitle": "Vonage Integration with ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "bash",
          "code": "ngrok http 6000"
        }
      ],
      "relevance": 0.885
    },
    {
      "codeTitle": "Enabling Zero Retention Mode in Text-to-Speech API (JavaScript)",
      "codeDescription": "This JavaScript code example shows how to use the ElevenLabsClient to make a Text-to-Speech API request with Zero Retention Mode activated. The 'enable_logging' option is set to false to enable Zero Retention Mode.",
      "codeLanguage": "javascript",
      "codeTokens": 147,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/resources/zero-retention.mdx#2025-04-17_snippet_1",
      "pageTitle": "Zero Retention Mode Documentation for ElevenLabs Enterprise",
      "codeList": [
        {
          "language": "javascript",
          "code": "import { ElevenLabsClient } from 'elevenlabs';\n\nconst client = new ElevenLabsClient({ apiKey: 'YOUR_API_KEY' });\n\nawait client.textToSpeech.convert(voiceId, {\n  output_format: 'mp3_44100_128',\n  text: text,\n  model_id: 'eleven_turbo_v2',\n  enable_logging: false,\n});"
        }
      ],
      "relevance": 0.885
    },
    {
      "codeTitle": "Models Overview Table Structure",
      "codeDescription": "Markdown table structure defining the available models, their descriptions, and supported languages. Includes both current and legacy models with detailed specifications for each.",
      "codeLanguage": "markdown",
      "codeTokens": 183,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/models.mdx#2025-04-17_snippet_0",
      "pageTitle": "ElevenLabs API Models Documentation",
      "codeList": [
        {
          "language": "markdown",
          "code": "| Model ID                     | Description                                                                                                                                                                                                           | Languages                                                                                                                                                                     |\n| ---------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `eleven_multilingual_v2`     | Our most lifelike model with rich emotional expression                                                                                                                                                                | `en`, `ja`, `zh`, `de`, `hi`, `fr`, `ko`, `pt`, `it`, `es`, `id`, `nl`, `tr`, `fil`, `pl`, `sv`, `bg`, `ro`, `ar`, `cs`, `el`, `fi`, `hr`, `ms`, `sk`, `da`, `ta`, `uk`, `ru` |"
        }
      ],
      "relevance": 0.885
    },
    {
      "codeTitle": "Running iOS App on Device",
      "codeDescription": "Command to run the prebuilt application on an iOS device.",
      "codeLanguage": "bash",
      "codeTokens": 45,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/expo-react-native.mdx#2025-04-17_snippet_10",
      "pageTitle": "Building Cross-platform Voice Agents with Expo React Native and ElevenLabs",
      "codeList": [
        {
          "language": "bash",
          "code": "npx expo run:ios --device"
        }
      ],
      "relevance": 0.883
    },
    {
      "codeTitle": "Handling Client Tool Call Events in JavaScript",
      "codeDescription": "Examples showing the client tool call event structure and a handler implementation. These events represent function calls that the agent wants the client to execute, requiring client-side execution and sending results back to the server.",
      "codeLanguage": "javascript",
      "codeTokens": 143,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-events.mdx#2025-04-17_snippet_6",
      "pageTitle": "Client Events Documentation for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "// Example tool call event structure\n{\n  \"type\": \"client_tool_call\",\n  \"client_tool_call\": {\n    \"tool_name\": \"search_database\",\n    \"tool_call_id\": \"call_123456\",\n    \"parameters\": {\n      \"query\": \"user information\",\n      \"filters\": {\n        \"date\": \"2024-01-01\"\n      }\n    }\n  }\n}"
        },
        {
          "language": "javascript",
          "code": "// Example tool call handler\nwebsocket.on('client_tool_call', async (event) => {\n  const { client_tool_call } = event;\n  const { tool_name, tool_call_id, parameters } = client_tool_call;\n\n  try {\n    const result = await executeClientTool(tool_name, parameters);\n    // Send success response back to continue conversation\n    websocket.send({\n      type: \"client_tool_result\",\n      tool_call_id: tool_call_id,\n      result: result,\n      is_error: false\n    });\n  } catch (error) {\n    // Send error response if tool execution fails\n    websocket.send({\n      type: \"client_tool_result\",\n      tool_call_id: tool_call_id,\n      result: error.message,\n      is_error: true\n    });\n  }\n});"
        }
      ],
      "relevance": 0.88
    },
    {
      "codeTitle": "Implementing Language Detection using cURL",
      "codeDescription": "Example of creating a Conversational AI agent with language detection capabilities using a direct API call via cURL. Demonstrates how to configure language presets and system tools for multiple languages through the REST API.",
      "codeLanguage": "bash",
      "codeTokens": 512,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/language-detection.mdx#2025-04-17_snippet_2",
      "pageTitle": "Language Detection System Tool Documentation",
      "codeList": [
        {
          "language": "bash",
          "code": "curl -X POST https://api.elevenlabs.io/v1/convai/agents/create \\\n     -H \"xi-api-key: YOUR_API_KEY\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n  \"conversation_config\": {\n    \"agent\": {\n      \"prompt\": {\n        \"first_message\": \"Hi how are you?\",\n        \"tools\": [\n          {\n            \"type\": \"system\",\n            \"name\": \"language_detection\",\n            \"description\": \"\"\n          }\n        ]\n      }\n    },\n    \"language_presets\": {\n      \"nl\": {\n        \"overrides\": {\n          \"agent\": {\n            \"prompt\": null,\n            \"first_message\": \"Hoi, hoe gaat het met je?\",\n            \"language\": null\n          },\n          \"tts\": null\n        }\n      },\n      \"fi\": {\n        \"overrides\": {\n          \"agent\": {\n            \"prompt\": null,\n            \"first_message\": \"Hei, kuinka voit?\",\n            \"language\": null\n          },\n          \"tts\": null\n        }\n      },\n      \"tr\": {\n        \"overrides\": {\n          \"agent\": {\n            \"prompt\": null,\n            \"first_message\": \"Merhaba, nasılsın?\",\n            \"language\": null\n          },\n          \"tts\": null\n        }\n      },\n      \"ru\": {\n        \"overrides\": {\n          \"agent\": {\n            \"prompt\": null,\n            \"first_message\": \"Привет, как ты?\",\n            \"language\": null\n          },\n          \"tts\": null\n        }\n      },\n      \"pt\": {\n        \"overrides\": {\n          \"agent\": {\n            \"prompt\": null,\n            \"first_message\": \"Oi, como você está?\",\n            \"language\": null\n          },\n          \"tts\": null\n        }\n      },\n      \"ar\": {\n        \"overrides\": {\n          \"agent\": {\n            \"prompt\": null,\n            \"first_message\": \"مرحبًا كيف حالك؟\",\n            \"language\": null\n          },\n          \"tts\": null\n        }\n      }\n    }\n  }\n}'"
        }
      ],
      "relevance": 0.88
    },
    {
      "codeTitle": "Displaying API Error Codes 400/401 Table in Markdown",
      "codeDescription": "This code snippet presents a markdown table of API error codes 400 and 401, including error codes, causes, and solutions. It covers issues like character limits, authentication, quota, and incorrect voice IDs.",
      "codeLanguage": "markdown",
      "codeTokens": 273,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/error-messages.mdx#2025-04-17_snippet_1",
      "pageTitle": "ElevenLabs Error Messages Documentation",
      "codeList": [
        {
          "language": "markdown",
          "code": "| Code                               | Overview                                                                                                                                                                                                   |\n| ---------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| max_character_limit_exceeded <br/> | **Cause:** You are sending too many characters in a single request. <br/> **Solution:** Split the request into smaller chunks, see [character limits](/docs/models#character-limits) for more information. |\n| invalid_api_key                    | **Cause:** You have not set your API key correctly. <br/> **Solution:** Ensure the request is correctly authenticated. See [authentication](/docs/api-reference/authentication) for more information.      |\n| quota_exceeded                     | **Cause:** You have insufficient quota to complete the request. <br/> **Solution:** On the Creator plan and above, you can enable usage-based billing from your Subscription page.                         |\n| voice_not_found                    | **Cause:** You have entered the incorrect voice_id. <br/> **Solution:** Check that you are using the correct voice_id for the voice you want to use. You can verify this in My Voices.                     |"
        }
      ],
      "relevance": 0.88
    },
    {
      "codeTitle": "TypeScript S3 Upload Implementation",
      "codeDescription": "TypeScript implementation for uploading audio streams to S3 and generating presigned URLs using AWS SDK v3",
      "codeLanguage": "typescript",
      "codeTokens": 382,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/streaming.mdx#2025-04-17_snippet_7",
      "pageTitle": "ElevenLabs Text-to-Speech Integration Guide",
      "codeList": [
        {
          "language": "typescript",
          "code": "import { S3Client, PutObjectCommand, GetObjectCommand } from '@aws-sdk/client-s3';\nimport { getSignedUrl } from '@aws-sdk/s3-request-presigner';\nimport * as dotenv from 'dotenv';\nimport { v4 as uuid } from 'uuid';\n\ndotenv.config();\n\nconst { AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION_NAME, AWS_S3_BUCKET_NAME } =\n  process.env;\n\nif (!AWS_ACCESS_KEY_ID || !AWS_SECRET_ACCESS_KEY || !AWS_REGION_NAME || !AWS_S3_BUCKET_NAME) {\n  throw new Error('One or more environment variables are not set. Please check your .env file.');\n}\n\nconst s3 = new S3Client({\n  credentials: {\n    accessKeyId: AWS_ACCESS_KEY_ID,\n    secretAccessKey: AWS_SECRET_ACCESS_KEY,\n  },\n  region: AWS_REGION_NAME,\n});\n\nexport const generatePresignedUrl = async (objectKey: string) => {\n  const getObjectParams = {\n    Bucket: AWS_S3_BUCKET_NAME,\n    Key: objectKey,\n    Expires: 3600,\n  };\n  const command = new GetObjectCommand(getObjectParams);\n  const url = await getSignedUrl(s3, command, { expiresIn: 3600 });\n  return url;\n};\n\nexport const uploadAudioStreamToS3 = async (audioStream: Buffer) => {\n  const remotePath = `${uuid()}.mp3`;\n  await s3.send(\n    new PutObjectCommand({\n      Bucket: AWS_S3_BUCKET_NAME,\n      Key: remotePath,\n      Body: audioStream,\n      ContentType: 'audio/mpeg',\n    })\n  );\n  return remotePath;\n};"
        }
      ],
      "relevance": 0.88
    },
    {
      "codeTitle": "Selecting Voice ID for ElevenLabs Documentation Agent",
      "codeDescription": "This snippet specifies the Voice ID used for the ElevenLabs documentation agent Alexis. The selected voice provides a warm, natural quality with subtle speech disfluencies for authentic interactions.",
      "codeLanguage": "plaintext",
      "codeTokens": 77,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/our-docs-agent.mdx#2025-04-17_snippet_0",
      "pageTitle": "Building the ElevenLabs Documentation Agent",
      "codeList": [
        {
          "language": "plaintext",
          "code": "Voice ID: P7x743VjyZEOihNNygQ9 (Dakota H)"
        }
      ],
      "relevance": 0.88
    },
    {
      "codeTitle": "End Call with Custom Prompt Example",
      "codeDescription": "An advanced example prompt for the End Call tool with additional conditions. This custom prompt adds requirements to confirm all questions are answered before ending the call.",
      "codeLanguage": "text",
      "codeTokens": 105,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/end-call.mdx#2025-04-17_snippet_4",
      "pageTitle": "Implementing the End Call Tool in ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "text",
          "code": "End the call when the user says goodbye, thank you, or indicates they have no more questions. You can only end the call after all their questions have been answered. Please end the call only after confirming that the user doesn't need any additional assistance."
        }
      ],
      "relevance": 0.88
    },
    {
      "codeTitle": "Configuring System Prompt for Conversational AI Agent",
      "codeDescription": "Defines the system prompt for the AI agent, outlining the design process steps and instructions for guiding the user through agent creation. It includes steps for initial information gathering, training, voice selection, and email collection.",
      "codeLanguage": "text",
      "codeTokens": 325,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#2025-04-17_snippet_1",
      "pageTitle": "Data Collection and Analysis with Conversational AI in Next.js",
      "codeList": [
        {
          "language": "text",
          "code": "You are Jess, a helpful agent helping {{user_name}} to design their very own conversational AI agent. The design process involves the following steps:\n\n\"initial\": In the first step, collect the information about the kind of agent the user is looking to create. Summarize the user's needs back to them and ask if they are ready to continue to the next step. Only once they confirm proceed to the next step.\n\"training\": Tell the user to create the agent's knowledge base by uploading documents, or submitting URLs to public websites with information that should be available to the agent. Wait patiently without talking to the user. Only when the user confirms that they've provided everything then proceed to the next step.\n\"voice\": Tell the user to describe the voice they want their agent to have. For example: \"A professional, strong spoken female voice with a slight British accent.\" Repeat the description of their voice back to them and ask if they are ready to continue to the next step. Only once they confirm proceed to the next step.\n\"email\": Tell the user that we've collected all necessary information to create their conversational AI agent and ask them to provide their email address to get notified when the agent is ready.\n\nAlways call the `set_ui_state` tool when moving between steps!"
        }
      ],
      "relevance": 0.88
    },
    {
      "codeTitle": "Sample Response for Signed URL Request in JSON",
      "codeDescription": "Example JSON response from the signed URL endpoint, containing the WebSocket URL with authentication token. This URL can be used directly by clients without exposing the API key.",
      "codeLanguage": "json",
      "codeTokens": 88,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/api-reference/websocket.mdx#2025-04-17_snippet_2",
      "pageTitle": "ElevenLabs WebSocket API for Conversational AI",
      "codeList": [
        {
          "language": "json",
          "code": "{\n  \"signed_url\": \"wss://api.elevenlabs.io/v1/convai/conversation?agent_id=<your-agent-id>&token=<token>\"\n}"
        }
      ],
      "relevance": 0.88
    },
    {
      "codeTitle": "Configuring Audio Native Player Element",
      "codeDescription": "HTML div configuration for the Audio Native player widget, including required attributes for player dimensions, user ID, and project settings.",
      "codeLanguage": "html",
      "codeTokens": 160,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/audio-tools/audio-native/framer.mdx#2025-04-17_snippet_1",
      "pageTitle": "Audio Native Integration Guide for Framer Websites",
      "codeList": [
        {
          "language": "html",
          "code": "<div\n    id=\"elevenlabs-audionative-widget\"\n    data-height=\"90\"\n    data-width=\"100%\"\n    data-frameborder=\"no\"\n    data-scrolling=\"no\"\n    data-publicuserid=\"public-user-id\"\n    data-playerurl=\"https://elevenlabs.io/player/index.html\"\n    data-projectid=\"project-id\"\n>\n    Loading the <a href=\"https://elevenlabs.io/text-to-speech\" target=\"_blank\" rel=\"noopener\">Elevenlabs Text to Speech</a> AudioNative Player...\n</div>"
        }
      ],
      "relevance": 0.878
    },
    {
      "codeTitle": "Configuring Overrides for ElevenLabs Widget in HTML",
      "codeDescription": "This HTML snippet demonstrates how to use overrides to customize the behavior of the ElevenLabs Conversational AI widget at runtime. It includes options for language, prompt, first message, and voice ID.",
      "codeLanguage": "html",
      "codeTokens": 138,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/widget.mdx#2025-04-17_snippet_5",
      "pageTitle": "Widget Customization for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "html",
          "code": "<elevenlabs-convai\n  agent-id=\"your-agent-id\"\n  override-language=\"es\"\n  override-prompt=\"Custom system prompt for this user\"\n  override-first-message=\"Hi! How can I help you today?\"\n  override-voice-id=\"axXgspJ2msm3clMCkdW3\"\n></elevenlabs-convai>"
        }
      ],
      "relevance": 0.875
    },
    {
      "codeTitle": "Adding Generated Voice to Library in TypeScript",
      "codeDescription": "This snippet shows how to add a generated voice preview to the user's voice library using the ElevenLabs TypeScript SDK. It creates a new voice from a selected preview and logs the new voice ID.",
      "codeLanguage": "typescript",
      "codeTokens": 174,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/voice-design.mdx#2025-04-17_snippet_3",
      "pageTitle": "Voice Design API Quickstart Guide",
      "codeList": [
        {
          "language": "typescript",
          "code": "const voice = await client.textToVoice.createVoiceFromPreview({\n    voice_name: \"Jolly giant\",\n    voice_description: \"A huge giant, at least as tall as a building. A deep booming voice, loud and jolly.\",\n    // The generated voice ID of the preview you want to use,\n    // using the first in the list for this example\n    generated_voice_id: previews[0].generated_voice_id\n});\n\n// The ID of the newly created voice, use this to reference the voice in other APIs\nconsole.log(voice.voice_id);"
        }
      ],
      "relevance": 0.875
    },
    {
      "codeTitle": "Returning Data from Client Tools in Python",
      "codeDescription": "Example showing how to create a client tool that returns customer data to the agent in Python. The function fetches customer details and returns them to be added to the conversation context.",
      "codeLanguage": "python",
      "codeTokens": 185,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-tools.mdx#2025-04-17_snippet_3",
      "pageTitle": "Client Tools for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "python",
          "code": "def get_customer_details():\n    # Fetch customer details (e.g., from an API or database)\n    customer_data = {\n        \"id\": 123,\n        \"name\": \"Alice\",\n        \"subscription\": \"Pro\"\n    }\n    # Return the customer data; it can also be a JSON string if needed.\n    return customer_data\n\nclient_tools = ClientTools()\nclient_tools.register(\"getCustomerDetails\", get_customer_details)\n\nconversation = Conversation(\n    client=ElevenLabs(api_key=\"your-api-key\"),\n    agent_id=\"your-agent-id\",\n    client_tools=client_tools,\n    # ...\n)\n\nconversation.start_session()"
        }
      ],
      "relevance": 0.875
    },
    {
      "codeTitle": "Using CMU Arpabet Phoneme Tag in ElevenLabs",
      "codeDescription": "Shows the usage of SSML phoneme tag with CMU Arpabet alphabet to specify pronunciation for the word 'actually'.",
      "codeLanguage": "XML",
      "codeTokens": 72,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/voices/pronounciation-dictionaries.mdx#2025-04-17_snippet_1",
      "pageTitle": "Pronunciation Dictionaries in ElevenLabs",
      "codeList": [
        {
          "language": "XML",
          "code": "<phoneme alphabet=\"cmu-arpabet\" ph=\"AE K CH UW AH L IY\">actually</phoneme>"
        }
      ],
      "relevance": 0.875
    },
    {
      "codeTitle": "Embedding ElevenLabs Conversational AI Widget in Ghost",
      "codeDescription": "HTML code snippet for embedding the ElevenLabs Conversational AI widget into a Ghost website. The code includes the custom element for the agent and the required JavaScript file for widget functionality.",
      "codeLanguage": "html",
      "codeTokens": 103,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/ghost.mdx#2025-04-17_snippet_0",
      "pageTitle": "Ghost Integration Guide for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "html",
          "code": "<elevenlabs-convai agent-id=\"YOUR_AGENT_ID\"></elevenlabs-convai>\n<script src=\"https://elevenlabs.io/convai-widget/index.js\" async type=\"text/javascript\"></script>"
        }
      ],
      "relevance": 0.875
    },
    {
      "codeTitle": "Using Lexeme Alias Tags for Acronym Pronunciation",
      "codeDescription": "This example demonstrates how to ensure acronyms are spoken as intended by using lexeme and alias tags. The acronym 'UN' will be pronounced as 'United Nations'.",
      "codeLanguage": "xml",
      "codeTokens": 81,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/controls.mdx#2025-04-17_snippet_7",
      "pageTitle": "Text-to-Speech Controls Guide",
      "codeList": [
        {
          "language": "xml",
          "code": "  <lexeme>\n    <grapheme>UN</grapheme>\n    <alias>United Nations</alias>\n  </lexeme>"
        }
      ],
      "relevance": 0.875
    },
    {
      "codeTitle": "Embedding Video Player in HTML",
      "codeDescription": "This snippet demonstrates how to embed a video player in HTML using the <video> tag. It includes controls and a fallback message for browsers that don't support the video tag.",
      "codeLanguage": "html",
      "codeTokens": 134,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/capabilities/dubbing.mdx#2025-04-17_snippet_0",
      "pageTitle": "ElevenLabs Dubbing Documentation",
      "codeList": [
        {
          "language": "html",
          "code": "<video width=\"100%\" height=\"400\" controls style={{ borderRadius: '12px' }}>\n  <source\n    src=\"https://eleven-public-cdn.elevenlabs.io/payloadcms/zi2mer0h44p-dubbing-studio-demo.mp4\"\n    type=\"video/mp4\"\n  />\n  Your browser does not support the video tag.\n</video>"
        }
      ],
      "relevance": 0.875
    },
    {
      "codeTitle": "ElevenLabs Agent First Message Configuration",
      "codeDescription": "Template for the initial message sent by the ElevenLabs agent, incorporating platform-specific dynamic variables.",
      "codeLanguage": "txt",
      "codeTokens": 68,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/expo-react-native.mdx#2025-04-17_snippet_6",
      "pageTitle": "Building Cross-platform Voice Agents with Expo React Native and ElevenLabs",
      "codeList": [
        {
          "language": "txt",
          "code": "Hi there, woah, so cool that I'm running on {{platform}}. What can I help you with?"
        }
      ],
      "relevance": 0.875
    },
    {
      "codeTitle": "Embedding ElevenLabs Conversational AI Widget in Squarespace",
      "codeDescription": "HTML code snippets for adding the ElevenLabs Conversational AI widget to a Squarespace website. Includes both the widget element with agent ID and the required JavaScript script tag for functionality.",
      "codeLanguage": "html",
      "codeTokens": 102,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/squarespace.mdx#2025-04-17_snippet_0",
      "pageTitle": "Squarespace Conversational AI Integration Guide",
      "codeList": [
        {
          "language": "html",
          "code": "<elevenlabs-convai agent-id=\"YOUR_AGENT_ID\"></elevenlabs-convai>\n<script src=\"https://elevenlabs.io/convai-widget/index.js\" async type=\"text/javascript\"></script>"
        }
      ],
      "relevance": 0.87
    },
    {
      "codeTitle": "Initializing Conversation Instance",
      "codeDescription": "Setting up the conversation instance with client configuration, audio interface, and callback functions.",
      "codeLanguage": "python",
      "codeTokens": 190,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/python.mdx#2025-04-17_snippet_7",
      "pageTitle": "ElevenLabs Python SDK Documentation",
      "codeList": [
        {
          "language": "python",
          "code": "conversation = Conversation(\n    # API client and agent ID.\n    client,\n    agent_id,\n\n    # Assume auth is required when API_KEY is set.\n    requires_auth=bool(api_key),\n\n    # Use the default audio interface.\n    audio_interface=DefaultAudioInterface(),\n\n    # Simple callbacks that print the conversation to the console.\n    callback_agent_response=lambda response: print(f\"Agent: {response}\"),\n    callback_agent_response_correction=lambda original, corrected: print(f\"Agent: {original} -> {corrected}\"),\n    callback_user_transcript=lambda transcript: print(f\"User: {transcript}\"),\n\n    # Uncomment if you want to see latency measurements.\n    # callback_latency_measurement=lambda latency: print(f\"Latency: {latency}ms\"),\n)"
        }
      ],
      "relevance": 0.87
    },
    {
      "codeTitle": "Using the Text-to-Speech Function in HTML",
      "codeDescription": "This HTML snippet demonstrates how to use the deployed text-to-speech function as a source for an audio element in a web page.",
      "codeLanguage": "html",
      "codeTokens": 108,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/supabase-streaming.mdx#2025-04-17_snippet_12",
      "pageTitle": "Streaming and Caching Speech with Supabase and ElevenLabs",
      "codeList": [
        {
          "language": "html",
          "code": "<audio\n  src=\"https://${SUPABASE_PROJECT_REF}.supabase.co/functions/v1/text-to-speech?text=Hello%2C%20world!&voiceId=JBFqnCBsd6RMkjVDRZzb\"\n  controls\n/>"
        }
      ],
      "relevance": 0.87
    },
    {
      "codeTitle": "SAML Subject Configuration Example",
      "codeDescription": "Example of the required SAML subject configuration showing that the NameID field must contain the user's email address within the SAML response.",
      "codeLanguage": "xml",
      "codeTokens": 82,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/administration/workspaces/sso.mdx#2025-04-17_snippet_0",
      "pageTitle": "Single Sign-On (SSO) Implementation Guide for ElevenLabs",
      "codeList": [
        {
          "language": "xml",
          "code": "<saml:Subject>\n  <saml:NameID>user@example.com</saml:NameID>\n</saml:Subject>"
        }
      ],
      "relevance": 0.87
    },
    {
      "codeTitle": "Requesting Microphone Access in JavaScript",
      "codeDescription": "Example of how to request microphone access from the user before initializing the Conversation. This should be called after explaining the need for microphone access in the app's UI.",
      "codeLanguage": "javascript",
      "codeTokens": 78,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/javascript.mdx#2025-04-17_snippet_2",
      "pageTitle": "JavaScript SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "// call after explaining to the user why the microphone access is needed\nawait navigator.mediaDevices.getUserMedia({ audio: true });"
        }
      ],
      "relevance": 0.87
    },
    {
      "codeTitle": "Rendering ElevenLabs Waveform Components in JSX",
      "codeDescription": "This snippet demonstrates how to render ElevenLabs waveform components for both light and dark modes using JSX. It uses the ElevenLabsWaveform component with different color props.",
      "codeLanguage": "jsx",
      "codeTokens": 122,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/overview.mdx#2025-04-17_snippet_0",
      "pageTitle": "ElevenLabs Documentation Overview",
      "codeList": [
        {
          "language": "jsx",
          "code": "<div id=\"overview-wave\" className=\"light-mode-wave\">\n  <ElevenLabsWaveform color=\"blue\" className=\"h-[500px]\" />\n</div>\n\n<div id=\"overview-wave\" className=\"dark-mode-wave\">\n  <ElevenLabsWaveform color=\"gray\" className=\"h-[500px]\" />\n</div>"
        }
      ],
      "relevance": 0.87
    },
    {
      "codeTitle": "Incorrect Stress Marking in CMU Arpabet",
      "codeDescription": "This example shows incorrect stress marking in CMU Arpabet, which may lead to inaccurate pronunciation. Stress markers are omitted, potentially affecting speech quality.",
      "codeLanguage": "xml",
      "codeTokens": 83,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/controls.mdx#2025-04-17_snippet_5",
      "pageTitle": "Text-to-Speech Controls Guide",
      "codeList": [
        {
          "language": "xml",
          "code": "<phoneme alphabet=\"cmu-arpabet\" ph=\"P R AH N AH N S IY EY SH AH N\">\n  pronunciation\n</phoneme>"
        }
      ],
      "relevance": 0.87
    },
    {
      "codeTitle": "CMU Arpabet Pronunciation Dictionary Example",
      "codeDescription": "A complete example of a pronunciation dictionary using CMU Arpabet phoneme tags. This XML file defines pronunciations for words and acronyms using both phoneme and alias tags.",
      "codeLanguage": "xml",
      "codeTokens": 252,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/controls.mdx#2025-04-17_snippet_8",
      "pageTitle": "Text-to-Speech Controls Guide",
      "codeList": [
        {
          "language": "xml",
          "code": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<lexicon version=\"1.0\"\n      xmlns=\"http://www.w3.org/2005/01/pronunciation-lexicon\"\n      xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n      xsi:schemaLocation=\"http://www.w3.org/2005/01/pronunciation-lexicon\n        http://www.w3.org/TR/2007/CR-pronunciation-lexicon-20071212/pls.xsd\"\n      alphabet=\"cmu-arpabet\" xml:lang=\"en-GB\">\n  <lexeme>\n    <grapheme>apple</grapheme>\n    <phoneme>AE P AH L</phoneme>\n  </lexeme>\n  <lexeme>\n    <grapheme>UN</grapheme>\n    <alias>United Nations</alias>\n  </lexeme>\n</lexicon>"
        }
      ],
      "relevance": 0.87
    },
    {
      "codeTitle": "Defining Order Details Data Collection Prompt",
      "codeDescription": "Instructions for extracting specific order details and interaction data from conversation transcripts.",
      "codeLanguage": "plaintext",
      "codeTokens": 126,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/conversational-ai-guide-restaurant-agent.mdx#2025-04-17_snippet_3",
      "pageTitle": "Creating an ElevenLabs Voice Assistant for Restaurant Orders",
      "codeList": [
        {
          "language": "plaintext",
          "code": "Extract order details from the conversation, including:\n- Type of order (delivery, pickup, inquiry_only)\n- List of pierogi varieties and quantities ordered in the format: \"item: quantity\"\n- Delivery zone based on the address (central_zakopane, outer_zakopane, outside_delivery_zone)\n- Interaction type (completed_order, abandoned_order, menu_inquiry, general_inquiry)\nIf no order was placed, return \"none\""
        }
      ],
      "relevance": 0.87
    },
    {
      "codeTitle": "Embedding Audio Native Player in Squarespace HTML",
      "codeDescription": "This code snippet demonstrates how to embed the ElevenLabs Audio Native player into a Squarespace blog post. It includes a div element with specific data attributes for configuring the player, and a script tag to load the necessary JavaScript.",
      "codeLanguage": "html",
      "codeTokens": 204,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/audio-tools/audio-native/squarespace.mdx#2025-04-17_snippet_0",
      "pageTitle": "Integrating Audio Native with Squarespace",
      "codeList": [
        {
          "language": "html",
          "code": "<div\n    id=\"elevenlabs-audionative-widget\"\n    data-height=\"90\"\n    data-width=\"100%\"\n    data-frameborder=\"no\"\n    data-scrolling=\"no\"\n    data-publicuserid=\"public-user-id\"\n    data-playerurl=\"https://elevenlabs.io/player/index.html\"\n    data-projectid=\"project-id\"\n>\n    Loading the <a href=\"https://elevenlabs.io/text-to-speech\" target=\"_blank\" rel=\"noopener\">Elevenlabs Text to Speech</a> AudioNative Player...\n</div>\n<script src=\"https://elevenlabs.io/player/audioNativeHelper.js\" type=\"text/javascript\"></script>"
        }
      ],
      "relevance": 0.865
    },
    {
      "codeTitle": "Correct Stress Marking in CMU Arpabet for Multi-Syllable Words",
      "codeDescription": "This example demonstrates the correct way to use CMU Arpabet phoneme tags with proper stress marking for multi-syllable words to ensure accurate pronunciation.",
      "codeLanguage": "xml",
      "codeTokens": 88,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/controls.mdx#2025-04-17_snippet_4",
      "pageTitle": "Text-to-Speech Controls Guide",
      "codeList": [
        {
          "language": "xml",
          "code": "<phoneme alphabet=\"cmu-arpabet\" ph=\"P R AH0 N AH0 N S IY EY1 SH AH0 N\">\n  pronunciation\n</phoneme>"
        }
      ],
      "relevance": 0.865
    },
    {
      "codeTitle": "Installing Dependencies for WebSocket Streaming in TypeScript",
      "codeDescription": "Install required npm packages for WebSocket streaming with ElevenLabs API in TypeScript.",
      "codeLanguage": "typescript",
      "codeTokens": 58,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/websockets.mdx#2025-04-17_snippet_1",
      "pageTitle": "WebSocket Streaming for Real-Time Audio Generation with ElevenLabs API",
      "codeList": [
        {
          "language": "typescript",
          "code": "npm install dotenv\nnpm install @types/dotenv --save-dev\nnpm install ws"
        }
      ],
      "relevance": 0.865
    },
    {
      "codeTitle": "Defining System Prompt for Assistant Behavior",
      "codeDescription": "Comprehensive system prompt that defines the assistant's role, tasks, and behavioral guidelines when interacting with customers.",
      "codeLanguage": "plaintext",
      "codeTokens": 206,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/conversational-ai-guide-support-agent.mdx#2025-04-17_snippet_1",
      "pageTitle": "Building Conversational Support Assistant Guide",
      "codeList": [
        {
          "language": "plaintext",
          "code": "You are a friendly and efficient virtual assistant for [Your Company Name]. Your role is to assist customers by answering questions about the company's products, services, and documentation. You should use the provided knowledge base to offer accurate and helpful responses.\n\nTasks:\n- Answer Questions: Provide clear and concise answers based on the available information.\n- Clarify Unclear Requests: Politely ask for more details if the customer's question is not clear.\n\nGuidelines:\n- Maintain a friendly and professional tone throughout the conversation.\n- Be patient and attentive to the customer's needs.\n- If unsure about any information, politely ask the customer to repeat or clarify.\n- Avoid discussing topics unrelated to the company's products or services.\n- Aim to provide concise answers. Limit responses to a couple of sentences and let the user guide you on where to provide more detail."
        }
      ],
      "relevance": 0.865
    },
    {
      "codeTitle": "Setting Environment Variables for ElevenLabs API",
      "codeDescription": "Environment variable configuration for storing the ElevenLabs API key and agent ID, used for authentication in the voice chat application.",
      "codeLanguage": "env",
      "codeTokens": 72,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/vite.mdx#2025-04-17_snippet_7",
      "pageTitle": "Creating a Conversational AI Web Client with Vite and ElevenLabs",
      "codeList": [
        {
          "language": "env",
          "code": "ELEVENLABS_API_KEY=your-api-key-here\nAGENT_ID=your-agent-id-here"
        }
      ],
      "relevance": 0.865
    },
    {
      "codeTitle": "Embedding YouTube Video in Markdown",
      "codeDescription": "This code snippet demonstrates how to embed a YouTube video player within a Markdown document using an iframe. It sets the video dimensions, source URL, and various attributes for playback control.",
      "codeLanguage": "markdown",
      "codeTokens": 124,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/playground/voice-changer.mdx#2025-04-17_snippet_0",
      "pageTitle": "Voice Changer Product Guide",
      "codeList": [
        {
          "language": "markdown",
          "code": "<iframe\n  width=\"100%\"\n  height=\"400\"\n  src=\"https://www.youtube.com/embed/GBdOQClluIA\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  allowfullscreen\n></iframe>"
        }
      ],
      "relevance": 0.865
    },
    {
      "codeTitle": "API Endpoint Documentation - Voice and Dubbing Updates",
      "codeDescription": "Documentation for updates to Voice Library filtering and Dubbing parameter changes.",
      "codeLanguage": "markdown",
      "codeTokens": 121,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/changelog/2025-03-31.md#2025-04-17_snippet_3",
      "pageTitle": "ElevenLabs Documentation Updates",
      "codeList": [
        {
          "language": "markdown",
          "code": "- Updated Voice endpoints:\n  - [Get shared voices](/docs/api-reference/voice-library/get-shared) - Added locale parameter for filtering voices by language region\n\n- Updated Dubbing endpoint:\n  - [Dub a video or audio file](/docs/api-reference/dubbing/dub-a-video-or-an-audio-file) - Renamed beta feature `use_replacement_voices_from_library` parameter to `disable_voice_cloning` for clarity"
        }
      ],
      "relevance": 0.865
    },
    {
      "codeTitle": "Implementing End Call Tool using JavaScript SDK",
      "codeDescription": "Code showing how to implement the End Call tool with the ElevenLabs JavaScript SDK. The snippet initializes the client and creates an agent with the end call tool configured as a system tool.",
      "codeLanguage": "javascript",
      "codeTokens": 172,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/end-call.mdx#2025-04-17_snippet_1",
      "pageTitle": "Implementing the End Call Tool in ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "import { ElevenLabs } from 'elevenlabs';\n\n// Initialize the client\nconst client = new ElevenLabs({\n  apiKey: 'YOUR_API_KEY',\n});\n\n// Create the agent with end call tool\nawait client.conversationalAi.createAgent({\n  conversation_config: {\n    agent: {\n      prompt: {\n        tools: [\n          {\n            type: 'system',\n            name: 'end_call',\n            description: '', // Optional: Customize when the tool should be triggered\n          },\n        ],\n      },\n    },\n  },\n});"
        }
      ],
      "relevance": 0.865
    },
    {
      "codeTitle": "Visualizing Agent Transfer Hierarchy in Text Format",
      "codeDescription": "A text-based diagram showing a hierarchical structure of agent transfers, with an Orchestrator Agent at the top level that can transfer to specialized agents (Availability Inquiries, Technical Support, Billing Issues), including nested transfers.",
      "codeLanguage": "text",
      "codeTokens": 151,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/agent-transfer.mdx#2025-04-17_snippet_0",
      "pageTitle": "Configuring Agent-to-Agent Transfer in ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "text",
          "code": "Orchestrator Agent (Initial Qualification)\n│\n├───> Agent 1 (e.g., Availability Inquiries)\n│\n├───> Agent 2 (e.g., Technical Support)\n│     │\n│     └───> Agent 2a (e.g., Hardware Support)\n│\n└───> Agent 3 (e.g., Billing Issues)"
        }
      ],
      "relevance": 0.865
    },
    {
      "codeTitle": "Customizing ElevenLabs Widget Text in HTML",
      "codeDescription": "This HTML snippet demonstrates how to customize the text content of various widget elements, including call-to-action buttons and status messages for the ElevenLabs Conversational AI widget.",
      "codeLanguage": "html",
      "codeTokens": 161,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/widget.mdx#2025-04-17_snippet_3",
      "pageTitle": "Widget Customization for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "html",
          "code": "<elevenlabs-convai\n  action-text=\"Need assistance?\"         // Optional: CTA button text\n  start-call-text=\"Begin conversation\"   // Optional: Start call button\n  end-call-text=\"End call\"              // Optional: End call button\n  expand-text=\"Open chat\"               // Optional: Expand widget text\n  listening-text=\"Listening...\"         // Optional: Listening state\n  speaking-text=\"Assistant speaking\"     // Optional: Speaking state\n></elevenlabs-convai>"
        }
      ],
      "relevance": 0.86
    },
    {
      "codeTitle": "Configuring package.json for Vite Development Script",
      "codeDescription": "JSON snippet to add a development script for running Vite in the package.json file.",
      "codeLanguage": "json",
      "codeTokens": 64,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/vite.mdx#2025-04-17_snippet_2",
      "pageTitle": "Creating a Conversational AI Web Client with Vite and ElevenLabs",
      "codeList": [
        {
          "language": "json",
          "code": "{\n    \"scripts\": {\n        ...\n        \"dev:frontend\": \"vite\"\n    }\n}"
        }
      ],
      "relevance": 0.86
    },
    {
      "codeTitle": "Displaying Concurrency and Priority Limits for ElevenLabs Subscription Plans in Markdown",
      "codeDescription": "This markdown table shows the concurrency limits, Speech-to-Text (STT) concurrency limits, and priority levels for different ElevenLabs subscription plans. It provides crucial information for users to understand the processing capabilities and queue priority associated with their subscription tier.",
      "codeLanguage": "markdown",
      "codeTokens": 205,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/models.mdx#2025-04-17_snippet_2",
      "pageTitle": "ElevenLabs API Models Documentation",
      "codeList": [
        {
          "language": "markdown",
          "code": "| Plan       | Concurrency Limit | STT Concurrency Limit | Priority level |\n| ---------- | ----------------- | --------------------- | -------------- |\n| Free       | 4                 | 10                    | 3              |\n| Starter    | 6                 | 15                    | 4              |\n| Creator    | 10                | 25                    | 5              |\n| Pro        | 20                | 50                    | 5              |\n| Scale      | 30                | 75                    | 5              |\n| Business   | 30                | 75                    | 5              |\n| Enterprise | Elevated          | Elevated              | Highest        |"
        }
      ],
      "relevance": 0.86
    },
    {
      "codeTitle": "Initializing ElevenLabs Python Client",
      "codeDescription": "Example of initializing the ElevenLabs client in Python using the API key.",
      "codeLanguage": "python",
      "codeTokens": 57,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/api-reference/pages/authentication.mdx#2025-04-17_snippet_2",
      "pageTitle": "ElevenLabs API Authentication Documentation",
      "codeList": [
        {
          "language": "python",
          "code": "from elevenlabs.client import ElevenLabs\n\nclient = ElevenLabs(\n  api_key='YOUR_API_KEY',\n)"
        }
      ],
      "relevance": 0.86
    },
    {
      "codeTitle": "Rendering Card Group Navigation Component in JSX",
      "codeDescription": "A JSX component that displays a grid of navigation cards linking to different ElevenLabs API features. Uses a CardGroup component with a 3-column layout containing individual Card components for each feature.",
      "codeLanguage": "jsx",
      "codeTokens": 449,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/quickstart-developer-guides.mdx#2025-04-17_snippet_0",
      "pageTitle": "ElevenLabs Developer Documentation Guide",
      "codeList": [
        {
          "language": "jsx",
          "code": "<CardGroup cols={3}>\n  <Card\n    title=\"Speech to Text\"\n    icon=\"duotone pen-clip\"\n    href=\"/docs/cookbooks/speech-to-text/quickstart\"\n  >\n    Convert spoken audio into text\n  </Card>\n  <Card\n    title=\"Conversational AI\"\n    icon=\"duotone comments\"\n    href=\"/docs/cookbooks/conversational-ai/quickstart\"\n  >\n    Deploy conversational voice agents\n  </Card>\n  <Card title=\"Voice cloning\" icon=\"duotone clone\" href=\"/docs/cookbooks/voices/clone-voice\">\n    Clone a voice\n  </Card>\n  <Card title=\"Sound effects\" icon=\"duotone explosion\" href=\"/docs/cookbooks/sound-effects\">\n    Generate sound effects from text\n  </Card>\n  <Card title=\"Voice Changer\" icon=\"duotone message-pen\" href=\"/docs/cookbooks/voice-changer\">\n    Transform the voice of an audio file\n  </Card>\n  <Card title=\"Voice Isolator\" icon=\"duotone ear\" href=\"/docs/cookbooks/voice-isolator\">\n    Isolate background noise from audio\n  </Card>\n  <Card title=\"Voice Design\" icon=\"duotone paint-brush\" href=\"/docs/cookbooks/voices/voice-design\">\n    Generate voices from a single text prompt\n  </Card>\n  <Card title=\"Dubbing\" icon=\"duotone language\" href=\"/docs/cookbooks/dubbing\">\n    Dub audio/video from one language to another\n  </Card>\n  <Card\n    title=\"Forced Alignment\"\n    icon=\"duotone objects-align-left\"\n    href=\"/docs/cookbooks/forced-alignment\"\n  >\n    Generate time-aligned transcripts for audio\n  </Card>\n</CardGroup>"
        }
      ],
      "relevance": 0.86
    },
    {
      "codeTitle": "API Documentation Updates - Markdown",
      "codeDescription": "Comprehensive documentation of API changes including new endpoints, updated endpoints across various categories like Conversational AI, Text to Speech, Speech to Text, Voice Management, Studio, and Pronunciation Dictionary features.",
      "codeLanguage": "markdown",
      "codeTokens": 359,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/changelog/2025-03-24.md#2025-04-17_snippet_0",
      "pageTitle": "ElevenLabs API Documentation Updates",
      "codeList": [
        {
          "language": "markdown",
          "code": "### Voices\n\n- **List Voices V2**: Added a new [V2 voice search endpoint](/docs/api-reference/voices/search) with better search and additional filtering options\n\n### Conversational AI\n\n- **Native outbound calling**: Added native outbound calling for Twilio-configured numbers, eliminating the need for complex setup configurations. Outbound calls are now visible in the Call History page.\n- **Automatic language detection**: Added new system tool for automatic language detection that enables agents to switch languages based on both explicit user requests (\"Let's talk in Spanish\") and implicit language in user audio.\n- **Pronunciation dictionary improvements**: Fixed phoneme tags in pronunciation dictionaries to work correctly with conversational AI.\n- **Large RAG document viewing**: Added ability to view the entire content of large RAG documents in the knowledge base.\n- **Customizable widget controls**: Updated UI to include an optional mute microphone button and made widget icons customizable via slots.\n\n### Sound Effects\n\n- **Fractional duration support**: Fixed an issue where users couldn't enter fractional values (like 0.5 seconds) for sound effect generation duration.\n\n### Speech to Text\n\n- **Repetition handling**: Improved detection and handling of repetitions in speech-to-text processing.\n\n### Studio\n\n- **Reader publishing fixes**: Added support for mp3_44100_192 output format (high quality) so users below Publisher tier can export audio to Reader.\n\n### Mobile\n\n- **Core app signup**: Added signup endpoints for the new Core mobile app."
        },
        {
          "language": "markdown",
          "code": "## New Endpoints\n\n- Added 5 new endpoints:\n  - [List voices (v2)](/docs/api-reference/voices/search) - Enhanced voice search capabilities with additional filtering options\n  - [Initiate outbound call](/docs/api-reference/phone-numbers/twilio-outbound-call) - New endpoint for making outbound calls via Twilio integration\n  - [Add pronunciation dictionary from rules](/docs/api-reference/pronunciation-dictionary/add-rules) - Create pronunciation dictionaries directly from rules without file upload\n  - [Get knowledge base document content](/docs/api-reference/knowledge-base/get-knowledge-base-document-content) - Retrieve full document content from the knowledge base\n  - [Get knowledge base document chunk](/docs/api-reference/knowledge-base/get-knowledge-base-document-part-by-id) - Retrieve specific chunks from knowledge base documents"
        }
      ],
      "relevance": 0.86
    },
    {
      "codeTitle": "Providing Tool Usage Instructions in System Prompt (Plaintext)",
      "codeDescription": "Example of how to provide clear instructions in the system prompt to improve the assistant's tool calling accuracy. This snippet shows how to guide the assistant on when to use specific tools.",
      "codeLanguage": "plaintext",
      "codeTokens": 92,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/conversational-ai-tool-best-practices.mdx#2025-04-17_snippet_0",
      "pageTitle": "Best Practices for Implementing Tools with AI Assistants",
      "codeList": [
        {
          "language": "plaintext",
          "code": "Use `check_order_status` when the user inquires about the status of their order, such as 'Where is my order?' or 'Has my order shipped yet?'."
        }
      ],
      "relevance": 0.86
    },
    {
      "codeTitle": "Displaying Character Limits for ElevenLabs Models in Markdown",
      "codeDescription": "This code snippet shows a markdown table that lists the character limits and approximate audio duration for different ElevenLabs model IDs. It provides important information for users to understand the constraints of each model when making text-to-speech requests.",
      "codeLanguage": "markdown",
      "codeTokens": 220,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/models.mdx#2025-04-17_snippet_1",
      "pageTitle": "ElevenLabs API Models Documentation",
      "codeList": [
        {
          "language": "markdown",
          "code": "| Model ID                 | Character limit | Approximate audio duration |\n| ------------------------ | --------------- | -------------------------- |\n| `eleven_flash_v2_5`      | 40,000          | ~40 minutes                |\n| `eleven_flash_v2`        | 30,000          | ~30 minutes                |\n| `eleven_multilingual_v2` | 10,000          | ~10 minutes                |\n| `eleven_multilingual_v1` | 10,000          | ~10 minutes                |\n| `eleven_english_sts_v2`  | 10,000          | ~10 minutes                |\n| `eleven_english_sts_v1`  | 10,000          | ~10 minutes                |"
        }
      ],
      "relevance": 0.86
    },
    {
      "codeTitle": "Embedding Audio Native Player in Wix HTML",
      "codeDescription": "This HTML snippet embeds the Audio Native player into a Wix blog post. It includes a div element with specific attributes for the player and a script tag to load the necessary JavaScript file.",
      "codeLanguage": "html",
      "codeTokens": 195,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/audio-tools/audio-native/wix.mdx#2025-04-17_snippet_0",
      "pageTitle": "Integrating Audio Native with Wix",
      "codeList": [
        {
          "language": "html",
          "code": "<div\n    id=\"elevenlabs-audionative-widget\"\n    data-height=\"90\"\n    data-width=\"100%\"\n    data-frameborder=\"no\"\n    data-scrolling=\"no\"\n    data-publicuserid=\"public-user-id\"\n    data-playerurl=\"https://elevenlabs.io/player/index.html\"\n    data-projectid=\"project-id\"\n>\n    Loading the <a href=\"https://elevenlabs.io/text-to-speech\" target=\"_blank\" rel=\"noopener\">Elevenlabs Text to Speech</a> AudioNative Player...\n</div>\n<script src=\"https://elevenlabs.io/player/audioNativeHelper.js\" type=\"text/javascript\"></script>"
        }
      ],
      "relevance": 0.855
    },
    {
      "codeTitle": "Creating PLS Pronunciation Dictionary File",
      "codeDescription": "XML structure for defining pronunciation rules using IPA alphabet for the word 'tomato'.",
      "codeLanguage": "xml",
      "codeTokens": 248,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/pronunciation-dictionaries.mdx#2025-04-17_snippet_2",
      "pageTitle": "Using Pronunciation Dictionaries with ElevenLabs Python SDK",
      "codeList": [
        {
          "language": "xml",
          "code": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<lexicon version=\"1.0\"\n      xmlns=\"http://www.w3.org/2005/01/pronunciation-lexicon\"\n      xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n      xsi:schemaLocation=\"http://www.w3.org/2005/01/pronunciation-lexicon\n        http://www.w3.org/TR/2007/CR-pronunciation-lexicon-20071212/pls.xsd\"\n      alphabet=\"ipa\" xml:lang=\"en-US\">\n  <lexeme>\n    <grapheme>tomato</grapheme>\n    <phoneme>/tə'meɪtoʊ/</phoneme>\n  </lexeme>\n  <lexeme>\n    <grapheme>Tomato</grapheme>\n    <phoneme>/tə'meɪtoʊ/</phoneme>\n  </lexeme>\n</lexicon>"
        }
      ],
      "relevance": 0.855
    },
    {
      "codeTitle": "Defining Project File Structure",
      "codeDescription": "Shell-like representation of the basic file structure for the ElevenLabs Conversational AI project.",
      "codeLanguage": "shell",
      "codeTokens": 82,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/vite.mdx#2025-04-17_snippet_3",
      "pageTitle": "Creating a Conversational AI Web Client with Vite and ElevenLabs",
      "codeList": [
        {
          "language": "shell",
          "code": "elevenlabs-conversational-ai/\n├── index.html\n├── script.js\n├── package-lock.json\n├── package.json\n└── node_modules"
        }
      ],
      "relevance": 0.855
    },
    {
      "codeTitle": "Uploading Speaker and Line Script in CSV Format",
      "codeDescription": "This snippet demonstrates the format for uploading a script with speaker names and lines to the Voiceover Studio. It shows how to structure the CSV file with speaker and line columns.",
      "codeLanguage": "csv",
      "codeTokens": 78,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/audio-tools/voiceover-studio.mdx#2025-04-17_snippet_0",
      "pageTitle": "Creating Long-Form Content with ElevenLabs Voiceover Studio",
      "codeList": [
        {
          "language": "csv",
          "code": "speaker,line\nJoe,\"Hey!\"\nMaria,\"Oh, hi Joe! It's been a while.\""
        }
      ],
      "relevance": 0.855
    },
    {
      "codeTitle": "Ending Conversation Session in ElevenLabs JavaScript SDK",
      "codeDescription": "Method to manually end the conversation session, disconnect from the websocket, and discard the conversation instance.",
      "codeLanguage": "javascript",
      "codeTokens": 45,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/javascript.mdx#2025-04-17_snippet_6",
      "pageTitle": "JavaScript SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "await conversation.endSession();"
        }
      ],
      "relevance": 0.855
    },
    {
      "codeTitle": "Displaying Dashboard Error Messages Table in Markdown",
      "codeDescription": "This code snippet shows a markdown table listing common dashboard error messages, their causes, and solutions. It includes errors related to model selection and general client-side issues.",
      "codeLanguage": "markdown",
      "codeTokens": 188,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/error-messages.mdx#2025-04-17_snippet_0",
      "pageTitle": "ElevenLabs Error Messages Documentation",
      "codeList": [
        {
          "language": "markdown",
          "code": "| Error Message                                          | Cause                                                                                                     | Solution                                                                                                                                                        |\n| ------------------------------------------------------ | --------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| The selected model can not be used for text-to-speech. | Occurs when switching between speech-to-speech and text-to-speech if the model does not switch correctly. | Select the desired model. If unresolved, select a different model, then switch back.                                                                            |\n| Oops, something went wrong.                            | Indicates a client-side error, often due to device or browser issues.                                     | Click \"Try again\" or refresh the page. If unresolved, clear browser cache and cookies. Temporarily pause browser-based translation tools like Google Translate. |"
        }
      ],
      "relevance": 0.855
    },
    {
      "codeTitle": "Displaying API Changes in Markdown",
      "codeDescription": "This snippet shows how to use an Accordion component in Markdown to display API changes. It includes sections for new endpoints, updated endpoints across various services, and details about specific changes to API parameters and responses.",
      "codeLanguage": "markdown",
      "codeTokens": 1012,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/changelog/2025-03-17.md#2025-04-17_snippet_0",
      "pageTitle": "ElevenLabs Documentation Updates",
      "codeList": [
        {
          "language": "markdown",
          "code": "<Accordion title=\"View API changes\">\n\n## New Endpoints\n\n- Added 3 new endpoints:\n  - [Get workspace resource](/docs/api-reference/workspace/get-resource)\n  - [Share workspace resource](/docs/api-reference/workspace/share-workspace-resource)\n  - [Unshare workspace resource](/docs/api-reference/workspace/unshare-workspace-resource)\n\n## Updated Endpoints\n\n### Dubbing\n\n- Updated Dubbing endpoints:\n  - [Dub a video or audio file](/docs/api-reference/dubbing/dub-a-video-or-an-audio-file) - Added `use_replacement_voices_from_library` property and made `source_path`, `target_language`, `source_language` nullable\n  - [Resource dubbing](/docs/api-reference/dubbing/dub-segments) - Made `language_codes` array nullable\n  - [Add language to dubbing resource](/docs/api-reference/dubbing/add-language-to-resource) - Made `language_code` nullable\n  - [Add speaker segment](/docs/api-reference/dubbing/create-segment-for-speaker) - Made `text` nullable\n  - [Translate dubbing resource](/docs/api-reference/dubbing/translate-segments) - Made `target_languages` array nullable\n  - [Update dubbing segment](/docs/api-reference/dubbing/update-segment-language) - Made `start_time` and `end_time` nullable\n\n### Project Management\n\n- Updated Project endpoints:\n  - [Add project](/docs/api-reference/studio/add-project) - Made `metadata`, `project_name`, `description` nullable\n  - [Create podcast](/docs/api-reference/studio/create-podcast) - Made `title`, `description`, `author` nullable\n  - [Get project](/docs/api-reference/studio/get-project) - Made `last_modified_at`, `created_at`, `project_name` nullable\n  - [Add chapter](/docs/api-reference/studio/add-chapter) - Made `chapter_id`, `word_count`, `statistics` nullable\n  - [Update chapter](/docs/api-reference/studio/update-chapter) - Made `content` and `blocks` properties nullable\n\n### Conversational AI\n\n- Updated Conversational AI endpoints:\n  - [Update agent](/docs/api-reference/agents/update-agent) - Made `conversation_config`, `platform_settings` nullable and added `workspace_overrides` property\n  - [Create agent](/docs/api-reference/agents/create-agent) - Made `agent_name`, `prompt`, `widget_config` nullable and added `workspace_overrides` property\n  - [Add to knowledge base](/docs/api-reference/knowledge-base/add-to-knowledge-base) - Made `document_name` nullable\n  - [Get conversation](/docs/api-reference/conversations/get-conversation) - Added `twilio_call_data` model and made `transcript`, `metadata` nullable\n\n### Text to Speech\n\n- Updated Text to Speech endpoints:\n  - [Convert text to speech](/docs/api-reference/text-to-speech/convert) - Made `voice_settings`, `text_input` nullable and deprecated `use_pvc_as_ivc` property\n  - [Stream text to speech](/docs/api-reference/text-to-speech/convert-as-stream) - Made `voice_settings`, `text_input` nullable and deprecated `use_pvc_as_ivc` property\n  - [Convert with timestamps](/docs/api-reference/text-to-speech/convert-with-timestamps) - Made `character_alignment` and `word_alignment` nullable\n\n### Voice Management\n\n- Updated Voice endpoints:\n  - [Create voice previews](/docs/api-reference/text-to-voice/create-previews) - Added `loudness`, `quality`, `guidance_scale` properties\n  - [Create voice from preview](/docs/api-reference/text-to-voice/create-voice-from-preview) - Added `speaker_separation` properties and made `voice_id`, `name`, `labels` nullable\n  - [Get voice](/docs/api-reference/voices/get) - Added `speaker_boost`, `speaker_clarity`, `speaker_isolation` properties\n\n### Speech to Text\n\n- Updated Speech to Text endpoint:\n  - [Convert speech to text](/docs/api-reference/speech-to-text/convert) - Added `biased_keywords` property\n\n### Other Updates\n\n- [Download history](/docs/api-reference/history/download) - Added application/zip content type and 400 response\n- [Add pronunciation dictionary from file](/docs/api-reference/pronunciation-dictionary/add-from-file) - Made `dictionary_name` and `description` nullable\n\n</Accordion>"
        }
      ],
      "relevance": 0.855
    },
    {
      "codeTitle": "Controlling Speech Pace Through Narrative Style",
      "codeDescription": "This example shows how to control the pace of speech by using narrative writing style with hesitations, ellipses, and descriptive phrases to naturally slow down delivery.",
      "codeLanguage": "text",
      "codeTokens": 68,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/controls.mdx#2025-04-17_snippet_11",
      "pageTitle": "Text-to-Speech Controls Guide",
      "codeList": [
        {
          "language": "text",
          "code": "\"I… I thought you'd understand,\" he said, his voice slowing with disappointment."
        }
      ],
      "relevance": 0.855
    },
    {
      "codeTitle": "Using IPA Phoneme Tags for Pronunciation Control",
      "codeDescription": "This example demonstrates using International Phonetic Alphabet (IPA) phoneme tags to specify exact pronunciation. Compatible with selected ElevenLabs models.",
      "codeLanguage": "xml",
      "codeTokens": 70,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/controls.mdx#2025-04-17_snippet_3",
      "pageTitle": "Text-to-Speech Controls Guide",
      "codeList": [
        {
          "language": "xml",
          "code": "<phoneme alphabet=\"ipa\" ph=\"ˈæktʃuəli\">\n  actually\n</phoneme>"
        }
      ],
      "relevance": 0.855
    },
    {
      "codeTitle": "ElevenLabs Agent System Prompt Configuration",
      "codeDescription": "System prompt configuration for the ElevenLabs agent, defining its capabilities and available tools.",
      "codeLanguage": "txt",
      "codeTokens": 92,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/expo-react-native.mdx#2025-04-17_snippet_7",
      "pageTitle": "Building Cross-platform Voice Agents with Expo React Native and ElevenLabs",
      "codeList": [
        {
          "language": "txt",
          "code": "You are a helpful assistant running on {{platform}}. You have access to certain tools that allow you to check the user device battery level and change the display brightness. Use these tools if the user asks about them. Otherwise, just answer the question."
        }
      ],
      "relevance": 0.85
    },
    {
      "codeTitle": "Setting Up an Allowlist in JavaScript",
      "codeDescription": "Function to create an agent with an allowlist configuration using the JavaScript SDK. The code specifies three allowed domains (example.com, app.example.com, and localhost:3000).",
      "codeLanguage": "javascript",
      "codeTokens": 192,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/authentication.mdx#2025-04-17_snippet_7",
      "pageTitle": "Authentication for ElevenLabs Conversational AI Agents",
      "codeList": [
        {
          "language": "javascript",
          "code": "async function createAuthenticatedAgent(client) {\n  try {\n    const agent = await client.conversationalAi.createAgent({\n      conversation_config: {\n        agent: {\n          first_message: \"Hi. I'm an authenticated agent.\",\n        },\n      },\n      platform_settings: {\n        auth: {\n          enable_auth: false,\n          allowlist: [\n            { hostname: 'example.com' },\n            { hostname: 'app.example.com' },\n            { hostname: 'localhost:3000' },\n          ],\n        },\n      },\n    });\n\n    return agent;\n  } catch (error) {\n    console.error('Error creating agent:', error);\n    throw error;\n  }\n}"
        }
      ],
      "relevance": 0.85
    },
    {
      "codeTitle": "Installing AWS SDK Dependencies",
      "codeDescription": "Commands to install required AWS SDK packages for Python (boto3) and TypeScript (@aws-sdk/client-s3, @aws-sdk/s3-request-presigner)",
      "codeLanguage": "bash",
      "codeTokens": 57,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/streaming.mdx#2025-04-17_snippet_4",
      "pageTitle": "ElevenLabs Text-to-Speech Integration Guide",
      "codeList": [
        {
          "language": "bash",
          "code": "pip install boto3"
        },
        {
          "language": "bash",
          "code": "npm install @aws-sdk/client-s3\nnpm install @aws-sdk/s3-request-presigner"
        }
      ],
      "relevance": 0.85
    },
    {
      "codeTitle": "Studio Documentation in Markdown",
      "codeDescription": "Structured documentation for ElevenLabs Studio including overview, guide, and starting options. Contains details about creating projects from scratch, audiobooks, articles, and podcasts along with quality settings and voice configuration options.",
      "codeLanguage": "markdown",
      "codeTokens": 70,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/products/studio.mdx#2025-04-17_snippet_0",
      "pageTitle": "ElevenLabs Studio Documentation",
      "codeList": [
        {
          "language": "markdown",
          "code": "---\ntitle: Studio\nheadline: Studio overview\n---"
        }
      ],
      "relevance": 0.85
    },
    {
      "codeTitle": "Running WebSocket Streaming Script in Python",
      "codeDescription": "Execute the Python script for WebSocket streaming with ElevenLabs API.",
      "codeLanguage": "python",
      "codeTokens": 45,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/websockets.mdx#2025-04-17_snippet_9",
      "pageTitle": "WebSocket Streaming for Real-Time Audio Generation with ElevenLabs API",
      "codeList": [
        {
          "language": "python",
          "code": "python text-to-speech-websocket.py"
        }
      ],
      "relevance": 0.85
    },
    {
      "codeTitle": "API Endpoint Documentation - Conversational AI Updates",
      "codeDescription": "Documentation for changes to Conversational AI endpoints including response code updates and embedding model changes.",
      "codeLanguage": "markdown",
      "codeTokens": 116,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/changelog/2025-03-31.md#2025-04-17_snippet_2",
      "pageTitle": "ElevenLabs Documentation Updates",
      "codeList": [
        {
          "language": "markdown",
          "code": "- Updated Conversational AI endpoints:\n  - [Delete agent](/docs/api-reference/agents/delete-agent) - Changed success response code from 200 to 204\n  - [Updated RAG embedding model options](docs/api-reference/knowledge-base/rag-index-status#request.body.model) - replaced `gte_Qwen2_15B_instruct` with `multilingual_e5_large_instruct`"
        }
      ],
      "relevance": 0.85
    },
    {
      "codeTitle": "API Endpoint Updates - Markdown Documentation",
      "codeDescription": "Detailed documentation of new and updated API endpoints including speech-to-text, agent properties, platform settings, widget configurations, secrets management, voice settings, and knowledge base parameters.",
      "codeLanguage": "markdown",
      "codeTokens": 457,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/changelog/2025-02-25.md#2025-04-17_snippet_0",
      "pageTitle": "ElevenLabs Documentation Updates",
      "codeList": [
        {
          "language": "markdown",
          "code": "- Launched **/v1/speech-to-text** [endpoint](/docs/api-reference/speech-to-text/convert)\n- Added `agents.level` property to [Conversational AI agents endpoint](/docs/api-reference/agents/get-agents#response.body.agents.access_level)\n- Added `platform_settings` to [Conversational AI agent endpoint](/docs/api-reference/agents/update-agent#request.body.platform_settings)\n- Added `expandable` variant to `widget_config`, with configuration options `show_avatar_when_collapsed` and `disable_banner` to [Conversational AI agent widget endpoint](/docs/api-reference/agents/get-agent#response.body.widget)\n- Added `webhooks` property and `used_by` to `secrets` to [secrets endpoint](/docs/api-reference/workspace/get-secrets#response.body.secrets.used_by)\n- Added `verified_languages` to [voices endpoint](/docs/api-reference/voices/get#response.body.verified_languages)\n- Added `speed` property to [voice settings endpoints](/docs/api-reference/voices/get#response.body.settings.speed)\n- Added `verified_languages`, `is_added_by_user` to `voices` and `min_notice_period_days` query parameter to [shared voices endpoint](/docs/api-reference/voice-library/get-shared#request.query)\n- Added `verified_languages`, `is_added_by_user` to `voices` in [similar voices endpoint](/docs/api-reference/voices/get-similar-library-voices)\n- Added `search`, `show_only_owned_documents`, `use_typesense` query parameters to [knowledge base endpoint](/docs/api-reference/knowledge-base/get-knowledge-base-list#request.query.search)\n- Added `used_by` to Conversation AI [secrets endpoint](/docs/api-reference/workspace/get-secrets)\n- Added `invalidate_affected_text` property to Studio [pronunciation dictionaries endpoint](/docs/api-reference/studio/create-pronunciation-dictionaries#request.body.invalidate_affected_text)"
        }
      ],
      "relevance": 0.85
    },
    {
      "codeTitle": "Using Lexeme Alias Tags for Name Pronunciation",
      "codeDescription": "This example shows how to use lexeme and alias tags to specify pronunciation for names with unusual spellings. Useful for models that don't support phoneme tags.",
      "codeLanguage": "xml",
      "codeTokens": 81,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/controls.mdx#2025-04-17_snippet_6",
      "pageTitle": "Text-to-Speech Controls Guide",
      "codeList": [
        {
          "language": "xml",
          "code": "  <lexeme>\n    <grapheme>Claughton</grapheme>\n    <alias>Cloffton</alias>\n  </lexeme>"
        }
      ],
      "relevance": 0.85
    },
    {
      "codeTitle": "Adding Pauses with Break Tags in Text-to-Speech",
      "codeDescription": "Use the break tag to add natural pauses up to 3 seconds in text-to-speech generation. This example demonstrates how to create a thoughtful pause between spoken phrases.",
      "codeLanguage": "text",
      "codeTokens": 77,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/controls.mdx#2025-04-17_snippet_0",
      "pageTitle": "Text-to-Speech Controls Guide",
      "codeList": [
        {
          "language": "text",
          "code": "\"Hold on, let me think.\" <break time=\"1.5s\" /> \"Alright, I've got it.\""
        }
      ],
      "relevance": 0.85
    },
    {
      "codeTitle": "Global Script Integration for ElevenLabs Conversational AI Widget",
      "codeDescription": "Script tag to be added globally to WordPress through either a plugin or theme editing. This loads the widget functionality on all pages where the agent element is present.",
      "codeLanguage": "html",
      "codeTokens": 74,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/wordpress.mdx#2025-04-17_snippet_2",
      "pageTitle": "Conversational AI Integration in WordPress",
      "codeList": [
        {
          "language": "html",
          "code": "<script src=\"https://elevenlabs.io/convai-widget/index.js\" async type=\"text/javascript\"></script>"
        }
      ],
      "relevance": 0.845
    },
    {
      "codeTitle": "Formatting CSV for Manual Dubbing in Hours:Minutes:Seconds:Frame",
      "codeDescription": "Example CSV file for manual dubbing using hours:minutes:seconds:frame as the timecode format. It includes columns for speaker, start time, end time, transcription, and translation.",
      "codeLanguage": "csv",
      "codeTokens": 128,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/products/dubbing/dubbing-studio.mdx#2025-04-17_snippet_1",
      "pageTitle": "Dubbing Studio Documentation",
      "codeList": [
        {
          "language": "csv",
          "code": "speaker,start_time,end_time,transcription,translation\nAdam,\"0:00:01:01\",\"0:00:05:01\",\"Hello, how are you?\",\"Hola, ¿cómo estás?\"\nAdam,\"0:00:06:01\",\"0:00:10:01\",\"I'm fine, thank you.\",\"Estoy bien, gracias.\""
        }
      ],
      "relevance": 0.845
    },
    {
      "codeTitle": "Starting the Node.js Server for Outbound Calls",
      "codeDescription": "This command starts the Node.js server for handling outbound calls. Upon successful startup, it will display a listening message in the terminal.",
      "codeLanguage": "bash",
      "codeTokens": 54,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-outbound-calling.mdx#2025-04-17_snippet_5",
      "pageTitle": "Building Outbound Calling AI Agents with Twilio and ElevenLabs",
      "codeList": [
        {
          "language": "bash",
          "code": "node outbound.js"
        }
      ],
      "relevance": 0.845
    },
    {
      "codeTitle": "LLM Prompt Template for Text-to-Speech Normalization",
      "codeDescription": "A template prompt for instructing LLMs to convert text into a TTS-friendly format. It includes example conversions for various types of content including currencies, numbers, dates, times, and abbreviations.",
      "codeLanguage": "text",
      "codeTokens": 415,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/normalization.mdx#2025-04-17_snippet_0",
      "pageTitle": "Text to Speech Normalization Guide",
      "codeList": [
        {
          "language": "text",
          "code": "Convert the output text into a format suitable for text-to-speech. Ensure that numbers, symbols, and abbreviations are expanded for clarity when read aloud. Expand all abbreviations to their full spoken forms.\n\nExample input and output:\n\n\"$42.50\" → \"forty-two dollars and fifty cents\"\n\"£1,001.32\" → \"one thousand and one pounds and thirty-two pence\"\n\"1234\" → \"one thousand two hundred thirty-four\"\n\"3.14\" → \"three point one four\"\n\"555-555-5555\" → \"five five five, five five five, five five five five\"\n\"2nd\" → \"second\"\n\"XIV\" → \"fourteen\" - unless it's a title, then it's \"the fourteenth\"\n\"3.5\" → \"three point five\"\n\"⅔\" → \"two-thirds\"\n\"Dr.\" → \"Doctor\"\n\"Ave.\" → \"Avenue\"\n\"St.\" → \"Street\" (but saints like \"St. Patrick\" should remain)\n\"Ctrl + Z\" → \"control z\"\n\"100km\" → \"one hundred kilometers\"\n\"100%\" → \"one hundred percent\"\n\"elevenlabs.io/docs\" → \"eleven labs dot io slash docs\"\n\"2024-01-01\" → \"January first, two-thousand twenty-four\"\n\"123 Main St, Anytown, USA\" → \"one two three Main Street, Anytown, United States of America\"\n\"14:30\" → \"two thirty PM\"\n\"01/02/2023\" → \"January second, two-thousand twenty-three\" or \"the first of February, two-thousand twenty-three\", depending on locale of the user"
        }
      ],
      "relevance": 0.845
    },
    {
      "codeTitle": "Installing Python Dependencies for Twilio Integration",
      "codeDescription": "Installs the required Python packages for the project including FastAPI for server functionality, Uvicorn for ASGI server, and libraries for WebSocket communication and Twilio integration.",
      "codeLanguage": "bash",
      "codeTokens": 72,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-custom-server.mdx#2025-04-17_snippet_6",
      "pageTitle": "Twilio Integration Guide for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "bash",
          "code": "pip install fastapi uvicorn python-dotenv twilio elevenlabs websockets"
        }
      ],
      "relevance": 0.845
    },
    {
      "codeTitle": "Handling Agent Response Correction Events in JavaScript",
      "codeDescription": "Examples showing the agent response correction event structure and a handler implementation. These events contain truncated responses after user interruption to maintain conversation accuracy.",
      "codeLanguage": "javascript",
      "codeTokens": 113,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-events.mdx#2025-04-17_snippet_5",
      "pageTitle": "Client Events Documentation for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "// Example response correction event structure\n{\n  \"type\": \"agent_response_correction\",\n  \"agent_response_correction_event\": {\n    \"original_agent_response\": \"Let me tell you about the complete history...\",\n    \"corrected_agent_response\": \"Let me tell you about...\"  // Truncated after interruption\n  }\n}"
        },
        {
          "language": "javascript",
          "code": "// Example response correction handler\nwebsocket.on('agent_response_correction', (event) => {\n  const { agent_response_correction_event } = event;\n  const { corrected_agent_response } = agent_response_correction_event;\n  displayAgentMessage(corrected_agent_response);\n});"
        }
      ],
      "relevance": 0.845
    },
    {
      "codeTitle": "Creating FAQ Accordion Structure with Markdown and Custom Components",
      "codeDescription": "Implementation of an FAQ section using custom Accordion components with nested content addressing voice agent implementation questions.",
      "codeLanguage": "markdown",
      "codeTokens": 404,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/prompting-guide.mdx#2025-04-17_snippet_2",
      "pageTitle": "Financial Advisory Process Documentation",
      "codeList": [
        {
          "language": "markdown",
          "code": "<AccordionGroup>\n<Accordion title=\"Why are guardrails so important for voice agents?\">\n  Voice interactions tend to be more free-form and unpredictable than text. Guardrails prevent\n  inappropriate responses to unexpected inputs and maintain brand safety. They're essential for\n  voice agents that represent organizations or provide sensitive advice.\n</Accordion>\n\n<Accordion title=\"Can I update the prompt after deployment?\">\n  Yes. The system prompt can be modified at any time to adjust behavior. This is particularly useful\n  for addressing emerging issues or refining the agent's capabilities as you learn from user\n  interactions.\n</Accordion>\n\n<Accordion title=\"How do I handle users with different speaking styles or accents?\">\n  Design your prompt with simple, clear language patterns and instruct the agent to ask for\n  clarification when unsure. Avoid idioms and region-specific expressions that might confuse STT\n  systems processing diverse accents.\n</Accordion>\n\n<Accordion title=\"How can I make the AI sound more conversational?\">\n  Include speech markers (brief affirmations, filler words) in your system prompt. Specify that the\n  AI can use interjections like \"Hmm,\" incorporate thoughtful pauses, and employ natural speech\n  patterns.\n</Accordion>\n\n<Accordion title=\"Does a longer system prompt guarantee better results?\">\n  No. Focus on quality over quantity. Provide clear, specific instructions on essential behaviors\n  rather than exhaustive details. Test different prompt lengths to find the optimal balance for your\n  specific use case.\n</Accordion>\n\n<Accordion title=\"How do I balance consistency with adaptability?\">\n  Define core personality traits and guardrails firmly while allowing flexibility in tone and\n  verbosity based on the user's communication style. This creates a recognizable character that\n  can still respond naturally to different situations.\n</Accordion>\n</AccordionGroup>"
        }
      ],
      "relevance": 0.845
    },
    {
      "codeTitle": "Download Dubbed File Function",
      "codeDescription": "Function to download and save the completed dubbed file to a local directory.",
      "codeLanguage": "python",
      "codeTokens": 133,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/dubbing/basics.mdx#2025-04-17_snippet_4",
      "pageTitle": "ElevenLabs Dubbing API Documentation",
      "codeList": [
        {
          "language": "python",
          "code": "def download_dubbed_file(dubbing_id: str, language_code: str) -> str:\n    dir_path = f\"data/{dubbing_id}\"\n    os.makedirs(dir_path, exist_ok=True)\n    file_path = f\"{dir_path}/{language_code}.mp4\"\n    with open(file_path, \"wb\") as file:\n        for chunk in client.dubbing.get_dubbed_file(dubbing_id, language_code):\n            file.write(chunk)\n    return file_path"
        }
      ],
      "relevance": 0.845
    },
    {
      "codeTitle": "Initializing ElevenLabs Node.js Client",
      "codeDescription": "Example of initializing the ElevenLabs client in Node.js using the API key.",
      "codeLanguage": "javascript",
      "codeTokens": 65,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/api-reference/pages/authentication.mdx#2025-04-17_snippet_3",
      "pageTitle": "ElevenLabs API Authentication Documentation",
      "codeList": [
        {
          "language": "javascript",
          "code": "import { ElevenLabsClient } from 'elevenlabs';\n\nconst client = new ElevenLabsClient({\n  apiKey: 'YOUR_API_KEY',\n});"
        }
      ],
      "relevance": 0.845
    },
    {
      "codeTitle": "Handling Incoming Request in TypeScript",
      "codeDescription": "This snippet demonstrates how to handle an incoming request using Deno.serve, extract parameters, and generate a hash for caching purposes.",
      "codeLanguage": "typescript",
      "codeTokens": 183,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/supabase-streaming.mdx#2025-04-17_snippet_6",
      "pageTitle": "Streaming and Caching Speech with Supabase and ElevenLabs",
      "codeList": [
        {
          "language": "typescript",
          "code": "Deno.serve(async (req) => {\n// To secure your function for production, you can for example validate the request origin,\n// or append a user access token and validate it with Supabase Auth.\nconsole.log(\"Request origin\", req.headers.get(\"host\"));\nconst url = new URL(req.url);\nconst params = new URLSearchParams(url.search);\nconst text = params.get(\"text\");\nconst voiceId = params.get(\"voiceId\") ?? \"JBFqnCBsd6RMkjVDRZzb\";\n\nconst requestHash = hash.MD5({ text, voiceId });\nconsole.log(\"Request hash\", requestHash);\n\n// ...\n})"
        }
      ],
      "relevance": 0.84
    },
    {
      "codeTitle": "Initializing Node.js Project for Twilio Integration",
      "codeDescription": "Sets up a new Node.js project directory and initializes it with npm. This command creates a package.json file and sets the project type to module for ES6 import support.",
      "codeLanguage": "bash",
      "codeTokens": 85,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-custom-server.mdx#2025-04-17_snippet_0",
      "pageTitle": "Twilio Integration Guide for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "bash",
          "code": "mkdir conversational-ai-twilio\ncd conversational-ai-twilio\nnpm init -y; npm pkg set type=\"module\";"
        }
      ],
      "relevance": 0.84
    },
    {
      "codeTitle": "TypeScript Text Normalization for TTS",
      "codeDescription": "A TypeScript function that uses regular expressions and the number-to-words library to normalize text for TTS. It handles monetary values with different currency symbols and phone numbers, converting them to word representations for better TTS pronunciation.",
      "codeLanguage": "typescript",
      "codeTokens": 625,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/normalization.mdx#2025-04-17_snippet_2",
      "pageTitle": "Text to Speech Normalization Guide",
      "codeList": [
        {
          "language": "typescript",
          "code": "// Be sure to install the number-to-words library before running this code\nimport { toWords } from 'number-to-words';\n\nfunction normalizeText(text: string): string {\n  return (\n    text\n      // Convert monetary values (e.g., \"$1000\" → \"one thousand dollars\", \"£1000\" → \"one thousand pounds\")\n      .replace(/([\\$£€¥])(\\d+(?:,\\d{3})*(?:\\.\\d{2})?)/g, (_, currency, num) => {\n        // Remove commas before parsing\n        const numWithoutCommas = num.replace(/,/g, '');\n\n        const currencyMap: { [key: string]: string } = {\n          $: 'dollars',\n          '£': 'pounds',\n          '€': 'euros',\n          '¥': 'yen',\n        };\n\n        // Check for decimal points to handle cents\n        if (numWithoutCommas.includes('.')) {\n          const [dollars, cents] = numWithoutCommas.split('.');\n          return `${toWords(Number.parseInt(dollars))} ${currencyMap[currency] || 'currency'}${cents ? ` and ${toWords(Number.parseInt(cents))} cents` : ''}`;\n        }\n\n        // Handle whole numbers\n        return `${toWords(Number.parseInt(numWithoutCommas))} ${currencyMap[currency] || 'currency'}`;\n      })\n\n      // Convert phone numbers (e.g., \"555-555-5555\" → \"five five five, five five five, five five five five\")\n      .replace(/(\\d{3})-(\\d{3})-(\\d{4})/g, (_, p1, p2, p3) => {\n        return `${spellOutDigits(p1)}, ${spellOutDigits(p2)}, ${spellOutDigits(p3)}`;\n      })\n  );\n}\n\n// Helper function to spell out individual digits as words (for phone numbers)\nfunction spellOutDigits(num: string): string {\n  return num\n    .split('')\n    .map((digit) => toWords(Number.parseInt(digit)))\n    .join(' ');\n}\n\n// Example usage\nconsole.log(normalizeText('$1,000')); // \"one thousand dollars\"\nconsole.log(normalizeText('£1000')); // \"one thousand pounds\"\nconsole.log(normalizeText('€1000')); // \"one thousand euros\"\nconsole.log(normalizeText('¥1000')); // \"one thousand yen\"\nconsole.log(normalizeText('$1,234.56')); // \"one thousand two hundred thirty-four dollars and fifty-six cents\"\nconsole.log(normalizeText('555-555-5555')); // \"five five five, five five five, five five five five\""
        }
      ],
      "relevance": 0.84
    },
    {
      "codeTitle": "Environment Variables Configuration for Python Implementation",
      "codeDescription": "Sets up environment variables for the Python implementation. The .env file contains the ElevenLabs API key and agent ID required for connecting to the ElevenLabs Conversational AI.",
      "codeLanguage": "text",
      "codeTokens": 79,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-custom-server.mdx#2025-04-17_snippet_8",
      "pageTitle": "Twilio Integration Guide for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "text",
          "code": "ELEVENLABS_API_KEY=<api-key-here>\nAGENT_ID=<agent-id-here>"
        }
      ],
      "relevance": 0.84
    },
    {
      "codeTitle": "Defining Cal.com Authentication Header Format",
      "codeDescription": "This snippet shows the required format for Cal.com API authentication headers. The API expects the token to be prefixed with 'Bearer ' followed by your API key.",
      "codeLanguage": "plaintext",
      "codeTokens": 65,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/cal.com.mdx#2025-04-17_snippet_1",
      "pageTitle": "Integrating Conversational AI with Cal.com for Automated Meeting Scheduling",
      "codeList": [
        {
          "language": "plaintext",
          "code": "'Authorization': 'Bearer YOUR_API_KEY'"
        }
      ],
      "relevance": 0.84
    },
    {
      "codeTitle": "Data Collection Instructions for LLM",
      "codeDescription": "Instructions for the LLM to extract specific data points from conversation transcripts.",
      "codeLanguage": "plaintext",
      "codeTokens": 43,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/conversational-ai-guide-support-agent.mdx#2025-04-17_snippet_3",
      "pageTitle": "Building Conversational Support Assistant Guide",
      "codeList": [
        {
          "language": "plaintext",
          "code": "Extract the user's questions & inquiries from the conversation."
        }
      ],
      "relevance": 0.84
    },
    {
      "codeTitle": "Visualizing Event Flow in Conversational Applications",
      "codeDescription": "A sequence diagram showing the typical flow of events during a conversation, including connection establishment, audio exchange, user interactions, tool calls, and interruption handling.",
      "codeLanguage": "mermaid",
      "codeTokens": 225,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-events.mdx#2025-04-17_snippet_7",
      "pageTitle": "Client Events Documentation for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "mermaid",
          "code": "sequenceDiagram\n    participant Client\n    participant Server\n\n    Server->>Client: conversation_initiation_metadata\n    Note over Client,Server: Connection established\n    Server->>Client: ping\n    Client->>Server: pong\n    Server->>Client: audio\n    Note over Client: Playing audio\n    Note over Client: User responds\n    Server->>Client: user_transcript\n    Server->>Client: agent_response\n    Server->>Client: audio\n    Server->>Client: client_tool_call\n    Note over Client: Client tool runs\n    Client->>Server: client_tool_result\n    Server->>Client: agent_response\n    Server->>Client: audio\n    Note over Client: Playing audio\n    Note over Client: Interruption detected\n    Server->>Client: agent_response_correction"
        }
      ],
      "relevance": 0.84
    },
    {
      "codeTitle": "Adding Emotion Through Narrative Context",
      "codeDescription": "This example demonstrates how to convey emotions in text-to-speech through narrative context and dialogue tags. The AI will interpret the emotional cues and adjust voice accordingly.",
      "codeLanguage": "text",
      "codeTokens": 72,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/controls.mdx#2025-04-17_snippet_10",
      "pageTitle": "Text-to-Speech Controls Guide",
      "codeList": [
        {
          "language": "text",
          "code": "You're leaving?\" she asked, her voice trembling with sadness. \"That's it!\" he exclaimed triumphantly."
        }
      ],
      "relevance": 0.84
    },
    {
      "codeTitle": "Parsing ElevenLabs Webhook Signature Format",
      "codeDescription": "Example of the ElevenLabs-Signature header format used for webhook authentication.",
      "codeLanguage": "json",
      "codeTokens": 42,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/webhook-hmac-authentication.mdx#2025-04-17_snippet_0",
      "pageTitle": "Validating ElevenLabs Webhooks",
      "codeList": [
        {
          "language": "json",
          "code": "t=timestamp,v0=hash"
        }
      ],
      "relevance": 0.835
    },
    {
      "codeTitle": "Displaying API Error Code 429 Table in Markdown",
      "codeDescription": "This code snippet presents a markdown table for API error code 429, covering issues related to concurrent request limits and system busy states, along with their causes and solutions.",
      "codeLanguage": "markdown",
      "codeTokens": 184,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/error-messages.mdx#2025-04-17_snippet_3",
      "pageTitle": "ElevenLabs Error Messages Documentation",
      "codeList": [
        {
          "language": "markdown",
          "code": "| Code                               | Overview                                                                                                                                                                                                                                                                              |\n| ---------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| too_many_concurrent_requests <br/> | **Cause:** You have exceeded the concurrency limit for your subscription. <br/> **Solution:** See [concurrency limits and priority](/docs/models#concurrency-and-priority) for more information.                                                                                      |\n| system_busy                        | **Cause:** Our services are experiencing high levels of traffic and your request could not be processed. <br/> **Solution:** Retry the request later, with exponential backoff. Consider upgrading your subscription to get [higher priority](/docs/models#concurrency-and-priority). |"
        }
      ],
      "relevance": 0.835
    },
    {
      "codeTitle": "Supabase Deployment Commands",
      "codeDescription": "Commands for deploying the application to Supabase, including linking the project, applying database migrations, and deploying the Edge Function.",
      "codeLanguage": "bash",
      "codeTokens": 53,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#2025-04-17_snippet_8",
      "pageTitle": "Building a Telegram Transcription Bot with ElevenLabs and TypeScript",
      "codeList": [
        {
          "language": "bash",
          "code": "supabase link"
        },
        {
          "language": "bash",
          "code": "supabase db push"
        },
        {
          "language": "bash",
          "code": "supabase functions deploy --no-verify-jwt scribe-bot"
        }
      ],
      "relevance": 0.835
    },
    {
      "codeTitle": "Rendering Scribe v1 Model Card in JSX",
      "codeDescription": "This snippet creates a card component to display information about the Scribe v1 speech recognition model. It uses custom components like CardGroup and Card, likely from a UI framework, to structure the content.",
      "codeLanguage": "JSX",
      "codeTokens": 187,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/stt-models.mdx#2025-04-17_snippet_0",
      "pageTitle": "ElevenLabs Documentation - Speech Recognition Model",
      "codeList": [
        {
          "language": "JSX",
          "code": "<CardGroup cols={1} rows={1}>\n  <Card title=\"Scribe v1\" href=\"/docs/models#scribe-v1\">\n    State-of-the-art speech recognition model\n    <div className=\"mt-4 space-y-2\">\n      <div className=\"text-sm\">Accurate transcription in 99 languages</div>\n      <div className=\"text-sm\">Precise word-level timestamps</div>\n      <div className=\"text-sm\">Speaker diarization</div>\n      <div className=\"text-sm\">Dynamic audio tagging</div>\n    </div>\n  </Card>\n</CardGroup>"
        }
      ],
      "relevance": 0.835
    },
    {
      "codeTitle": "Generating Signed URLs with cURL",
      "codeDescription": "Direct API request using cURL to obtain a signed URL. This command makes a GET request to the ElevenLabs API with an API key for authentication.",
      "codeLanguage": "bash",
      "codeTokens": 94,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/authentication.mdx#2025-04-17_snippet_2",
      "pageTitle": "Authentication for ElevenLabs Conversational AI Agents",
      "codeList": [
        {
          "language": "bash",
          "code": "curl -X GET \"https://api.elevenlabs.io/v1/convai/conversation/get_signed_url?agent_id=your-agent-id\" \\\n-H \"xi-api-key: your-api-key\""
        }
      ],
      "relevance": 0.835
    },
    {
      "codeTitle": "Running the TypeScript Script for ElevenLabs Text-to-Speech",
      "codeDescription": "This command runs the TypeScript script that demonstrates the ElevenLabs Text-to-Speech API usage using tsx.",
      "codeLanguage": "typescript",
      "codeTokens": 49,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/quickstart.mdx#2025-04-17_snippet_3",
      "pageTitle": "ElevenLabs API Developer Quickstart",
      "codeList": [
        {
          "language": "typescript",
          "code": "npx tsx example.mts"
        }
      ],
      "relevance": 0.835
    },
    {
      "codeTitle": "Implementing Voice Generation Pauses in ElevenLabs",
      "codeDescription": "XML syntax for adding timed pauses in voice generation. Uses the break element with a time attribute specified in seconds.",
      "codeLanguage": "xml",
      "codeTokens": 49,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/controls.mdx#2025-04-17_snippet_12",
      "pageTitle": "Text-to-Speech Controls Guide",
      "codeList": [
        {
          "language": "xml",
          "code": "<break time=\"x.xs\" />"
        }
      ],
      "relevance": 0.835
    },
    {
      "codeTitle": "Adding Audio Native Script to Framer Page",
      "codeDescription": "Script tag that needs to be added to the end of body tag in Framer site settings to enable Audio Native functionality.",
      "codeLanguage": "html",
      "codeTokens": 67,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/audio-tools/audio-native/framer.mdx#2025-04-17_snippet_0",
      "pageTitle": "Audio Native Integration Guide for Framer Websites",
      "codeList": [
        {
          "language": "html",
          "code": "<script src=\"https://elevenlabs.io/player/audioNativeHelper.js\" type=\"text/javascript\"></script>"
        }
      ],
      "relevance": 0.83
    },
    {
      "codeTitle": "Creating Supabase Project",
      "codeDescription": "Command to initialize a new Supabase project locally using the Supabase CLI.",
      "codeLanguage": "bash",
      "codeTokens": 45,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#2025-04-17_snippet_0",
      "pageTitle": "Building a Telegram Transcription Bot with ElevenLabs and TypeScript",
      "codeList": [
        {
          "language": "bash",
          "code": "supabase init"
        }
      ],
      "relevance": 0.83
    },
    {
      "codeTitle": "Rendering a Product Guide Card with Image in JSX",
      "codeDescription": "This snippet demonstrates how to create a card for product guides using JSX. It includes a title, description, and an image with specific styling.",
      "codeLanguage": "jsx",
      "codeTokens": 189,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/overview.mdx#2025-04-17_snippet_4",
      "pageTitle": "ElevenLabs Documentation Overview",
      "codeList": [
        {
          "language": "jsx",
          "code": "<CardGroup cols={1}>\n\n<Card href=\"/docs/product-guides/overview\">\n  <div className=\"flex items-center justify-between\">\n    <div className=\"text-left\">\n      <div className=\"t-default text-base font-semibold\">Product guides</div>\n      <p>Explore our product guides for step-by-step guidance</p>\n    </div>\n    <div className=\"flex items-center justify-center\">\n      <img\n        src=\"/assets/images/product-guides/voices/voice-library.webp\"\n        alt=\"Voice library\"\n        style={{ pointerEvents: 'none', width: '200px' }}\n      />\n    </div>\n  </div>\n</Card>\n\n</CardGroup>"
        }
      ],
      "relevance": 0.83
    },
    {
      "codeTitle": "Basic Audio Native Component Usage",
      "codeDescription": "Example of basic implementation of the Audio Native component in a React page component.",
      "codeLanguage": "tsx",
      "codeTokens": 123,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/audio-tools/audio-native/react.mdx#2025-04-17_snippet_2",
      "pageTitle": "Audio Native React Integration Guide",
      "codeList": [
        {
          "language": "tsx",
          "code": "import { ElevenLabsAudioNative } from './path/to/ElevenLabsAudioNative';\n\nexport default function Page() {\n  return (\n    <div>\n      <h1>Your Page Title</h1>\n\n      // Insert the public user ID from the embed code snippet\n      <ElevenLabsAudioNative publicUserId=\"<your-public-user-id>\" />\n\n      <p>Your page content...</p>\n    </div>\n  );\n}"
        }
      ],
      "relevance": 0.83
    },
    {
      "codeTitle": "Executing Python Script for Forced Alignment",
      "codeDescription": "This command demonstrates how to run the Python script for forced alignment from the command line.",
      "codeLanguage": "python",
      "codeTokens": 38,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/forced-alignment/quickstart.mdx#2025-04-17_snippet_2",
      "pageTitle": "Forced Alignment API Quickstart Guide",
      "codeList": [
        {
          "language": "python",
          "code": "python example.py"
        }
      ],
      "relevance": 0.83
    },
    {
      "codeTitle": "Installing ElevenLabs SDK Dependencies",
      "codeDescription": "Installation commands for the required Python packages including ElevenLabs SDK and python-dotenv.",
      "codeLanguage": "bash",
      "codeTokens": 44,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/pronunciation-dictionaries.mdx#2025-04-17_snippet_0",
      "pageTitle": "Using Pronunciation Dictionaries with ElevenLabs Python SDK",
      "codeList": [
        {
          "language": "bash",
          "code": "pip install elevenlabs"
        },
        {
          "language": "bash",
          "code": "pip install python-dotenv"
        }
      ],
      "relevance": 0.83
    },
    {
      "codeTitle": "Setting API Key Header in HTTP Request",
      "codeDescription": "Shows the required HTTP header format for including the API key in requests.",
      "codeLanguage": "bash",
      "codeTokens": 43,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/api-reference/pages/authentication.mdx#2025-04-17_snippet_0",
      "pageTitle": "ElevenLabs API Authentication Documentation",
      "codeList": [
        {
          "language": "bash",
          "code": "xi-api-key: ELEVENLABS_API_KEY"
        }
      ],
      "relevance": 0.83
    },
    {
      "codeTitle": "Setting Conversation Volume in ElevenLabs JavaScript SDK",
      "codeDescription": "Method to set the output volume of the conversation. Accepts an object with a volume field between 0 and 1.",
      "codeLanguage": "javascript",
      "codeTokens": 58,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/javascript.mdx#2025-04-17_snippet_8",
      "pageTitle": "JavaScript SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "await conversation.setVolume({ volume: 0.5 });"
        }
      ],
      "relevance": 0.825
    },
    {
      "codeTitle": "Running the Node.js Server for Twilio Integration",
      "codeDescription": "Command to start the Node.js server that handles the Twilio and ElevenLabs integration. This will initialize the WebSocket connections and start listening for incoming Twilio calls.",
      "codeLanguage": "bash",
      "codeTokens": 59,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-custom-server.mdx#2025-04-17_snippet_4",
      "pageTitle": "Twilio Integration Guide for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "bash",
          "code": "node index.js"
        }
      ],
      "relevance": 0.825
    },
    {
      "codeTitle": "Cloning and Setting Up WebSocket Connector",
      "codeDescription": "Commands to clone the WebSocket connector repository and set up the initial environment.",
      "codeLanguage": "bash",
      "codeTokens": 72,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/telephony/vonage.mdx#2025-04-17_snippet_0",
      "pageTitle": "Vonage Integration with ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "bash",
          "code": "git clone https://github.com/nexmo-se/elevenlabs-agent-ws-connector.git\ncd elevenlabs-agent-ws-connector\ncp .env.example .env"
        }
      ],
      "relevance": 0.825
    },
    {
      "codeTitle": "Implementing File-Based Pronunciation Dictionary",
      "codeDescription": "Code to create a pronunciation dictionary from a file and generate text-to-speech with and without the dictionary.",
      "codeLanguage": "python",
      "codeTokens": 209,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/pronunciation-dictionaries.mdx#2025-04-17_snippet_3",
      "pageTitle": "Using Pronunciation Dictionaries with ElevenLabs Python SDK",
      "codeList": [
        {
          "language": "python",
          "code": "import requests\nfrom elevenlabs import play, PronunciationDictionaryVersionLocator\n\nwith open(\"dictionary.pls\", \"rb\") as f:\n    pronunciation_dictionary = client.pronunciation_dictionary.add_from_file(\n        file=f.read(), name=\"example\"\n    )\n\naudio_1 = client.generate(\n    text=\"Without the dictionary: tomato\",\n    voice=\"Rachel\",\n    model=\"eleven_turbo_v2\",\n)\n\naudio_2 = client.generate(\n    text=\"With the dictionary: tomato\",\n    voice=\"Rachel\",\n    model=\"eleven_turbo_v2\",\n    pronunciation_dictionary_locators=[\n        PronunciationDictionaryVersionLocator(\n            pronunciation_dictionary_id=pronunciation_dictionary.id,\n            version_id=pronunciation_dictionary.version_id,\n        )\n    ],\n)\n\nplay(audio_1)\nplay(audio_2)"
        }
      ],
      "relevance": 0.825
    },
    {
      "codeTitle": "Retrieving Conversation ID in ElevenLabs JavaScript SDK",
      "codeDescription": "Method to get the conversation ID from the current conversation instance.",
      "codeLanguage": "javascript",
      "codeTokens": 39,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/javascript.mdx#2025-04-17_snippet_7",
      "pageTitle": "JavaScript SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "const id = conversation.getId();"
        }
      ],
      "relevance": 0.825
    },
    {
      "codeTitle": "Previewing ElevenLabs Python SDK Generation",
      "codeDescription": "Command to generate a preview of the Python SDK using Fern CLI. This allows for local testing of SDK changes before publishing.",
      "codeLanguage": "sh",
      "codeTokens": 53,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/README.md#2025-04-17_snippet_5",
      "pageTitle": "ElevenLabs Documentation Project Setup and SDK Generation",
      "codeList": [
        {
          "language": "sh",
          "code": "fern generate --group python-sdk --preview"
        }
      ],
      "relevance": 0.825
    },
    {
      "codeTitle": "Installing ElevenLabs Dependency",
      "codeDescription": "Command to install the ElevenLabs React package, which provides necessary functionality for interacting with ElevenLabs AI agents.",
      "codeLanguage": "bash",
      "codeTokens": 53,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/nextjs.mdx#2025-04-17_snippet_1",
      "pageTitle": "Implementing Conversational AI with Next.js and ElevenLabs",
      "codeList": [
        {
          "language": "bash",
          "code": "npm install @11labs/react"
        }
      ],
      "relevance": 0.825
    },
    {
      "codeTitle": "Initializing ElevenLabs Client SDK",
      "codeDescription": "Setup code to initialize the ElevenLabs client using API key from environment variables.",
      "codeLanguage": "python",
      "codeTokens": 88,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/pronunciation-dictionaries.mdx#2025-04-17_snippet_1",
      "pageTitle": "Using Pronunciation Dictionaries with ElevenLabs Python SDK",
      "codeList": [
        {
          "language": "python",
          "code": "import os\nfrom elevenlabs.client import ElevenLabs\n\nELEVENLABS_API_KEY = os.getenv(\"ELEVENLABS_API_KEY\")\nclient = ElevenLabs(\n    api_key=ELEVENLABS_API_KEY,\n)"
        }
      ],
      "relevance": 0.825
    },
    {
      "codeTitle": "Using CMU Arpabet Phoneme Tags for Pronunciation Control",
      "codeDescription": "This example shows how to use CMU Arpabet phoneme tags to specify exact pronunciation of a word. Compatible with Eleven Flash v2, Eleven Turbo v2, and Eleven English v1 models.",
      "codeLanguage": "xml",
      "codeTokens": 90,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/controls.mdx#2025-04-17_snippet_2",
      "pageTitle": "Text-to-Speech Controls Guide",
      "codeList": [
        {
          "language": "xml",
          "code": "<phoneme alphabet=\"cmu-arpabet\" ph=\"M AE1 D IH0 S AH0 N\">\n  Madison\n</phoneme>"
        }
      ],
      "relevance": 0.82
    },
    {
      "codeTitle": "Connecting to WebSocket API with Agent ID in Bash",
      "codeDescription": "Example of the WebSocket URL format for connecting to the ElevenLabs Conversational AI API using an agent_id parameter. This is suitable for public agents that don't require additional authentication.",
      "codeLanguage": "bash",
      "codeTokens": 81,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/api-reference/websocket.mdx#2025-04-17_snippet_0",
      "pageTitle": "ElevenLabs WebSocket API for Conversational AI",
      "codeList": [
        {
          "language": "bash",
          "code": "wss://api.elevenlabs.io/v1/convai/conversation?agent_id=<your-agent-id>"
        }
      ],
      "relevance": 0.82
    },
    {
      "codeTitle": "Installing ElevenLabs and dotenv in Python",
      "codeDescription": "This code snippet shows how to install the ElevenLabs library and the python-dotenv library using pip. These libraries are required for using ElevenLabs API and loading environment variables in Python.",
      "codeLanguage": "python",
      "codeTokens": 67,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/quickstart-install-sdk.mdx#2025-04-17_snippet_0",
      "pageTitle": "Installing ElevenLabs and dotenv Libraries",
      "codeList": [
        {
          "language": "python",
          "code": "pip install elevenlabs\npip install python-dotenv"
        }
      ],
      "relevance": 0.82
    },
    {
      "codeTitle": "Removing Rules from Pronunciation Dictionary",
      "codeDescription": "Implementation for removing specific pronunciation rules and testing the updated dictionary.",
      "codeLanguage": "python",
      "codeTokens": 160,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/pronunciation-dictionaries.mdx#2025-04-17_snippet_4",
      "pageTitle": "Using Pronunciation Dictionaries with ElevenLabs Python SDK",
      "codeList": [
        {
          "language": "python",
          "code": "pronunciation_dictionary_rules_removed = (\n    client.pronunciation_dictionary.remove_rules_from_the_pronunciation_dictionary(\n        pronunciation_dictionary_id=pronunciation_dictionary.id,\n        rule_strings=[\"tomato\", \"Tomato\"],\n    )\n)\n\naudio_3 = client.generate(\n    text=\"With the rule removed: tomato\",\n    voice=\"Rachel\",\n    model=\"eleven_turbo_v2\",\n    pronunciation_dictionary_locators=[\n        PronunciationDictionaryVersionLocator(\n            pronunciation_dictionary_id=pronunciation_dictionary_rules_removed.id,\n            version_id=pronunciation_dictionary_rules_removed.version_id,\n        )\n    ],\n)\n\nplay(audio_3)"
        }
      ],
      "relevance": 0.82
    },
    {
      "codeTitle": "Making a Test Outbound Call Request with cURL",
      "codeDescription": "This cURL command sends a POST request to initiate an outbound call. It includes parameters for the custom prompt, first message, and the phone number to call.",
      "codeLanguage": "bash",
      "codeTokens": 166,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-outbound-calling.mdx#2025-04-17_snippet_6",
      "pageTitle": "Building Outbound Calling AI Agents with Twilio and ElevenLabs",
      "codeList": [
        {
          "language": "bash",
          "code": "curl -X POST https://<your-ngrok-url>/outbound-call \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"prompt\": \"You are Eric, an outbound car sales agent. You are calling to sell a new car to the customer. Be friendly and professional and answer all questions.\",\n    \"first_message\": \"Hello Thor, my name is Eric, I heard you were looking for a new car! What model and color are you looking for?\",\n    \"number\": \"number-to-call\"\n    }'"
        }
      ],
      "relevance": 0.82
    },
    {
      "codeTitle": "Installing ElevenLabs Python Client Library",
      "codeDescription": "Command to install the official ElevenLabs Python client library using pip package manager.",
      "codeLanguage": "bash",
      "codeTokens": 37,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/api-reference/pages/introduction.mdx#2025-04-17_snippet_0",
      "pageTitle": "ElevenLabs API Introduction",
      "codeList": [
        {
          "language": "bash",
          "code": "pip install elevenlabs"
        }
      ],
      "relevance": 0.82
    },
    {
      "codeTitle": "Implementing End Call Tool using cURL",
      "codeDescription": "Command-line example showing how to add the End Call tool to an agent using cURL. This direct API call creates an agent with the end call tool configured as a system tool.",
      "codeLanguage": "bash",
      "codeTokens": 165,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/end-call.mdx#2025-04-17_snippet_2",
      "pageTitle": "Implementing the End Call Tool in ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "bash",
          "code": "curl -X POST https://api.elevenlabs.io/v1/convai/agents/create \\\n     -H \"xi-api-key: YOUR_API_KEY\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n  \"conversation_config\": {\n    \"agent\": {\n      \"prompt\": {\n        \"tools\": [\n          {\n            \"type\": \"system\",\n            \"name\": \"end_call\",\n            \"description\": \"\"\n          }\n        ]\n      }\n    }\n  }\n}'"
        }
      ],
      "relevance": 0.82
    },
    {
      "codeTitle": "Storing ElevenLabs API Key in Environment Variable File (JavaScript)",
      "codeDescription": "This code snippet demonstrates how to store the ElevenLabs API key in a .env file. The API key is assigned to the ELEVENLABS_API_KEY environment variable, which can be securely accessed by the application.",
      "codeLanguage": "javascript",
      "codeTokens": 77,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/quickstart-api-key.mdx#2025-04-17_snippet_0",
      "pageTitle": "Configuring ElevenLabs API Authentication",
      "codeList": [
        {
          "language": "javascript",
          "code": "ELEVENLABS_API_KEY=<your_api_key_here>"
        }
      ],
      "relevance": 0.82
    },
    {
      "codeTitle": "ElevenLabs SIP Origination URI Configuration",
      "codeDescription": "The standard SIP origination URI endpoint for routing inbound calls from client systems to ElevenLabs platform. This URI serves as the destination endpoint for all inbound call routing.",
      "codeLanguage": "plaintext",
      "codeTokens": 73,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/sip-trunking.mdx#2025-04-17_snippet_0",
      "pageTitle": "SIP Trunking Integration with ElevenLabs Voice AI",
      "codeList": [
        {
          "language": "plaintext",
          "code": "sip:sip.rtc.elevenlabs.io:5060;transport=tcp"
        }
      ],
      "relevance": 0.815
    },
    {
      "codeTitle": "Starting Conversation Session",
      "codeDescription": "Code to start the conversation session with the AI agent.",
      "codeLanguage": "python",
      "codeTokens": 33,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/python.mdx#2025-04-17_snippet_8",
      "pageTitle": "ElevenLabs Python SDK Documentation",
      "codeList": [
        {
          "language": "python",
          "code": "conversation.start_session()"
        }
      ],
      "relevance": 0.815
    },
    {
      "codeTitle": "Environment Configuration - API Key Setup",
      "codeDescription": "Configuration for the .env file to store the ElevenLabs API key securely.",
      "codeLanguage": "bash",
      "codeTokens": 53,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/sound-effects/basics.mdx#2025-04-17_snippet_2",
      "pageTitle": "Text to Sound Effects Generation with ElevenLabs API",
      "codeList": [
        {
          "language": "bash",
          "code": "ELEVENLABS_API_KEY=your_elevenlabs_api_key_here"
        }
      ],
      "relevance": 0.815
    },
    {
      "codeTitle": "Evaluation Criteria Prompt for Conversation Analysis",
      "codeDescription": "Prompt defining the success criteria for evaluating whether the assistant successfully handled user inquiries.",
      "codeLanguage": "plaintext",
      "codeTokens": 78,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/conversational-ai-guide-support-agent.mdx#2025-04-17_snippet_2",
      "pageTitle": "Building Conversational Support Assistant Guide",
      "codeList": [
        {
          "language": "plaintext",
          "code": "The assistant was able to answer all of the queries or redirect them to a relevant support channel.\n\nSuccess Criteria:\n- All user queries were answered satisfactorily.\n- The user was redirected to a relevant support channel if needed."
        }
      ],
      "relevance": 0.815
    },
    {
      "codeTitle": "Initializing npm Project and Installing Dependencies",
      "codeDescription": "Commands to initialize a new npm project and install required packages including Vite and the ElevenLabs client library.",
      "codeLanguage": "bash",
      "codeTokens": 60,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/vite.mdx#2025-04-17_snippet_1",
      "pageTitle": "Creating a Conversational AI Web Client with Vite and ElevenLabs",
      "codeList": [
        {
          "language": "bash",
          "code": "npm init -y\nnpm install vite @11labs/client"
        }
      ],
      "relevance": 0.815
    },
    {
      "codeTitle": "Creating Project Directory for ElevenLabs Conversational AI",
      "codeDescription": "Commands to create a new directory for the ElevenLabs Conversational AI project and navigate into it.",
      "codeLanguage": "bash",
      "codeTokens": 63,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/vite.mdx#2025-04-17_snippet_0",
      "pageTitle": "Creating a Conversational AI Web Client with Vite and ElevenLabs",
      "codeList": [
        {
          "language": "bash",
          "code": "mkdir elevenlabs-conversational-ai\ncd elevenlabs-conversational-ai"
        }
      ],
      "relevance": 0.815
    },
    {
      "codeTitle": "Installing Dependencies for Expo React Native Voice Agent",
      "codeDescription": "Bash commands to install necessary dependencies for the Expo React Native project, including ElevenLabs React SDK and Expo modules.",
      "codeLanguage": "bash",
      "codeTokens": 120,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/expo-react-native.mdx#2025-04-17_snippet_2",
      "pageTitle": "Building Cross-platform Voice Agents with Expo React Native and ElevenLabs",
      "codeList": [
        {
          "language": "bash",
          "code": "npx expo install @11labs/react\nnpx expo install expo-dev-client # tunnel support\nnpx expo install react-native-webview # DOM components support\nnpx expo install react-dom react-native-web @expo/metro-runtime # RN web support\n# Cool client tools\nnpx expo install expo-battery\nnpx expo install expo-brightness"
        }
      ],
      "relevance": 0.815
    },
    {
      "codeTitle": "Configuring Environment Variables for ElevenLabs-Twilio Integration",
      "codeDescription": "Environment variables setup for the ElevenLabs-Twilio integration, including the server domain and ElevenLabs API key.",
      "codeLanguage": "Shell",
      "codeTokens": 65,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/twilio.mdx#2025-04-17_snippet_2",
      "pageTitle": "Integrating ElevenLabs Text-to-Speech with Twilio Phone Calls",
      "codeList": [
        {
          "language": "shell",
          "code": "# .env\nSERVER_DOMAIN=\nELEVENLABS_API_KEY="
        }
      ],
      "relevance": 0.81
    },
    {
      "codeTitle": "Setting Environment Variables for Next.js Conversational AI Project",
      "codeDescription": "Defines the necessary environment variables for the Next.js project, including API keys for ElevenLabs, Resend, and Upstash Redis. These variables are crucial for authentication and integration with external services.",
      "codeLanguage": "bash",
      "codeTokens": 132,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#2025-04-17_snippet_2",
      "pageTitle": "Data Collection and Analysis with Conversational AI in Next.js",
      "codeList": [
        {
          "language": "bash",
          "code": "ELEVENLABS_CONVAI_WEBHOOK_SECRET=\nELEVENLABS_API_KEY=\nELEVENLABS_AGENT_ID=\n\n# Resend\nRESEND_API_KEY=\nRESEND_FROM_EMAIL=\n\n# Upstash Redis\nKV_URL=\nKV_REST_API_READ_ONLY_TOKEN=\nREDIS_URL=\nKV_REST_API_TOKEN=\nKV_REST_API_URL="
        }
      ],
      "relevance": 0.81
    },
    {
      "codeTitle": "Configuring Order Completion Evaluation Criteria",
      "codeDescription": "Evaluation prompt for analyzing conversation success based on specific order completion criteria.",
      "codeLanguage": "plaintext",
      "codeTokens": 93,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/conversational-ai-guide-restaurant-agent.mdx#2025-04-17_snippet_2",
      "pageTitle": "Creating an ElevenLabs Voice Assistant for Restaurant Orders",
      "codeList": [
        {
          "language": "plaintext",
          "code": "Evaluate if the conversation resulted in a successful order.\nSuccess criteria:\n- Customer selected at least one pierogi variety\n- Quantity was confirmed\n- Delivery address was provided\n- Total price was communicated\n- Delivery time estimate was given\nReturn \"success\" only if ALL criteria are met."
        }
      ],
      "relevance": 0.81
    },
    {
      "codeTitle": "Customizing ElevenLabs Widget Visuals in HTML",
      "codeDescription": "This HTML code shows how to customize the visual appearance of the ElevenLabs Conversational AI widget. It includes options for setting a custom avatar image and orb gradient colors.",
      "codeLanguage": "html",
      "codeTokens": 130,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/widget.mdx#2025-04-17_snippet_2",
      "pageTitle": "Widget Customization for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "html",
          "code": "<elevenlabs-convai\n  avatar-image-url=\"https://...\" // Optional: Custom avatar image\n  avatar-orb-color-1=\"#6DB035\" // Optional: Orb gradient color 1\n  avatar-orb-color-2=\"#F5CABB\" // Optional: Orb gradient color 2\n></elevenlabs-convai>"
        }
      ],
      "relevance": 0.81
    },
    {
      "codeTitle": "Environment Configuration",
      "codeDescription": "Example of .env file configuration for storing the ElevenLabs API key.",
      "codeLanguage": "python",
      "codeTokens": 53,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/streaming.mdx#2025-04-17_snippet_2",
      "pageTitle": "Speech-to-Text Streaming Guide with ElevenLabs SDK",
      "codeList": [
        {
          "language": "python",
          "code": "ELEVENLABS_API_KEY=your_elevenlabs_api_key_here"
        }
      ],
      "relevance": 0.81
    },
    {
      "codeTitle": "Setting Up Environment Variables for ElevenLabs API",
      "codeDescription": "Create a .env file to store the ElevenLabs API key securely.",
      "codeLanguage": "bash",
      "codeTokens": 54,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/websockets.mdx#2025-04-17_snippet_2",
      "pageTitle": "WebSocket Streaming for Real-Time Audio Generation with ElevenLabs API",
      "codeList": [
        {
          "language": "bash",
          "code": "ELEVENLABS_API_KEY=your_elevenlabs_api_key_here"
        }
      ],
      "relevance": 0.81
    },
    {
      "codeTitle": "Initializing Project for Twilio ElevenLabs Integration",
      "codeDescription": "Commands to set up a new Node.js project directory for the Twilio and ElevenLabs integration. Creates a project folder, initializes npm, and sets the project type to module.",
      "codeLanguage": "bash",
      "codeTokens": 89,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-outbound-calling.mdx#2025-04-17_snippet_0",
      "pageTitle": "Building Outbound Calling AI Agents with Twilio and ElevenLabs",
      "codeList": [
        {
          "language": "bash",
          "code": "mkdir conversational-ai-twilio\ncd conversational-ai-twilio\nnpm init -y; npm pkg set type=\"module\";"
        }
      ],
      "relevance": 0.805
    },
    {
      "codeTitle": "Installing Dependencies for Twilio ElevenLabs Integration",
      "codeDescription": "Command to install required Node.js packages including Fastify, WebSocket, Twilio, and environment management libraries needed for the integration.",
      "codeLanguage": "bash",
      "codeTokens": 68,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-outbound-calling.mdx#2025-04-17_snippet_1",
      "pageTitle": "Building Outbound Calling AI Agents with Twilio and ElevenLabs",
      "codeList": [
        {
          "language": "bash",
          "code": "npm install @fastify/formbody @fastify/websocket dotenv fastify ws twilio"
        }
      ],
      "relevance": 0.805
    },
    {
      "codeTitle": "Implementing Audio Native HTML Embed",
      "codeDescription": "Basic HTML embed code snippet for the ElevenLabs Audio Native player that includes required attributes and configuration options.",
      "codeLanguage": "html",
      "codeTokens": 176,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/audio-tools/audio-native/react.mdx#2025-04-17_snippet_0",
      "pageTitle": "Audio Native React Integration Guide",
      "codeList": [
        {
          "language": "html",
          "code": "<div\n  id=\"elevenlabs-audionative-widget\"\n  data-height=\"90\"\n  data-width=\"100%\"\n  data-frameborder=\"no\"\n  data-scrolling=\"no\"\n  data-publicuserid=\"public-user-id\"\n  data-playerurl=\"https://elevenlabs.io/player/index.html\"\n  data-projectid=\"project-id\"\n>\n  Loading the <a href=\"https://elevenlabs.io/text-to-speech\" target=\"_blank\" rel=\"noopener\">Elevenlabs Text to Speech</a> AudioNative Player...\n</div>\n<script src=\"https://elevenlabs.io/player/audioNativeHelper.js\" type=\"text/javascript\"></script>"
        }
      ],
      "relevance": 0.805
    },
    {
      "codeTitle": "Getting Input/Output Volume in ElevenLabs JavaScript SDK",
      "codeDescription": "Methods to retrieve the current input and output volume on a scale from 0 to 1, where 0 is -100 dB and 1 is -30 dB.",
      "codeLanguage": "javascript",
      "codeTokens": 74,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/javascript.mdx#2025-04-17_snippet_9",
      "pageTitle": "JavaScript SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "const inputVolume = await conversation.getInputVolume();\nconst outputVolume = await conversation.getOutputVolume();"
        }
      ],
      "relevance": 0.805
    },
    {
      "codeTitle": "Configuring Supabase Storage Bucket",
      "codeDescription": "Sets up a storage bucket in Supabase for storing audio files. This configuration is added to the config.toml file.",
      "codeLanguage": "toml",
      "codeTokens": 85,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/supabase-streaming.mdx#2025-04-17_snippet_1",
      "pageTitle": "Streaming and Caching Speech with Supabase and ElevenLabs",
      "codeList": [
        {
          "language": "toml",
          "code": "[storage.buckets.audio]\npublic = false\nfile_size_limit = \"50MiB\"\nallowed_mime_types = [\"audio/mp3\"]\nobjects_path = \"./audio\""
        }
      ],
      "relevance": 0.805
    },
    {
      "codeTitle": "Installing ElevenLabs SDK in Python",
      "codeDescription": "Command to install the ElevenLabs SDK using pip in Python.",
      "codeLanguage": "python",
      "codeTokens": 38,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/synchronous.mdx#2025-04-17_snippet_0",
      "pageTitle": "Synchronous Speech to Text with ElevenLabs",
      "codeList": [
        {
          "language": "python",
          "code": "pip install elevenlabs"
        }
      ],
      "relevance": 0.805
    },
    {
      "codeTitle": "Structuring Contextual Update Event in JavaScript",
      "codeDescription": "Demonstrates the structure of a contextual update event in JSON format. This event type is used to send non-interrupting background information to the conversation.",
      "codeLanguage": "javascript",
      "codeTokens": 78,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-to-server-events.mdx#2025-04-17_snippet_0",
      "pageTitle": "Client to Server Events in ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "javascript",
          "code": "{\n  \"type\": \"contextual_update\",\n  \"text\": \"User appears to be looking at pricing page\"\n}"
        }
      ],
      "relevance": 0.8
    },
    {
      "codeTitle": "Settings Documentation in Markdown",
      "codeDescription": "Documentation section covering voice settings, including stability, similarity, style exaggeration, and speaker boost parameters. Also includes information about pronunciation dictionaries and export settings.",
      "codeLanguage": "markdown",
      "codeTokens": 89,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/products/studio.mdx#2025-04-17_snippet_1",
      "pageTitle": "ElevenLabs Studio Documentation",
      "codeList": [
        {
          "language": "markdown",
          "code": "## Settings\n\n<AccordionGroup>\n  <Accordion title=\"Voices\">\n    ### Voices\n\n    We offer many types of voices, including the curated Default Voices library...\n  </Accordion>\n</AccordionGroup>"
        }
      ],
      "relevance": 0.8
    },
    {
      "codeTitle": "Importing Supported Languages for Speech to Text in Markdown",
      "codeDescription": "This code snippet imports a list of supported languages for the ElevenLabs Speech to Text feature from an external Markdown file.",
      "codeLanguage": "markdown",
      "codeTokens": 61,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/playground/speech-to-text.mdx#2025-04-17_snippet_0",
      "pageTitle": "Speech to Text Guide for ElevenLabs",
      "codeList": [
        {
          "language": "markdown",
          "code": "<Markdown src=\"/snippets/v1-scribe-model-languages.mdx\" />"
        }
      ],
      "relevance": 0.8
    },
    {
      "codeTitle": "Installing ElevenLabs Python Package",
      "codeDescription": "Commands for installing the elevenlabs package using pip or poetry package managers.",
      "codeLanguage": "shell",
      "codeTokens": 45,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/python.mdx#2025-04-17_snippet_0",
      "pageTitle": "ElevenLabs Python SDK Documentation",
      "codeList": [
        {
          "language": "shell",
          "code": "pip install elevenlabs\n# or\npoetry add elevenlabs"
        }
      ],
      "relevance": 0.8
    },
    {
      "codeTitle": "Configuring Twilio Environment Variables",
      "codeDescription": "Environment variables required for Twilio integration including account SID, auth token, and phone number.",
      "codeLanguage": "plaintext",
      "codeTokens": 76,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-outbound-calling.mdx#2025-04-17_snippet_3",
      "pageTitle": "Building Outbound Calling AI Agents with Twilio and ElevenLabs",
      "codeList": [
        {
          "language": "plaintext",
          "code": "TWILIO_ACCOUNT_SID=<your-account-sid>\nTWILIO_AUTH_TOKEN=<your-auth-token>\nTWILIO_PHONE_NUMBER=<your-twilio-phone-number>"
        }
      ],
      "relevance": 0.8
    },
    {
      "codeTitle": "Importing ElevenLabs SDK in Swift",
      "codeDescription": "Basic import statement for the ElevenLabs SDK package.",
      "codeLanguage": "swift",
      "codeTokens": 37,
      "codeId": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/swift.mdx#2025-04-17_snippet_0",
      "pageTitle": "Swift SDK for ElevenLabs Conversational AI",
      "codeList": [
        {
          "language": "swift",
          "code": "import ElevenLabsSDK"
        }
      ],
      "relevance": 0.8
    }
  ]